{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Redshift - Load TSV Data Into Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Get region \n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Set S3 prefixes\n",
    "tsv_prefix = 'amazon-reviews-pds/tsv'\n",
    "\n",
    "# Set S3 destination paths\n",
    "s3_destination_path_tsv = 's3://{}/{}'.format(bucket, tsv_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Amazon Redshift\n",
    "\n",
    "To create an Amazon Redshift cluster, follow these steps:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Configuration Parameters (VPC ID, Security Group ID etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "#### Get VPC ID\n",
    "# --filters \"Name=tag:Name,Values=eksctl-${AWS_CLUSTER_NAME}-cluster/VPC\"\n",
    "# Make sure this VPC is the same this notebook is running within\n",
    "# Make sure this VPC has the following 2 properties enabled\n",
    "#     DNS resolution = Enabled\n",
    "#     DNS hostnames = Enabled\n",
    "# This allows private, internal access to Redshift from this SageMaker notebook using the fully qualified endpoint name\n",
    "\n",
    "export vpc_id=$(aws ec2 describe-vpcs  --query \"Vpcs[0].VpcId\" --output text)\n",
    "export sub_id=$(aws ec2 describe-subnets --filters \"Name=vpc-id,Values=${vpc_id}\" --query \"Subnets[0].SubnetId\" --output text)\n",
    "\n",
    "# --group-name eks-fsx-security-group\n",
    "# --description \"FSx for Lustre Security Group\"\n",
    "#  --vpc-id ${vpc_id}\n",
    "# --query \"Groups[0].GroupId\"\n",
    "export sec_id=$(aws ec2 describe-security-groups --filters \"Name=vpc-id,Values=${vpc_id}\" --query \"SecurityGroups[0].GroupId\"  --output text)\n",
    "echo $sec_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This security group might need to have port 5349 open\n",
    "# COPY FROM ABOVE\n",
    "SECURITY_GROUP_ID='xxxxxx'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Redshift Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift configuration parameters\n",
    "DB_NAME = 'dsoaws'\n",
    "CLUSTER_IDENTIFIER = 'dsoaws'\n",
    "CLUSTER_TYPE = 'multi-node'\n",
    "\n",
    "# Note that only some Instance Types support Redshift Query Editor \n",
    "# (https://docs.aws.amazon.com/redshift/latest/mgmt/query-editor.html)\n",
    "NODE_TYPE = 'dc2.large'\n",
    "NUMBER_NODES = '2' \n",
    "\n",
    "MASTER_USER_NAME = 'dsoaws'\n",
    "MASTER_USER_PW = '<password>'\n",
    "\n",
    "# TODO: Must create a new IAM Role with at least S3 Access to your data bucket that you are loading into Redshift\n",
    "IAM_ROLE = '<IAM_ROLE>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = boto3.client('redshift')\n",
    "\n",
    "response = redshift.create_cluster(\n",
    "        DBName=DB_NAME,\n",
    "        ClusterIdentifier=CLUSTER_IDENTIFIER,\n",
    "        ClusterType=CLUSTER_TYPE,\n",
    "        NodeType=NODE_TYPE,\n",
    "        NumberOfNodes=int(NUMBER_NODES),       \n",
    "        MasterUsername=MASTER_USER_NAME,\n",
    "        MasterUserPassword=MASTER_USER_PW,\n",
    "        IamRoles=[IAM_ROLE],\n",
    "        VpcSecurityGroupIds=[SECURITY_GROUP_ID],\n",
    "        Port=5439,\n",
    "        PubliclyAccessible=False\n",
    ")\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Redshift Connection Via SQLAlchemy\n",
    "https://pypi.org/project/SQLAlchemy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q SQLAlchemy==1.3.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get Endpoint name programatically\n",
    "redshift_endpoint = '<endpoint-name>'\n",
    "redshift_port = '5439'\n",
    "\n",
    "SCHEMA = 'public'\n",
    "table_name_tsv = 'amazon_reviews_tsv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Redshift database engine\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(MASTER_USER_NAME, MASTER_USER_PW, redshift_endpoint, redshift_port, DB_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Session\n",
    "session = sessionmaker()\n",
    "session.configure(bind=engine)\n",
    "s = session()\n",
    "SetPath = \"SET search_path TO %s\" % SCHEMA\n",
    "s.execute(SetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TSV Table In Redshift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"DROP TABLE IF EXISTS {}\"\"\".format(table_name_tsv)\n",
    "\n",
    "s = session()\n",
    "s.execute(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"CREATE TABLE {}( \n",
    "         marketplace varchar(2),\n",
    "         customer_id varchar(8),\n",
    "         review_id varchar(14),\n",
    "         product_id varchar(10),\n",
    "         product_parent varchar(9),\n",
    "         product_title varchar(400),\n",
    "         product_category varchar(24),\n",
    "         star_rating int,\n",
    "         helpful_votes int,\n",
    "         total_votes int,\n",
    "         vine varchar(1),\n",
    "         verified_purchase varchar(1),\n",
    "         review_headline varchar(128),\n",
    "         review_body varchar(65535),\n",
    "         review_date varchar(10)\n",
    ")\"\"\".format(table_name_tsv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load TSV Data From S3 Into Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The statement below runs for approx. 30min. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"COPY {} (marketplace, customer_id, review_id, product_id, \n",
    "                product_parent, product_title, product_category, star_rating, helpful_votes, total_votes, \n",
    "                vine, verified_purchase, review_headline, review_body, review_date) \n",
    "            FROM \\'{}/\\'\n",
    "            IAM_ROLE \\'{}\\'\n",
    "            IGNOREHEADER 1 DELIMITER '\\\\t' \n",
    "            GZIP TRUNCATECOLUMNS;\"\"\".format(table_name_tsv, s3_destination_path_tsv, IAM_ROLE)\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = session()\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a sample query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql_query(\"\"\"SELECT product_category,\n",
    "                            COUNT(star_rating) AS count_star_rating\n",
    "                        FROM {}\n",
    "                        GROUP BY product_category\n",
    "                        ORDER BY count_star_rating DESC\"\"\".format(table_name_tsv), engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
