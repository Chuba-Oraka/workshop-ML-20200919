{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Redshift - Unload Parquet Data To S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Describe scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "# Get region \n",
    "session = boto3.session.Session()\n",
    "region_name = session.region_name\n",
    "\n",
    "# Get SageMaker session & default S3 bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "# Set S3 prefixes\n",
    "parquet_prefix_unload = 'amazon-reviews-pds/parquet-from-redshift'\n",
    "\n",
    "# Set S3 destination paths\n",
    "s3_destination_path_parquet_unload = 's3://{}/{}'.format(bucket, parquet_prefix_unload)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Redshift Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redshift configuration parameters\n",
    "DB_NAME = 'dsoaws'\n",
    "\n",
    "MASTER_USER_NAME = 'dsoaws'\n",
    "MASTER_USER_PW = '<password>'\n",
    "\n",
    "# TODO: Must create a new IAM Role with at least S3 Access to your data bucket that you are loading into Redshift\n",
    "IAM_ROLE = '<IAM_ROLE>'\n",
    "\n",
    "# TODO: get Endpoint name programatically\n",
    "redshift_endpoint = '<endpoint-name>'\n",
    "redshift_port = '5439'\n",
    "\n",
    "SCHEMA = 'public'\n",
    "table_name_tsv = 'amazon_reviews_tsv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Redshift Connection Via SQLAlchemy\n",
    "https://pypi.org/project/SQLAlchemy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q SQLAlchemy==1.3.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Redshift database engine\n",
    "engine = create_engine('postgresql://{}:{}@{}:{}/{}'.format(MASTER_USER_NAME, MASTER_USER_PW, redshift_endpoint, redshift_port, DB_NAME))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Session\n",
    "session = sessionmaker()\n",
    "session.configure(bind=engine)\n",
    "s = session()\n",
    "SetPath = \"SET search_path TO %s\" % SCHEMA\n",
    "s.execute(SetPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unload Parquet Data From Redshift To S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statement = \"\"\"UNLOAD ('SELECT marketplace, customer_id, review_id, product_id, product_parent, \n",
    "                        product_title, product_category, star_rating, helpful_votes, total_votes, \n",
    "                        vine, verified_purchase, review_headline, review_body, review_date FROM {}')\n",
    "                TO '{}/'\n",
    "                IAM_ROLE '{}'\n",
    "                PARQUET PARALLEL ON \n",
    "                PARTITION BY (product_category)\"\"\".format(table_name_tsv, s3_destination_path_parquet_unload, IAM_ROLE)\n",
    "\n",
    "print(statement)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: This query execution takes approx. 20min and might not show up as finished in the notebook (Check Redshift in the AWS console directly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = session()\n",
    "s.execute(statement)\n",
    "s.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List S3 directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $s3_destination_path_parquet_unload/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
