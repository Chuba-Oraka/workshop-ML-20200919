{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analyze Data Quality with SageMaker Processing Jobs and Spark\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to process and analyze data sets in order to detect data quality issues and prepare them for model training.  \n",
    "\n",
    "In this notebook we'll use Amazon SageMaker Processing with a library called [**Deequ**](https://github.com/awslabs/deequ), and leverage the power of Spark with a managed SageMaker Processing Job to run our data processing workloads.\n",
    "\n",
    "Here are some great resources on Deequ: \n",
    "* Blog Post:  https://aws.amazon.com/blogs/big-data/test-data-quality-at-scale-with-deequ/\n",
    "* Research Paper:  https://assets.amazon.science/4a/75/57047bd343fabc46ec14b34cdb3b/towards-automated-data-quality-management-for-machine-learning.pdf\n",
    "\n",
    "![Deequ](img/deequ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/processing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Customer Reviews Dataset\n",
    "\n",
    "https://s3.amazonaws.com/amazon-reviews-pds/readme.html\n",
    "\n",
    "### Dataset Columns:\n",
    "\n",
    "- `marketplace`: 2-letter country code (in this case all \"US\").\n",
    "- `customer_id`: Random identifier that can be used to aggregate reviews written by a single author.\n",
    "- `review_id`: A unique ID for the review.\n",
    "- `product_id`: The Amazon Standard Identification Number (ASIN).  `http://www.amazon.com/dp/<ASIN>` links to the product's detail page.\n",
    "- `product_parent`: The parent of that ASIN.  Multiple ASINs (color or format variations of the same product) can roll up into a single parent.\n",
    "- `product_title`: Title description of the product.\n",
    "- `product_category`: Broad product category that can be used to group reviews (in this case digital videos).\n",
    "- `star_rating`: The review's rating (1 to 5 stars).\n",
    "- `helpful_votes`: Number of helpful votes for the review.\n",
    "- `total_votes`: Number of total votes the review received.\n",
    "- `vine`: Was the review written as part of the [Vine](https://www.amazon.com/gp/vine/help) program?\n",
    "- `verified_purchase`: Was the review from a verified purchase?\n",
    "- `review_headline`: The title of the review itself.\n",
    "- `review_body`: The text of the review.\n",
    "- `review_date`: The date the review was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull the Spark-Deequ Docker Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "public_image_uri='docker.io/datascienceonaws/spark-deequ:1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0: Pulling from datascienceonaws/spark-deequ\n",
      "\n",
      "\u001b[1B52930446: Pulling fs layer \n",
      "\u001b[1B9b8e633f: Pulling fs layer \n",
      "\u001b[1B86d6fc62: Pulling fs layer \n",
      "\u001b[1Bca826205: Pulling fs layer \n",
      "\u001b[1B4eef3ec7: Pulling fs layer \n",
      "\u001b[1B53d21ef8: Pulling fs layer \n",
      "\u001b[2B53d21ef8: Waiting fs layer \n",
      "\u001b[1B407e90b8: Pulling fs layer \n",
      "\u001b[2B407e90b8: Waiting fs layer \n",
      "\u001b[1B41894336: Pulling fs layer \n",
      "\u001b[2B41894336: Waiting fs layer \n",
      "\u001b[1B7ba7bae0: Pulling fs layer \n",
      "\u001b[1B9af8056f: Pulling fs layer \n",
      "\u001b[1Ba3da4052: Pulling fs layer \n",
      "\u001b[1BDigest: sha256:4c897e8742a77beee7c473c463c21acbc1eb23da37bfface9da10d92e994ed2cK\u001b[12A\u001b[2K\u001b[10A\u001b[2K\u001b[15A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[12A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[15A\u001b[2K\u001b[6A\u001b[2K\u001b[14A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[13A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[5A\u001b[2K\u001b[4A\u001b[2K\u001b[3A\u001b[2K\u001b[2A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K\n",
      "Status: Downloaded newer image for datascienceonaws/spark-deequ:1.0.0\n",
      "docker.io/datascienceonaws/spark-deequ:1.0.0\n"
     ]
    }
   ],
   "source": [
    "!docker pull $public_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push the Image to a Private Docker Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "private_docker_repo = 'spark-deequ'\n",
    "private_docker_tag = '1.0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "889926741212.dkr.ecr.us-east-1.amazonaws.com/spark-deequ:1.0.0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "private_image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, private_docker_repo, private_docker_tag)\n",
    "print(private_image_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $public_image_uri $private_image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryNotFoundException) when calling the DescribeRepositories operation: The repository with name 'spark-deequ' does not exist in the registry with id '889926741212'\n",
      "{\n",
      "    \"repository\": {\n",
      "        \"repositoryArn\": \"arn:aws:ecr:us-east-1:889926741212:repository/spark-deequ\",\n",
      "        \"registryId\": \"889926741212\",\n",
      "        \"repositoryName\": \"spark-deequ\",\n",
      "        \"repositoryUri\": \"889926741212.dkr.ecr.us-east-1.amazonaws.com/spark-deequ\",\n",
      "        \"createdAt\": 1600543740.0,\n",
      "        \"imageTagMutability\": \"MUTABLE\",\n",
      "        \"imageScanningConfiguration\": {\n",
      "            \"scanOnPush\": false\n",
      "        },\n",
      "        \"encryptionConfiguration\": {\n",
      "            \"encryptionType\": \"AES256\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $private_docker_repo || aws ecr create-repository --repository-name $private_docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [889926741212.dkr.ecr.us-east-1.amazonaws.com/spark-deequ]\n",
      "\n",
      "\u001b[1B27265c55: Preparing \n",
      "\u001b[1B9fead42e: Preparing \n",
      "\u001b[1B08d6508a: Preparing \n",
      "\u001b[1Bc679c06f: Preparing \n",
      "\u001b[1Be5a9c66a: Preparing \n",
      "\u001b[1Bff8f884c: Preparing \n",
      "\u001b[1B71dfa891: Preparing \n",
      "\u001b[1B7150ba0e: Preparing \n",
      "\u001b[1B6dddc178: Preparing \n",
      "\u001b[1Bf98af65b: Preparing \n",
      "\u001b[1Bfd80cb68: Preparing \n",
      "\u001b[1B8102614d: Preparing \n",
      "\u001b[1B52ed4cbd: Preparing \n",
      "\u001b[1B0fa5728d: Preparing \n",
      "\u001b[6Bf98af65b: Pushed   490.3MB/481.8MB\u001b[15A\u001b[2K\u001b[15A\u001b[2K\u001b[11A\u001b[2K\u001b[14A\u001b[2K\u001b[11A\u001b[2K\u001b[12A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[8A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[6A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[7A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[5A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[10A\u001b[2K\u001b[11A\u001b[2K\u001b[11A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[3A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[3A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[2A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[4A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[1A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[10A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K\u001b[6A\u001b[2K1.0.0: digest: sha256:4c897e8742a77beee7c473c463c21acbc1eb23da37bfface9da10d92e994ed2c size: 3472\n"
     ]
    }
   ],
   "source": [
    "!docker push $private_image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Analysis Job using a SageMaker Processing Job\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built with our Spark script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m print_function\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36m__future__\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m unicode_literals\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mshutil\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcsv\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m SparkSession\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpyspark\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36msql\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mfunctions\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m *\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\r\n",
      "    args_iter = \u001b[36miter\u001b[39;49;00m(sys.argv[\u001b[34m1\u001b[39;49;00m:])\r\n",
      "    args = \u001b[36mdict\u001b[39;49;00m(\u001b[36mzip\u001b[39;49;00m(args_iter, args_iter))\r\n",
      "    \r\n",
      "    \u001b[37m# Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\u001b[39;49;00m\r\n",
      "    s3_input_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_input_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_input_data)\r\n",
      "    s3_output_analyze_data = args[\u001b[33m'\u001b[39;49;00m\u001b[33ms3_output_analyze_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].replace(\u001b[33m'\u001b[39;49;00m\u001b[33ms3://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33ms3a://\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[36mprint\u001b[39;49;00m(s3_output_analyze_data)\r\n",
      "    \r\n",
      "    spark = SparkSession.builder \\\r\n",
      "        .appName(\u001b[33m\"\u001b[39;49;00m\u001b[33mAmazon_Reviews_Spark_Analyzer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \\\r\n",
      "        .getOrCreate()\r\n",
      "\r\n",
      "    \u001b[37m# Invoke Main from preprocess-deequ.jar\u001b[39;49;00m\r\n",
      "    \u001b[36mgetattr\u001b[39;49;00m(spark._jvm.SparkAmazonReviewsAnalyzer, \u001b[33m\"\u001b[39;49;00m\u001b[33mrun\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)(s3_input_data, s3_output_analyze_data)\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-deequ.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.runners.\u001b[39;49;00m{\u001b[04m\u001b[32mAnalysisRunner\u001b[39;49;00m, \u001b[04m\u001b[32mAnalyzerContext\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.runners.AnalyzerContext.successMetricsAsDataFrame\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.analyzers.\u001b[39;49;00m{\u001b[04m\u001b[32mCompliance\u001b[39;49;00m, \u001b[04m\u001b[32mCorrelation\u001b[39;49;00m, \u001b[04m\u001b[32mSize\u001b[39;49;00m, \u001b[04m\u001b[32mCompleteness\u001b[39;49;00m, \u001b[04m\u001b[32mMean\u001b[39;49;00m, \u001b[04m\u001b[32mApproxCountDistinct\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.\u001b[39;49;00m{\u001b[04m\u001b[32mVerificationSuite\u001b[39;49;00m, \u001b[04m\u001b[32mVerificationResult\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.VerificationResult.checkResultsAsDataFrame\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.checks.\u001b[39;49;00m{\u001b[04m\u001b[32mCheck\u001b[39;49;00m, \u001b[04m\u001b[32mCheckLevel\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mcom.amazon.deequ.suggestions.\u001b[39;49;00m{\u001b[04m\u001b[32mConstraintSuggestionRunner\u001b[39;49;00m, \u001b[04m\u001b[32mRules\u001b[39;49;00m}\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.SparkSession\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.SaveMode\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36morg.apache.spark.sql.types.\u001b[39;49;00m{\u001b[04m\u001b[32mStructType\u001b[39;49;00m, \u001b[04m\u001b[32mStructField\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m}\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mobject\u001b[39;49;00m \u001b[04m\u001b[32mSparkAmazonReviewsAnalyzer\u001b[39;49;00m {\r\n",
      "  \u001b[34mdef\u001b[39;49;00m run(s3InputData\u001b[34m:\u001b[39;49;00m \u001b[36mString\u001b[39;49;00m, s3OutputAnalyzeData\u001b[34m:\u001b[39;49;00m \u001b[36mString\u001b[39;49;00m)\u001b[34m:\u001b[39;49;00m \u001b[36mUnit\u001b[39;49;00m = {\r\n",
      "\r\n",
      "    \u001b[04m\u001b[32mSystem\u001b[39;49;00m.out.println(\u001b[33ms\"\u001b[39;49;00m\u001b[33ms3_input_data: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3InputData\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \u001b[04m\u001b[32mSystem\u001b[39;49;00m.out.println(\u001b[33ms\"\u001b[39;49;00m\u001b[33ms3_output_analyze_data: \u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "      \r\n",
      "    \u001b[34mval\u001b[39;49;00m spark \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mSparkSession\u001b[39;49;00m\r\n",
      "      .builder\r\n",
      "      .appName(\u001b[33m\"SparkAmazonReviewsAnalyzer\"\u001b[39;49;00m)\r\n",
      "      .getOrCreate()\r\n",
      "    \r\n",
      "    \u001b[34mval\u001b[39;49;00m schema \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mStructType\u001b[39;49;00m(\u001b[04m\u001b[32mArray\u001b[39;49;00m(\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"marketplace\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"customer_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_id\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_parent\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_title\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"product_category\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"helpful_votes\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[04m\u001b[32mIntegerType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"vine\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"verified_purchase\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_headline\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_body\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m),\r\n",
      "        \u001b[04m\u001b[32mStructField\u001b[39;49;00m(\u001b[33m\"review_date\"\u001b[39;49;00m, \u001b[04m\u001b[32mStringType\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "    ))\r\n",
      "      \r\n",
      "    \u001b[34mval\u001b[39;49;00m dataset \u001b[34m=\u001b[39;49;00m spark.read.option(\u001b[33m\"sep\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "                            .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[33m\"true\"\u001b[39;49;00m)\r\n",
      "                            .option(\u001b[33m\"quote\"\u001b[39;49;00m, \u001b[33m\"\"\u001b[39;49;00m)\r\n",
      "                            .schema(schema)\r\n",
      "                            .csv(s3InputData)\r\n",
      "\r\n",
      "    \u001b[37m// define analyzers that compute metrics\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m analysisResult\u001b[34m:\u001b[39;49;00m \u001b[36mAnalyzerContext\u001b[39;49;00m = { \u001b[04m\u001b[32mAnalysisRunner\u001b[39;49;00m\r\n",
      "          .onData(dataset)\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mSize\u001b[39;49;00m())\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCompleteness\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mApproxCountDistinct\u001b[39;49;00m(\u001b[33m\"review_id\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mMean\u001b[39;49;00m(\u001b[33m\"star_rating\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCompliance\u001b[39;49;00m(\u001b[33m\"top star_rating\"\u001b[39;49;00m, \u001b[33m\"star_rating >= 4.0\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCorrelation\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[33m\"star_rating\"\u001b[39;49;00m))\r\n",
      "          .addAnalyzer(\u001b[04m\u001b[32mCorrelation\u001b[39;49;00m(\u001b[33m\"total_votes\"\u001b[39;49;00m, \u001b[33m\"helpful_votes\"\u001b[39;49;00m))\r\n",
      "          \u001b[37m// compute metrics\u001b[39;49;00m\r\n",
      "          .run()\r\n",
      "        }\r\n",
      "\r\n",
      "    \u001b[37m// retrieve successfully computed metrics as a Spark data frame\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m metrics \u001b[34m=\u001b[39;49;00m successMetricsAsDataFrame(spark, analysisResult)\r\n",
      "    metrics.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    metrics\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)      \r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/dataset-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m// define data quality checks,\u001b[39;49;00m\r\n",
      "    \u001b[37m// compute metrics \u001b[39;49;00m\r\n",
      "    \u001b[37m// verify check conditions\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m verificationResult\u001b[34m:\u001b[39;49;00m \u001b[36mVerificationResult\u001b[39;49;00m = { \u001b[04m\u001b[32mVerificationSuite\u001b[39;49;00m()\r\n",
      "          \u001b[37m// data to run the verification on\u001b[39;49;00m\r\n",
      "          .onData(dataset)\r\n",
      "          .addCheck(\r\n",
      "            \u001b[04m\u001b[32mCheck\u001b[39;49;00m(\u001b[04m\u001b[32mCheckLevel\u001b[39;49;00m.\u001b[04m\u001b[32mError\u001b[39;49;00m, \u001b[33m\"Review Check\"\u001b[39;49;00m) \r\n",
      "              .hasSize(\u001b[34m_\u001b[39;49;00m >= \u001b[34m200000\u001b[39;49;00m) \u001b[37m// at least 200.000 rows\u001b[39;49;00m\r\n",
      "              .hasMin(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[34m_\u001b[39;49;00m == \u001b[34m1.0\u001b[39;49;00m) \u001b[37m// min is 1.0\u001b[39;49;00m\r\n",
      "              .hasMax(\u001b[33m\"star_rating\"\u001b[39;49;00m, \u001b[34m_\u001b[39;49;00m == \u001b[34m5.0\u001b[39;49;00m) \u001b[37m// max is 5.0\u001b[39;49;00m\r\n",
      "              .isComplete(\u001b[33m\"review_id\"\u001b[39;49;00m) \u001b[37m// should never be NULL\u001b[39;49;00m\r\n",
      "              .isUnique(\u001b[33m\"review_id\"\u001b[39;49;00m) \u001b[37m// should not contain duplicates\u001b[39;49;00m\r\n",
      "              .isComplete(\u001b[33m\"marketplace\"\u001b[39;49;00m) \u001b[37m// should never be NULL\u001b[39;49;00m\r\n",
      "              .isContainedIn(\u001b[33m\"marketplace\"\u001b[39;49;00m, \u001b[04m\u001b[32mArray\u001b[39;49;00m(\u001b[33m\"US\"\u001b[39;49;00m, \u001b[33m\"UK\"\u001b[39;49;00m, \u001b[33m\"DE\"\u001b[39;49;00m, \u001b[33m\"JP\"\u001b[39;49;00m, \u001b[33m\"FR\"\u001b[39;49;00m)) \r\n",
      "              )\r\n",
      "          .run()\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[37m// convert check results to a Spark data frame\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m resultsDataFrame \u001b[34m=\u001b[39;49;00m checkResultsAsDataFrame(spark, verificationResult)\r\n",
      "    resultsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    resultsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/constraint-checks\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    \u001b[37m// generate the success metrics as a dataframe\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m verificationSuccessMetricsDataFrame \u001b[34m=\u001b[39;49;00m \u001b[04m\u001b[32mVerificationResult\u001b[39;49;00m\r\n",
      "      .successMetricsAsDataFrame(spark, verificationResult)\r\n",
      "\r\n",
      "    verificationSuccessMetricsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    verificationSuccessMetricsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)\r\n",
      "      .write\r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/success-metrics\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)      \r\n",
      "\r\n",
      "    \u001b[37m// We ask deequ to compute constraint suggestions for us on the data\u001b[39;49;00m\r\n",
      "    \u001b[37m// using a default set of rules for constraint suggestion\u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m suggestionsResult \u001b[34m=\u001b[39;49;00m { \u001b[04m\u001b[32mConstraintSuggestionRunner\u001b[39;49;00m()\r\n",
      "          .onData(dataset)\r\n",
      "          .addConstraintRules(\u001b[04m\u001b[32mRules\u001b[39;49;00m.\u001b[04m\u001b[32mDEFAULT\u001b[39;49;00m)\r\n",
      "          .run()\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mspark.implicits._\u001b[39;49;00m \u001b[37m// for toDS method below\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[37m// We can now investigate the constraints that Deequ suggested. \u001b[39;49;00m\r\n",
      "    \u001b[34mval\u001b[39;49;00m suggestionsDataFrame \u001b[34m=\u001b[39;49;00m suggestionsResult.constraintSuggestions.flatMap { \r\n",
      "          \u001b[34mcase\u001b[39;49;00m (column, suggestions) \u001b[34m=>\u001b[39;49;00m \r\n",
      "            suggestions.map { constraint \u001b[34m=>\u001b[39;49;00m\r\n",
      "              (column, constraint.description, constraint.codeForConstraint)\r\n",
      "            } \r\n",
      "    }.toSeq.toDS()\r\n",
      "      \r\n",
      "    suggestionsDataFrame.show(truncate\u001b[34m=\u001b[39;49;00m\u001b[34mfalse\u001b[39;49;00m)\r\n",
      "    suggestionsDataFrame\r\n",
      "      .repartition(\u001b[34m1\u001b[39;49;00m)      \r\n",
      "      .write      \r\n",
      "      .mode(\u001b[04m\u001b[32mSaveMode\u001b[39;49;00m.\u001b[04m\u001b[32mOverwrite\u001b[39;49;00m)\r\n",
      "      .option(\u001b[33m\"header\"\u001b[39;49;00m, \u001b[34mtrue\u001b[39;49;00m)  \r\n",
      "      .option(\u001b[33m\"delimiter\"\u001b[39;49;00m, \u001b[33m\"\\t\"\u001b[39;49;00m)\r\n",
      "      .csv(\u001b[33ms\"\u001b[39;49;00m\u001b[33m${\u001b[39;49;00ms3OutputAnalyzeData\u001b[33m}\u001b[39;49;00m\u001b[33m/constraint-suggestions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)      \r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize preprocess-deequ.scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-analyzer',\n",
    "                            image_uri=private_image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=2, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.2xlarge',\n",
    "                            env={\n",
    "                                'mode': 'jar',\n",
    "                                'main_class': 'Main'\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-889926741212/amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "s3_input_data = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-19 18:20:56   18997559 amazon_reviews_us_Digital_Software_v1_00.tsv.gz\r\n",
      "2020-09-19 18:20:59   27442648 amazon_reviews_us_Digital_Video_Games_v1_00.tsv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Output Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  amazon-reviews-spark-analyzer-2020-09-19-19-29-30\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "processing_job_name = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "\n",
    "print('Processing job name:  {}'.format(processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output\n"
     ]
    }
   ],
   "source": [
    "s3_output_analyze_data = 's3://{}/{}/output'.format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_analyze_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Spark Processing Job\n",
    "\n",
    "_Notes on Invoking from Lambda:_\n",
    "* However, if we use the boto3 SDK (ie. with a Lambda), we need to copy the `preprocess.py` file to S3 and specify the everything include --py-files, etc.\n",
    "* We would need to do the following before invoking the Lambda:\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/code/preprocess.py\n",
    "     !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/py_files/preprocess.py\n",
    "* Then reference the s3://<location> above in the --py-files, etc.\n",
    "* See Lambda example code in this same project for more details.\n",
    "\n",
    "_Notes on not using ProcessingInput and Output:_\n",
    "* Since Spark natively reads/writes from/to S3 using s3a://, we can avoid the copy required by ProcessingInput and ProcessingOutput (FullyReplicated or ShardedByS3Key) and just specify the S3 input and output buckets/prefixes._\"\n",
    "* See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "* If we use ProcessingInput, the data will be copied to each node (which we don't want in this case since Spark already handles this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-analyzer-2020-09-19-19-29-30-295\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-889926741212/spark-amazon-reviews-analyzer-2020-09-19-19-29-30-295/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'null-output', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-889926741212/spark-amazon-reviews-analyzer-2020-09-19-19-29-30-295/output/null-output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingOutput\n",
    "\n",
    "processor.run(code='preprocess-deequ.py',\n",
    "              arguments=['s3_input_data', s3_input_data,\n",
    "                         's3_output_analyze_data', s3_output_analyze_data,\n",
    "              ],\n",
    "              # See https://github.com/aws/sagemaker-python-sdk/issues/1341 \n",
    "              #   for why we need to specify a null-output\n",
    "              outputs=[\n",
    "                  ProcessingOutput(s3_upload_mode='EndOfJob',\n",
    "                                   output_name='null-output',\n",
    "                                   source='/opt/ml/processing/output')\n",
    "              ],\n",
    "              logs=True,\n",
    "              wait=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/processing-jobs/spark-amazon-reviews-analyzer-2020-09-19-19-29-30-295\">Processing Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/processing-jobs/{}\">Processing Job</a></b>'.format(region, processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-analyzer-2020-09-19-19-29-30-295;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After a Few Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "processing_job_name = processor.jobs[-1].describe()['ProcessingJobName']\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After a Few Minutes</b>'.format(region, processing_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "s3_job_output_prefix = output_prefix\n",
    "\n",
    "display(HTML('<b>Review <a target=\"blank\" href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, s3_job_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Wait Until the Processing Job Completes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n",
    "                                                                            sagemaker_session=sagemaker_session)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Please Wait Until the ^^ Processing Job ^^ Completes Above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......................\u001b[35mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: Use of this script to start YARN daemons is deprecated.\u001b[0m\n",
      "\u001b[35mWARNING: Attempting to execute replacement \"yarn --daemon start\" instead.\u001b[0m\n",
      "\u001b[35mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[35mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.2.1/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:12,686 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.176.27\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.2.1\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.2.1/etc/hadoop:/usr/hadoop-3.2.1/share/hadoop/common/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/hadoop-aws-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.375.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/common/hadoop-kms-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/audience-annotations-0.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-security-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-auth-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-text-1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-servlet-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-recipes-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-framework-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-http-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-webapp-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-codec-1.11.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/curator-client-2.13.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpclient-4.5.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/error_prone_annotations-2.2.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/checker-qual-2.5.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-net-3.6.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/hadoop-annotations-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/failureaccess-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-xml-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/dnsjava-2.1.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-compress-1.18.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/httpcore-4.4.10.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okhttp-2.7.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-server-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-core-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/netty-all-4.0.52.Final.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-util-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-lang3-3.7.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/j2objc-annotations-1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/guava-27.0-jre.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/animal-sniffer-annotations-1.17.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/okio-1.6.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-io-2.5.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jackson-databind-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/zookeeper-3.4.13.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jetty-io-9.3.24.v20180605.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-nfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-native-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-rbf-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/hdfs/hadoop-hdfs-client-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-uploader-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.2.1-tests.jar:/usr/hadoop-3.2.1/share/hadoop/yarn:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/swagger-annotations-1.5.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/objenesis-1.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/snakeyaml-1.16.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcprov-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/metrics-core-3.2.4.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/bcpkix-jdk15on-1.60.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jackson-jaxrs-base-2.9.8.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-client-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-router-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn\u001b[0m\n",
      "\u001b[34m-server-tests-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-registry-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-common-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-api-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-services-core-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.2.1.jar:/usr/hadoop-3.2.1/share/hadoop/yarn/hadoop-yarn-submarine-3.2.1.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://gitbox.apache.org/repos/asf/hadoop.git -r b3cbbb467e22ea829b3808f4b7b01d07e0bf3842; compiled by 'rohithsharmaks' on 2019-09-10T15:56Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_265\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:12,695 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:12,765 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-c2492974-29df-4c41-9dc0-ea6362224c59\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,080 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,093 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,094 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,095 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,099 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,099 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,099 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,099 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,140 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,150 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,150 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,156 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,157 INFO blockmanagement.BlockManager: The block deletion will start around 2020 Sep 19 19:33:13\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,158 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,158 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,159 INFO util.GSet: 2.0% max memory 13.7 GB = 280.5 MB\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,159 INFO util.GSet: capacity      = 2^25 = 33554432 entries\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,233 INFO blockmanagement.BlockManager: Storage policy satisfier is disabled\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,233 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,239 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,239 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,239 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,239 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,240 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,240 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,240 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,240 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,240 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,240 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,240 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,263 INFO namenode.FSDirectory: GLOBAL serial map: bits=29 maxEntries=536870911\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,263 INFO namenode.FSDirectory: USER serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,263 INFO namenode.FSDirectory: GROUP serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,263 INFO namenode.FSDirectory: XATTR serial map: bits=24 maxEntries=16777215\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,275 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,275 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,275 INFO util.GSet: 1.0% max memory 13.7 GB = 140.2 MB\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,275 INFO util.GSet: capacity      = 2^24 = 16777216 entries\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,503 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,503 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,503 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,504 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,508 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,510 INFO snapshot.SnapshotManager: SkipList is disabled\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,513 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,513 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,513 INFO util.GSet: 0.25% max memory 13.7 GB = 35.1 MB\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,513 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,521 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,521 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,521 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,524 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,524 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,525 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,525 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,525 INFO util.GSet: 0.029999999329447746% max memory 13.7 GB = 4.2 MB\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,525 INFO util.GSet: capacity      = 2^19 = 524288 entries\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,545 INFO namenode.FSImage: Allocated new BlockPoolId: BP-696565242-10.0.176.27-1600543993539\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,557 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,576 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,669 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 399 bytes saved in 0 seconds .\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,681 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,684 INFO namenode.FSImage: FSImageSaver clean checkpoint: txid=0 when meet shutdown.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:13,684 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.176.27\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34mStarting namenodes on [algo-1]\u001b[0m\n",
      "\u001b[34malgo-1: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting datanodes\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mStarting secondary namenodes [ip-10-0-176-27.ec2.internal]\u001b[0m\n",
      "\u001b[34mip-10-0-176-27.ec2.internal: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n",
      "\u001b[34mWARNING: Use of this script to start HDFS daemons is deprecated.\u001b[0m\n",
      "\u001b[34mWARNING: Attempting to execute replacement \"hdfs --daemon start\" instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mStarting resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34mStarting nodemanagers\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mlocalhost: /usr/hadoop-3.2.1/bin/../libexec/hadoop-functions.sh: line 982: ssh: command not found\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:24,865 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-889926741212/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,419 INFO spark.SparkContext: Running Spark version 2.4.6\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,437 INFO spark.SparkContext: Submitted application: Amazon_Reviews_Spark_Analyzer\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,478 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,478 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,478 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,478 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,478 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,679 INFO util.Utils: Successfully started service 'sparkDriver' on port 33275.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,699 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,713 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,715 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,715 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,722 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-3eff80ae-eae3-4548-9514-a6e6339baf6d\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,734 INFO memory.MemoryStore: MemoryStore started with capacity 366.3 MB\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,770 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,833 INFO util.log: Logging initialized @1841ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,885 INFO server.Server: jetty-9.3.z-SNAPSHOT, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,897 INFO server.Server: Started @1906ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,911 INFO server.AbstractConnector: Started ServerConnector@74f1bae5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,911 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,932 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5194e618{/jobs,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,933 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@9aaf373{/jobs/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,933 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@48e9e006{/jobs/job,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,935 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1f0ce844{/jobs/job/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,935 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@18e2d273{/stages,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,936 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@49f9c315{/stages/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,936 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@15155060{/stages/stage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,937 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64ba74c6{/stages/stage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,938 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5b13ae61{/stages/pool,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,938 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28e100f9{/stages/pool/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,938 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2caf354c{/storage,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,939 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b84685b{/storage/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,939 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5da2adec{/storage/rdd,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,940 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@7d648869{/storage/rdd/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,940 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2834d7dc{/environment,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,940 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@10caa431{/environment/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,941 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@584fa609{/executors,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,941 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1314aa9{/executors/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,942 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@f35f764{/executors/threadDump,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,942 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4a9b219f{/executors/threadDump/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,947 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6fc1a92b{/static,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,948 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1506ea8d{/,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,949 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ed22c4c{/api,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,950 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5c608af4{/jobs/job/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,950 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1e0a828f{/stages/stage/kill,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:25,952 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://10.0.176.27:4040\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,540 INFO client.RMProxy: Connecting to ResourceManager at /10.0.176.27:8032\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,786 INFO yarn.Client: Requesting a new application from cluster with 1 NodeManagers\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,846 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,846 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,860 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (63625 MB per container)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,860 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,861 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,864 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,869 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:26,911 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:27,725 INFO yarn.Client: Uploading resource file:/tmp/spark-f327819a-d20f-4813-9ae7-c1a9a1bedb14/__spark_libs__7832825153642810714.zip -> hdfs://10.0.176.27/user/root/.sparkStaging/application_1600544003530_0001/__spark_libs__7832825153642810714.zip\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,038 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,552 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,709 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/pyspark.zip -> hdfs://10.0.176.27/user/root/.sparkStaging/application_1600544003530_0001/pyspark.zip\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,717 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,734 INFO yarn.Client: Uploading resource file:/usr/spark-2.4.6/python/lib/py4j-0.10.7-src.zip -> hdfs://10.0.176.27/user/root/.sparkStaging/application_1600544003530_0001/py4j-0.10.7-src.zip\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,741 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,868 INFO yarn.Client: Uploading resource file:/tmp/spark-f327819a-d20f-4813-9ae7-c1a9a1bedb14/__spark_conf__1830819993635005333.zip -> hdfs://10.0.176.27/user/root/.sparkStaging/application_1600544003530_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,876 INFO sasl.SaslDataTransferClient: SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,905 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,905 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,905 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,905 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:28,905 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:29,614 INFO yarn.Client: Submitting application application_1600544003530_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:29,965 INFO impl.YarnClientImpl: Submitted application application_1600544003530_0001\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:29,967 INFO cluster.SchedulerExtensionServices: Starting Yarn extension services with app application_1600544003530_0001 and attemptId None\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-09-19 19:33:30,973 INFO yarn.Client: Application report for application_1600544003530_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:30,975 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1600544009702\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1600544003530_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:31,978 INFO yarn.Client: Application report for application_1600544003530_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:32,980 INFO yarn.Client: Application report for application_1600544003530_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:33,984 INFO yarn.Client: Application report for application_1600544003530_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:34,728 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1600544003530_0001), /proxy/application_1600544003530_0001\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:34,884 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:34,986 INFO yarn.Client: Application report for application_1600544003530_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:34,987 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.131.52\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1600544009702\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1600544003530_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:34,988 INFO cluster.YarnClientSchedulerBackend: Application application_1600544003530_0001 has started running.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:34,994 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41597.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:34,994 INFO netty.NettyBlockTransferService: Server created on 10.0.176.27:41597\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:34,995 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:35,011 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.176.27, 41597, None)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:35,013 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.176.27:41597 with 366.3 MB RAM, BlockManagerId(driver, 10.0.176.27, 41597, None)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:35,015 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.176.27, 41597, None)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:35,016 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.176.27, 41597, None)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:35,116 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /metrics/json.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:35,122 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@41935c0e{/metrics/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:37,267 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.131.52:52416) with ID 1\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:37,363 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-2:44167 with 24.1 GB RAM, BlockManagerId(1, algo-2, 44167, None)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,054 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,238 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/usr/spark-2.4.6/spark-warehouse').\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,239 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-2.4.6/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,244 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,245 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@39865ef7{/SQL,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,245 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/json.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,245 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62a003c3{/SQL/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,245 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,246 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@4e9e8725{/SQL/execution,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,246 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /SQL/execution/json.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,247 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5385de72{/SQL/execution/json,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,247 INFO ui.JettyUtils: Adding filter org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter to /static/sql.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,248 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@94b98cf{/static/sql,null,AVAILABLE,@Spark}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,513 INFO state.StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\u001b[0m\n",
      "\u001b[34ms3_input_data: s3a://sagemaker-us-east-1-889926741212/amazon-reviews-pds/tsv/\u001b[0m\n",
      "\u001b[34ms3_output_analyze_data: s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,526 WARN sql.SparkSession$Builder: Using an existing SparkSession; some spark core configurations may not take effect.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,644 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,686 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:56,686 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:57,846 INFO datasources.InMemoryFileIndex: It took 95 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:58,742 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:58,744 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:58,746 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string, star_rating: int, helpful_votes: int, total_votes: int ... 2 more fields>\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:58,751 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:58,797 WARN util.Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,093 INFO codegen.CodeGenerator: Code generated in 199.166904 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,225 INFO codegen.CodeGenerator: Code generated in 9.095756 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,263 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 400.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,316 INFO spark.ContextCleaner: Cleaned accumulator 1\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,329 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,331 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.176.27:41597 (size: 42.9 KB, free: 366.3 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,333 INFO spark.SparkContext: Created broadcast 0 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,350 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,550 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,564 INFO scheduler.DAGScheduler: Registering RDD 3 (collect at AnalysisRunner.scala:303) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,566 INFO scheduler.DAGScheduler: Got job 0 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,567 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,567 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,569 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 0)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,572 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,590 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 32.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,593 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 13.6 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,593 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.176.27:41597 (size: 13.6 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,594 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,606 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,607 INFO cluster.YarnScheduler: Adding task set 0.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,744 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,746 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:33:59,948 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-2:44167 (size: 13.6 KB, free: 24.1 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-09-19 19:34:00,632 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-2:44167 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,631 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 5003 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,633 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 0.0 (TID 1) in 4887 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,634 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,639 INFO scheduler.DAGScheduler: ShuffleMapStage 0 (collect at AnalysisRunner.scala:303) finished in 5.054 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,639 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,639 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,640 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,640 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,644 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,652 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 36.9 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,654 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 15.3 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,654 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.176.27:41597 (size: 15.3 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,655 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,656 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,656 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,661 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,697 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-2:44167 (size: 15.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,724 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,954 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 2) in 296 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,955 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,955 INFO scheduler.DAGScheduler: ResultStage 1 (collect at AnalysisRunner.scala:303) finished in 0.305 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:04,959 INFO scheduler.DAGScheduler: Job 0 finished: collect at AnalysisRunner.scala:303, took 5.409517 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:05,111 INFO codegen.CodeGenerator: Code generated in 19.948808 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:05,186 INFO codegen.CodeGenerator: Code generated in 12.492658 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:05,201 INFO codegen.CodeGenerator: Code generated in 10.250005 ms\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+-------------------+\u001b[0m\n",
      "\u001b[34m|entity     |instance                 |name               |value              |\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+-------------------+\u001b[0m\n",
      "\u001b[34m|Column     |review_id                |Completeness       |1.0                |\u001b[0m\n",
      "\u001b[34m|Column     |review_id                |ApproxCountDistinct|238027.0           |\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,star_rating  |Correlation        |-0.0808806564857777|\u001b[0m\n",
      "\u001b[34m|Dataset    |*                        |Size               |247515.0           |\u001b[0m\n",
      "\u001b[34m|Column     |star_rating              |Mean               |3.7237056340019796 |\u001b[0m\n",
      "\u001b[34m|Column     |top star_rating          |Compliance         |0.6633375755004747 |\u001b[0m\n",
      "\u001b[34m|Mutlicolumn|total_votes,helpful_votes|Correlation        |0.9805294402834748 |\u001b[0m\n",
      "\u001b[34m+-----------+-------------------------+-------------------+-------------------+\n",
      "\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:05,377 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:05,378 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:05,378 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:05,378 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200919193405_0000}; taskId=attempt_20200919193405_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@7d7e868}; outputPath=s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/dataset-metrics, workPath=s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/dataset-metrics/_temporary/0/_temporary/attempt_20200919193405_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/dataset-metrics\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:05,379 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:05,972 INFO codegen.CodeGenerator: Code generated in 10.311707 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,011 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:77\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,012 INFO scheduler.DAGScheduler: Registering RDD 9 (csv at preprocess-deequ.scala:77) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,012 INFO scheduler.DAGScheduler: Got job 1 (csv at preprocess-deequ.scala:77) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,012 INFO scheduler.DAGScheduler: Final stage: ResultStage 3 (csv at preprocess-deequ.scala:77)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,012 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,012 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,013 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[9] at csv at preprocess-deequ.scala:77), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,017 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 5.1 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,018 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.0 KB, free 365.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,018 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.176.27:41597 (size: 3.0 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,018 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,019 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[9] at csv at preprocess-deequ.scala:77) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,019 INFO cluster.YarnScheduler: Adding task set 2.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,021 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 3, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,022 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 2.0 (TID 4, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8116 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,022 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 2.0 (TID 5, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8205 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,022 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 2.0 (TID 6, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,022 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 2.0 (TID 7, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8229 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,036 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-2:44167 (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,054 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 2.0 (TID 4) in 33 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,054 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 3) in 35 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,055 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 2.0 (TID 7) in 33 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,056 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 2.0 (TID 5) in 34 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,056 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 2.0 (TID 6) in 34 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,057 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,057 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (csv at preprocess-deequ.scala:77) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,057 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,057 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,057 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 3)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,057 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,058 INFO scheduler.DAGScheduler: Submitting ResultStage 3 (ShuffledRowRDD[10] at csv at preprocess-deequ.scala:77), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,079 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 245.2 KB, free 365.5 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,081 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 90.5 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,081 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.176.27:41597 (size: 90.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,082 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,082 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (ShuffledRowRDD[10] at csv at preprocess-deequ.scala:77) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,082 INFO cluster.YarnScheduler: Adding task set 3.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,083 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 3.0 (TID 8, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,094 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-2:44167 (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:06,128 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:07,462 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 3.0 (TID 8) in 1379 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:07,463 INFO cluster.YarnScheduler: Removed TaskSet 3.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:07,463 INFO scheduler.DAGScheduler: ResultStage 3 (csv at preprocess-deequ.scala:77) finished in 1.405 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:07,464 INFO scheduler.DAGScheduler: Job 1 finished: csv at preprocess-deequ.scala:77, took 1.452680 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:07,950 INFO datasources.FileFormatWriter: Write Job d66899d1-3096-4bf4-893c-d9a123828c24 committed.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:07,953 INFO datasources.FileFormatWriter: Finished processing stats for write job d66899d1-3096-4bf4-893c-d9a123828c24.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,048 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,049 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,049 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, review_id: string, star_rating: int ... 1 more fields>\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,049 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,067 INFO codegen.CodeGenerator: Code generated in 7.775644 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,105 INFO codegen.CodeGenerator: Code generated in 16.46273 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,128 INFO codegen.CodeGenerator: Code generated in 14.742052 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,133 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 400.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,144 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,145 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.176.27:41597 (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,146 INFO spark.SparkContext: Created broadcast 5 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,146 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,155 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,157 INFO scheduler.DAGScheduler: Registering RDD 15 (collect at AnalysisRunner.scala:303) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,157 INFO scheduler.DAGScheduler: Got job 2 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,157 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,157 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,157 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 4)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,157 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[15] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,161 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 20.4 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,163 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 9.3 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,163 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.176.27:41597 (size: 9.3 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,163 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,164 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[15] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,164 INFO cluster.YarnScheduler: Adding task set 4.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,165 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 9, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,165 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 4.0 (TID 10, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,175 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-2:44167 (size: 9.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:08,221 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-2:44167 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,459 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 4.0 (TID 10) in 1294 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,684 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 9) in 1519 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,684 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,684 INFO scheduler.DAGScheduler: ShuffleMapStage 4 (collect at AnalysisRunner.scala:303) finished in 1.526 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,685 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,685 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,685 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,685 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,685 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[18] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,686 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 13.7 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,688 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 5.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,688 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.176.27:41597 (size: 5.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,688 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,689 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[18] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,689 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,690 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 11, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,699 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-2:44167 (size: 5.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,702 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,724 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 11) in 34 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,724 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,725 INFO scheduler.DAGScheduler: ResultStage 5 (collect at AnalysisRunner.scala:303) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,725 INFO scheduler.DAGScheduler: Job 2 finished: collect at AnalysisRunner.scala:303, took 1.569574 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,756 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,757 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,757 INFO datasources.FileSourceStrategy: Output Data Schema: struct<>\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,757 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,769 INFO codegen.CodeGenerator: Code generated in 7.959191 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,777 INFO codegen.CodeGenerator: Code generated in 5.998266 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,780 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 400.9 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,792 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,792 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.176.27:41597 (size: 42.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,792 INFO spark.SparkContext: Created broadcast 8 from count at GroupingAnalyzers.scala:76\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,793 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,801 INFO spark.SparkContext: Starting job: count at GroupingAnalyzers.scala:76\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,802 INFO scheduler.DAGScheduler: Registering RDD 21 (count at GroupingAnalyzers.scala:76) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,802 INFO scheduler.DAGScheduler: Got job 3 (count at GroupingAnalyzers.scala:76) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,803 INFO scheduler.DAGScheduler: Final stage: ResultStage 7 (count at GroupingAnalyzers.scala:76)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,803 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,803 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 6)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,803 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[21] at count at GroupingAnalyzers.scala:76), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,805 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 13.0 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,817 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 6.9 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,818 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.176.27:41597 (size: 6.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,819 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,819 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[21] at count at GroupingAnalyzers.scala:76) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,819 INFO cluster.YarnScheduler: Adding task set 6.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,820 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 12, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,820 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 6.0 (TID 13, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,821 INFO spark.ContextCleaner: Cleaned accumulator 169\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,821 INFO spark.ContextCleaner: Cleaned accumulator 118\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,821 INFO spark.ContextCleaner: Cleaned accumulator 72\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,821 INFO spark.ContextCleaner: Cleaned accumulator 143\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,821 INFO spark.ContextCleaner: Cleaned accumulator 190\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,821 INFO spark.ContextCleaner: Cleaned accumulator 175\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,821 INFO spark.ContextCleaner: Cleaned accumulator 145\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,821 INFO spark.ContextCleaner: Cleaned accumulator 130\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,821 INFO spark.ContextCleaner: Cleaned accumulator 89\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,837 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.176.27:41597 in memory (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,837 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-2:44167 (size: 6.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,838 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-2:44167 in memory (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,843 INFO spark.ContextCleaner: Cleaned accumulator 101\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,843 INFO spark.ContextCleaner: Cleaned accumulator 161\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,843 INFO spark.ContextCleaner: Cleaned accumulator 126\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,843 INFO spark.ContextCleaner: Cleaned accumulator 147\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,843 INFO spark.ContextCleaner: Cleaned accumulator 121\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,843 INFO spark.ContextCleaner: Cleaned accumulator 122\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,844 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.176.27:41597 in memory (size: 5.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,846 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-2:44167 in memory (size: 5.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,853 INFO spark.ContextCleaner: Cleaned accumulator 123\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,853 INFO spark.ContextCleaner: Cleaned accumulator 139\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,853 INFO spark.ContextCleaner: Cleaned accumulator 150\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,853 INFO spark.ContextCleaner: Cleaned accumulator 88\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,853 INFO spark.ContextCleaner: Cleaned accumulator 91\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,853 INFO spark.ContextCleaner: Cleaned accumulator 171\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,853 INFO spark.ContextCleaner: Cleaned accumulator 79\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,853 INFO spark.ContextCleaner: Cleaned accumulator 180\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,853 INFO spark.ContextCleaner: Cleaned accumulator 146\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,856 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-2:44167 in memory (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,857 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.176.27:41597 in memory (size: 3.0 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,862 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-2:44167 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 106\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 76\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 144\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 80\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 70\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 82\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 74\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 78\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 185\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 186\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 159\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 160\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 133\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 170\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 136\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 187\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 168\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 108\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 165\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 77\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 151\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 97\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 142\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,865 INFO spark.ContextCleaner: Cleaned accumulator 129\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned shuffle 2\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 124\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 109\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 178\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 116\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 90\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 183\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 75\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 155\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 87\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 140\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 158\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 148\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 99\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 93\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 84\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,869 INFO spark.ContextCleaner: Cleaned accumulator 192\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,870 INFO spark.ContextCleaner: Cleaned accumulator 104\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,870 INFO spark.ContextCleaner: Cleaned accumulator 157\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,870 INFO spark.ContextCleaner: Cleaned accumulator 176\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,870 INFO spark.ContextCleaner: Cleaned accumulator 113\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,870 INFO spark.ContextCleaner: Cleaned accumulator 149\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,873 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-2:44167 in memory (size: 9.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,876 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.176.27:41597 in memory (size: 9.3 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,885 INFO spark.ContextCleaner: Cleaned accumulator 152\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,888 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on algo-2:44167 in memory (size: 15.3 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,889 INFO storage.BlockManagerInfo: Removed broadcast_2_piece0 on 10.0.176.27:41597 in memory (size: 15.3 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 95\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 166\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 107\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 184\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 81\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 96\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 71\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 111\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 83\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 102\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 167\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 105\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 137\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 68\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 135\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 174\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 164\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 191\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 85\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 120\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 110\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 154\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 69\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,896 INFO spark.ContextCleaner: Cleaned accumulator 94\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,897 INFO spark.ContextCleaner: Cleaned accumulator 128\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,897 INFO spark.ContextCleaner: Cleaned accumulator 193\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,897 INFO spark.ContextCleaner: Cleaned accumulator 114\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,897 INFO spark.ContextCleaner: Cleaned accumulator 194\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,897 INFO spark.ContextCleaner: Cleaned accumulator 153\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,897 INFO spark.ContextCleaner: Cleaned accumulator 182\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,897 INFO spark.ContextCleaner: Cleaned accumulator 100\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,897 INFO spark.ContextCleaner: Cleaned accumulator 181\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,897 INFO spark.ContextCleaner: Cleaned accumulator 173\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned shuffle 1\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 73\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 127\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 119\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 141\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 103\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 112\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 132\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 138\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 115\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 98\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 134\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 162\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 92\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 172\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 189\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 86\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 125\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 179\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,900 INFO spark.ContextCleaner: Cleaned accumulator 131\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,903 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-2:44167 in memory (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,905 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.176.27:41597 in memory (size: 90.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,912 INFO spark.ContextCleaner: Cleaned accumulator 188\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,912 INFO spark.ContextCleaner: Cleaned accumulator 156\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,912 INFO spark.ContextCleaner: Cleaned accumulator 117\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:09,912 INFO spark.ContextCleaner: Cleaned accumulator 177\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-09-19 19:34:10,499 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 6.0 (TID 13) in 679 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,682 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 12) in 862 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,682 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,683 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (count at GroupingAnalyzers.scala:76) finished in 0.879 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,683 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,683 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,683 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 7)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,683 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,683 INFO scheduler.DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[24] at count at GroupingAnalyzers.scala:76), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,685 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.3 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,686 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.9 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,687 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.176.27:41597 (size: 3.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,687 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,688 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[24] at count at GroupingAnalyzers.scala:76) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,688 INFO cluster.YarnScheduler: Adding task set 7.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,689 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 14, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,697 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-2:44167 (size: 3.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,700 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,717 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 14) in 29 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,717 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,718 INFO scheduler.DAGScheduler: ResultStage 7 (count at GroupingAnalyzers.scala:76) finished in 0.034 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,718 INFO scheduler.DAGScheduler: Job 3 finished: count at GroupingAnalyzers.scala:76, took 0.916849 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,815 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,815 INFO datasources.FileSourceStrategy: Post-Scan Filters: isnotnull(review_id#2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,815 INFO datasources.FileSourceStrategy: Output Data Schema: struct<review_id: string>\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,817 INFO execution.FileSourceScanExec: Pushed Filters: IsNotNull(review_id)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,840 INFO codegen.CodeGenerator: Code generated in 7.109014 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,855 INFO codegen.CodeGenerator: Code generated in 10.772515 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,895 INFO codegen.CodeGenerator: Code generated in 26.708386 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,951 INFO codegen.CodeGenerator: Code generated in 33.4852 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,957 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 400.9 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,970 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,971 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.176.27:41597 (size: 42.9 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,971 INFO spark.SparkContext: Created broadcast 11 from collect at AnalysisRunner.scala:499\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,971 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,990 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:499\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,990 INFO scheduler.DAGScheduler: Registering RDD 27 (collect at AnalysisRunner.scala:499) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,991 INFO scheduler.DAGScheduler: Registering RDD 30 (collect at AnalysisRunner.scala:499) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,991 INFO scheduler.DAGScheduler: Got job 4 (collect at AnalysisRunner.scala:499) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,991 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (collect at AnalysisRunner.scala:499)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,991 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,991 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,992 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:499), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,995 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 27.2 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,997 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 12.7 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,997 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.176.27:41597 (size: 12.7 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,998 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,998 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[27] at collect at AnalysisRunner.scala:499) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,998 INFO cluster.YarnScheduler: Adding task set 8.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,999 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 15, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:10,999 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 8.0 (TID 16, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:11,010 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-2:44167 (size: 12.7 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:11,112 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-2:44167 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,502 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 8.0 (TID 16) in 1503 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,752 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 15) in 1753 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,752 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,753 INFO scheduler.DAGScheduler: ShuffleMapStage 8 (collect at AnalysisRunner.scala:499) finished in 1.760 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,753 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,753 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,753 INFO scheduler.DAGScheduler: waiting: Set(ShuffleMapStage 9, ResultStage 10)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,753 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,753 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[30] at collect at AnalysisRunner.scala:499), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,767 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 29.8 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,768 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 14.2 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,768 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.176.27:41597 (size: 14.2 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,769 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,770 INFO scheduler.DAGScheduler: Submitting 200 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[30] at collect at AnalysisRunner.scala:499) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,770 INFO cluster.YarnScheduler: Adding task set 9.0 with 200 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,773 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 17, algo-2, executor 1, partition 0, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,773 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 9.0 (TID 18, algo-2, executor 1, partition 1, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,774 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 9.0 (TID 19, algo-2, executor 1, partition 2, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,774 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 9.0 (TID 20, algo-2, executor 1, partition 3, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,774 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 9.0 (TID 21, algo-2, executor 1, partition 4, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,784 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-2:44167 (size: 14.2 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,800 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,856 INFO scheduler.TaskSetManager: Starting task 5.0 in stage 9.0 (TID 22, algo-2, executor 1, partition 5, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,857 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 9.0 (TID 21) in 83 ms on algo-2 (executor 1) (1/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,857 INFO scheduler.TaskSetManager: Starting task 6.0 in stage 9.0 (TID 23, algo-2, executor 1, partition 6, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,857 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 9.0 (TID 18) in 84 ms on algo-2 (executor 1) (2/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,878 INFO scheduler.TaskSetManager: Starting task 7.0 in stage 9.0 (TID 24, algo-2, executor 1, partition 7, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,879 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 17) in 106 ms on algo-2 (executor 1) (3/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,880 INFO scheduler.TaskSetManager: Starting task 8.0 in stage 9.0 (TID 25, algo-2, executor 1, partition 8, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,881 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 9.0 (TID 19) in 107 ms on algo-2 (executor 1) (4/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,886 INFO scheduler.TaskSetManager: Starting task 9.0 in stage 9.0 (TID 26, algo-2, executor 1, partition 9, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,886 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 9.0 (TID 20) in 112 ms on algo-2 (executor 1) (5/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,898 INFO scheduler.TaskSetManager: Starting task 10.0 in stage 9.0 (TID 27, algo-2, executor 1, partition 10, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,898 INFO scheduler.TaskSetManager: Finished task 5.0 in stage 9.0 (TID 22) in 42 ms on algo-2 (executor 1) (6/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,899 INFO scheduler.TaskSetManager: Starting task 11.0 in stage 9.0 (TID 28, algo-2, executor 1, partition 11, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,899 INFO scheduler.TaskSetManager: Finished task 7.0 in stage 9.0 (TID 24) in 21 ms on algo-2 (executor 1) (7/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,904 INFO scheduler.TaskSetManager: Starting task 12.0 in stage 9.0 (TID 29, algo-2, executor 1, partition 12, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,904 INFO scheduler.TaskSetManager: Finished task 8.0 in stage 9.0 (TID 25) in 24 ms on algo-2 (executor 1) (8/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,907 INFO scheduler.TaskSetManager: Starting task 13.0 in stage 9.0 (TID 30, algo-2, executor 1, partition 13, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,907 INFO scheduler.TaskSetManager: Finished task 9.0 in stage 9.0 (TID 26) in 21 ms on algo-2 (executor 1) (9/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,908 INFO scheduler.TaskSetManager: Starting task 14.0 in stage 9.0 (TID 31, algo-2, executor 1, partition 14, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,908 INFO scheduler.TaskSetManager: Finished task 6.0 in stage 9.0 (TID 23) in 51 ms on algo-2 (executor 1) (10/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,919 INFO scheduler.TaskSetManager: Starting task 15.0 in stage 9.0 (TID 32, algo-2, executor 1, partition 15, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,919 INFO scheduler.TaskSetManager: Finished task 10.0 in stage 9.0 (TID 27) in 21 ms on algo-2 (executor 1) (11/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,922 INFO scheduler.TaskSetManager: Starting task 16.0 in stage 9.0 (TID 33, algo-2, executor 1, partition 16, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,922 INFO scheduler.TaskSetManager: Finished task 11.0 in stage 9.0 (TID 28) in 23 ms on algo-2 (executor 1) (12/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,925 INFO scheduler.TaskSetManager: Starting task 17.0 in stage 9.0 (TID 34, algo-2, executor 1, partition 17, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,925 INFO scheduler.TaskSetManager: Finished task 12.0 in stage 9.0 (TID 29) in 21 ms on algo-2 (executor 1) (13/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,927 INFO scheduler.TaskSetManager: Starting task 18.0 in stage 9.0 (TID 35, algo-2, executor 1, partition 18, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,927 INFO scheduler.TaskSetManager: Finished task 13.0 in stage 9.0 (TID 30) in 21 ms on algo-2 (executor 1) (14/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,929 INFO scheduler.TaskSetManager: Starting task 19.0 in stage 9.0 (TID 36, algo-2, executor 1, partition 19, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,930 INFO scheduler.TaskSetManager: Finished task 14.0 in stage 9.0 (TID 31) in 22 ms on algo-2 (executor 1) (15/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,942 INFO scheduler.TaskSetManager: Starting task 20.0 in stage 9.0 (TID 37, algo-2, executor 1, partition 20, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,942 INFO scheduler.TaskSetManager: Finished task 15.0 in stage 9.0 (TID 32) in 23 ms on algo-2 (executor 1) (16/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,953 INFO scheduler.TaskSetManager: Starting task 21.0 in stage 9.0 (TID 38, algo-2, executor 1, partition 21, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,953 INFO scheduler.TaskSetManager: Finished task 18.0 in stage 9.0 (TID 35) in 26 ms on algo-2 (executor 1) (17/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,955 INFO scheduler.TaskSetManager: Starting task 22.0 in stage 9.0 (TID 39, algo-2, executor 1, partition 22, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,956 INFO scheduler.TaskSetManager: Finished task 16.0 in stage 9.0 (TID 33) in 34 ms on algo-2 (executor 1) (18/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,956 INFO scheduler.TaskSetManager: Starting task 23.0 in stage 9.0 (TID 40, algo-2, executor 1, partition 23, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,956 INFO scheduler.TaskSetManager: Finished task 17.0 in stage 9.0 (TID 34) in 31 ms on algo-2 (executor 1) (19/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,957 INFO scheduler.TaskSetManager: Starting task 24.0 in stage 9.0 (TID 41, algo-2, executor 1, partition 24, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,957 INFO scheduler.TaskSetManager: Finished task 19.0 in stage 9.0 (TID 36) in 28 ms on algo-2 (executor 1) (20/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,966 INFO scheduler.TaskSetManager: Starting task 25.0 in stage 9.0 (TID 42, algo-2, executor 1, partition 25, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,966 INFO scheduler.TaskSetManager: Finished task 20.0 in stage 9.0 (TID 37) in 24 ms on algo-2 (executor 1) (21/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,969 INFO scheduler.TaskSetManager: Starting task 26.0 in stage 9.0 (TID 43, algo-2, executor 1, partition 26, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,969 INFO scheduler.TaskSetManager: Finished task 21.0 in stage 9.0 (TID 38) in 16 ms on algo-2 (executor 1) (22/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,971 INFO scheduler.TaskSetManager: Starting task 27.0 in stage 9.0 (TID 44, algo-2, executor 1, partition 27, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,972 INFO scheduler.TaskSetManager: Finished task 22.0 in stage 9.0 (TID 39) in 16 ms on algo-2 (executor 1) (23/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,979 INFO scheduler.TaskSetManager: Starting task 28.0 in stage 9.0 (TID 45, algo-2, executor 1, partition 28, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,979 INFO scheduler.TaskSetManager: Finished task 23.0 in stage 9.0 (TID 40) in 23 ms on algo-2 (executor 1) (24/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,981 INFO scheduler.TaskSetManager: Starting task 29.0 in stage 9.0 (TID 46, algo-2, executor 1, partition 29, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,981 INFO scheduler.TaskSetManager: Finished task 24.0 in stage 9.0 (TID 41) in 25 ms on algo-2 (executor 1) (25/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,984 INFO scheduler.TaskSetManager: Starting task 30.0 in stage 9.0 (TID 47, algo-2, executor 1, partition 30, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,984 INFO scheduler.TaskSetManager: Finished task 25.0 in stage 9.0 (TID 42) in 18 ms on algo-2 (executor 1) (26/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,989 INFO scheduler.TaskSetManager: Starting task 31.0 in stage 9.0 (TID 48, algo-2, executor 1, partition 31, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,990 INFO scheduler.TaskSetManager: Starting task 32.0 in stage 9.0 (TID 49, algo-2, executor 1, partition 32, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,990 INFO scheduler.TaskSetManager: Finished task 27.0 in stage 9.0 (TID 44) in 19 ms on algo-2 (executor 1) (27/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,990 INFO scheduler.TaskSetManager: Finished task 26.0 in stage 9.0 (TID 43) in 21 ms on algo-2 (executor 1) (28/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,996 INFO scheduler.TaskSetManager: Starting task 33.0 in stage 9.0 (TID 50, algo-2, executor 1, partition 33, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,996 INFO scheduler.TaskSetManager: Finished task 28.0 in stage 9.0 (TID 45) in 18 ms on algo-2 (executor 1) (29/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,997 INFO scheduler.TaskSetManager: Starting task 34.0 in stage 9.0 (TID 51, algo-2, executor 1, partition 34, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:12,997 INFO scheduler.TaskSetManager: Finished task 29.0 in stage 9.0 (TID 46) in 17 ms on algo-2 (executor 1) (30/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,008 INFO scheduler.TaskSetManager: Starting task 35.0 in stage 9.0 (TID 52, algo-2, executor 1, partition 35, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,008 INFO scheduler.TaskSetManager: Finished task 30.0 in stage 9.0 (TID 47) in 24 ms on algo-2 (executor 1) (31/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,009 INFO scheduler.TaskSetManager: Starting task 36.0 in stage 9.0 (TID 53, algo-2, executor 1, partition 36, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,009 INFO scheduler.TaskSetManager: Starting task 37.0 in stage 9.0 (TID 54, algo-2, executor 1, partition 37, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,009 INFO scheduler.TaskSetManager: Finished task 32.0 in stage 9.0 (TID 49) in 19 ms on algo-2 (executor 1) (32/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,010 INFO scheduler.TaskSetManager: Starting task 38.0 in stage 9.0 (TID 55, algo-2, executor 1, partition 38, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,010 INFO scheduler.TaskSetManager: Finished task 31.0 in stage 9.0 (TID 48) in 21 ms on algo-2 (executor 1) (33/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,011 INFO scheduler.TaskSetManager: Finished task 33.0 in stage 9.0 (TID 50) in 15 ms on algo-2 (executor 1) (34/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,013 INFO scheduler.TaskSetManager: Starting task 39.0 in stage 9.0 (TID 56, algo-2, executor 1, partition 39, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,013 INFO scheduler.TaskSetManager: Finished task 34.0 in stage 9.0 (TID 51) in 17 ms on algo-2 (executor 1) (35/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,025 INFO scheduler.TaskSetManager: Starting task 40.0 in stage 9.0 (TID 57, algo-2, executor 1, partition 40, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,026 INFO scheduler.TaskSetManager: Finished task 36.0 in stage 9.0 (TID 53) in 17 ms on algo-2 (executor 1) (36/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,026 INFO scheduler.TaskSetManager: Starting task 41.0 in stage 9.0 (TID 58, algo-2, executor 1, partition 41, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,026 INFO scheduler.TaskSetManager: Finished task 38.0 in stage 9.0 (TID 55) in 16 ms on algo-2 (executor 1) (37/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,027 INFO scheduler.TaskSetManager: Starting task 42.0 in stage 9.0 (TID 59, algo-2, executor 1, partition 42, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,027 INFO scheduler.TaskSetManager: Finished task 35.0 in stage 9.0 (TID 52) in 19 ms on algo-2 (executor 1) (38/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,029 INFO scheduler.TaskSetManager: Starting task 43.0 in stage 9.0 (TID 60, algo-2, executor 1, partition 43, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,030 INFO scheduler.TaskSetManager: Finished task 37.0 in stage 9.0 (TID 54) in 21 ms on algo-2 (executor 1) (39/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,034 INFO scheduler.TaskSetManager: Starting task 44.0 in stage 9.0 (TID 61, algo-2, executor 1, partition 44, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,035 INFO scheduler.TaskSetManager: Finished task 39.0 in stage 9.0 (TID 56) in 21 ms on algo-2 (executor 1) (40/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,042 INFO scheduler.TaskSetManager: Starting task 45.0 in stage 9.0 (TID 62, algo-2, executor 1, partition 45, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,042 INFO scheduler.TaskSetManager: Finished task 41.0 in stage 9.0 (TID 58) in 16 ms on algo-2 (executor 1) (41/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,043 INFO scheduler.TaskSetManager: Starting task 46.0 in stage 9.0 (TID 63, algo-2, executor 1, partition 46, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,043 INFO scheduler.TaskSetManager: Finished task 42.0 in stage 9.0 (TID 59) in 16 ms on algo-2 (executor 1) (42/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,046 INFO scheduler.TaskSetManager: Starting task 47.0 in stage 9.0 (TID 64, algo-2, executor 1, partition 47, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,046 INFO scheduler.TaskSetManager: Finished task 43.0 in stage 9.0 (TID 60) in 17 ms on algo-2 (executor 1) (43/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,049 INFO scheduler.TaskSetManager: Starting task 48.0 in stage 9.0 (TID 65, algo-2, executor 1, partition 48, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,049 INFO scheduler.TaskSetManager: Finished task 44.0 in stage 9.0 (TID 61) in 15 ms on algo-2 (executor 1) (44/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,050 INFO scheduler.TaskSetManager: Starting task 49.0 in stage 9.0 (TID 66, algo-2, executor 1, partition 49, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,050 INFO scheduler.TaskSetManager: Finished task 40.0 in stage 9.0 (TID 57) in 25 ms on algo-2 (executor 1) (45/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,058 INFO scheduler.TaskSetManager: Starting task 50.0 in stage 9.0 (TID 67, algo-2, executor 1, partition 50, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,058 INFO scheduler.TaskSetManager: Finished task 45.0 in stage 9.0 (TID 62) in 16 ms on algo-2 (executor 1) (46/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,058 INFO scheduler.TaskSetManager: Starting task 51.0 in stage 9.0 (TID 68, algo-2, executor 1, partition 51, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,059 INFO scheduler.TaskSetManager: Finished task 46.0 in stage 9.0 (TID 63) in 16 ms on algo-2 (executor 1) (47/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,063 INFO scheduler.TaskSetManager: Starting task 52.0 in stage 9.0 (TID 69, algo-2, executor 1, partition 52, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,063 INFO scheduler.TaskSetManager: Finished task 47.0 in stage 9.0 (TID 64) in 17 ms on algo-2 (executor 1) (48/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,069 INFO scheduler.TaskSetManager: Starting task 53.0 in stage 9.0 (TID 70, algo-2, executor 1, partition 53, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,070 INFO scheduler.TaskSetManager: Finished task 48.0 in stage 9.0 (TID 65) in 20 ms on algo-2 (executor 1) (49/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,072 INFO scheduler.TaskSetManager: Starting task 54.0 in stage 9.0 (TID 71, algo-2, executor 1, partition 54, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,073 INFO scheduler.TaskSetManager: Finished task 49.0 in stage 9.0 (TID 66) in 23 ms on algo-2 (executor 1) (50/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,073 INFO scheduler.TaskSetManager: Starting task 55.0 in stage 9.0 (TID 72, algo-2, executor 1, partition 55, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,073 INFO scheduler.TaskSetManager: Finished task 50.0 in stage 9.0 (TID 67) in 15 ms on algo-2 (executor 1) (51/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,074 INFO scheduler.TaskSetManager: Starting task 56.0 in stage 9.0 (TID 73, algo-2, executor 1, partition 56, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,075 INFO scheduler.TaskSetManager: Finished task 51.0 in stage 9.0 (TID 68) in 17 ms on algo-2 (executor 1) (52/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,089 INFO scheduler.TaskSetManager: Starting task 57.0 in stage 9.0 (TID 74, algo-2, executor 1, partition 57, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,090 INFO scheduler.TaskSetManager: Starting task 58.0 in stage 9.0 (TID 75, algo-2, executor 1, partition 58, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,090 INFO scheduler.TaskSetManager: Finished task 52.0 in stage 9.0 (TID 69) in 27 ms on algo-2 (executor 1) (53/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,090 INFO scheduler.TaskSetManager: Finished task 54.0 in stage 9.0 (TID 71) in 18 ms on algo-2 (executor 1) (54/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,091 INFO scheduler.TaskSetManager: Starting task 59.0 in stage 9.0 (TID 76, algo-2, executor 1, partition 59, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,091 INFO scheduler.TaskSetManager: Finished task 55.0 in stage 9.0 (TID 72) in 18 ms on algo-2 (executor 1) (55/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,092 INFO scheduler.TaskSetManager: Starting task 60.0 in stage 9.0 (TID 77, algo-2, executor 1, partition 60, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,093 INFO scheduler.TaskSetManager: Finished task 56.0 in stage 9.0 (TID 73) in 19 ms on algo-2 (executor 1) (56/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,094 INFO scheduler.TaskSetManager: Starting task 61.0 in stage 9.0 (TID 78, algo-2, executor 1, partition 61, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,094 INFO scheduler.TaskSetManager: Finished task 53.0 in stage 9.0 (TID 70) in 25 ms on algo-2 (executor 1) (57/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,106 INFO scheduler.TaskSetManager: Starting task 62.0 in stage 9.0 (TID 79, algo-2, executor 1, partition 62, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,106 INFO scheduler.TaskSetManager: Finished task 57.0 in stage 9.0 (TID 74) in 17 ms on algo-2 (executor 1) (58/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,108 INFO scheduler.TaskSetManager: Starting task 63.0 in stage 9.0 (TID 80, algo-2, executor 1, partition 63, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,108 INFO scheduler.TaskSetManager: Finished task 58.0 in stage 9.0 (TID 75) in 18 ms on algo-2 (executor 1) (59/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,108 INFO scheduler.TaskSetManager: Starting task 64.0 in stage 9.0 (TID 81, algo-2, executor 1, partition 64, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,109 INFO scheduler.TaskSetManager: Finished task 61.0 in stage 9.0 (TID 78) in 15 ms on algo-2 (executor 1) (60/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,111 INFO scheduler.TaskSetManager: Starting task 65.0 in stage 9.0 (TID 82, algo-2, executor 1, partition 65, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,111 INFO scheduler.TaskSetManager: Finished task 59.0 in stage 9.0 (TID 76) in 20 ms on algo-2 (executor 1) (61/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,116 INFO scheduler.TaskSetManager: Starting task 66.0 in stage 9.0 (TID 83, algo-2, executor 1, partition 66, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,117 INFO scheduler.TaskSetManager: Finished task 60.0 in stage 9.0 (TID 77) in 25 ms on algo-2 (executor 1) (62/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,124 INFO scheduler.TaskSetManager: Starting task 67.0 in stage 9.0 (TID 84, algo-2, executor 1, partition 67, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,125 INFO scheduler.TaskSetManager: Starting task 68.0 in stage 9.0 (TID 85, algo-2, executor 1, partition 68, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,125 INFO scheduler.TaskSetManager: Finished task 62.0 in stage 9.0 (TID 79) in 19 ms on algo-2 (executor 1) (63/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,125 INFO scheduler.TaskSetManager: Finished task 63.0 in stage 9.0 (TID 80) in 17 ms on algo-2 (executor 1) (64/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,126 INFO scheduler.TaskSetManager: Starting task 69.0 in stage 9.0 (TID 86, algo-2, executor 1, partition 69, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,127 INFO scheduler.TaskSetManager: Starting task 70.0 in stage 9.0 (TID 87, algo-2, executor 1, partition 70, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,127 INFO scheduler.TaskSetManager: Finished task 65.0 in stage 9.0 (TID 82) in 16 ms on algo-2 (executor 1) (65/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,127 INFO scheduler.TaskSetManager: Finished task 64.0 in stage 9.0 (TID 81) in 19 ms on algo-2 (executor 1) (66/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,134 INFO scheduler.TaskSetManager: Starting task 71.0 in stage 9.0 (TID 88, algo-2, executor 1, partition 71, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,134 INFO scheduler.TaskSetManager: Finished task 66.0 in stage 9.0 (TID 83) in 18 ms on algo-2 (executor 1) (67/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,142 INFO scheduler.TaskSetManager: Starting task 72.0 in stage 9.0 (TID 89, algo-2, executor 1, partition 72, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,142 INFO scheduler.TaskSetManager: Finished task 67.0 in stage 9.0 (TID 84) in 18 ms on algo-2 (executor 1) (68/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,142 INFO scheduler.TaskSetManager: Starting task 73.0 in stage 9.0 (TID 90, algo-2, executor 1, partition 73, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,143 INFO scheduler.TaskSetManager: Finished task 68.0 in stage 9.0 (TID 85) in 19 ms on algo-2 (executor 1) (69/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,143 INFO scheduler.TaskSetManager: Starting task 74.0 in stage 9.0 (TID 91, algo-2, executor 1, partition 74, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,144 INFO scheduler.TaskSetManager: Finished task 69.0 in stage 9.0 (TID 86) in 18 ms on algo-2 (executor 1) (70/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,145 INFO scheduler.TaskSetManager: Starting task 75.0 in stage 9.0 (TID 92, algo-2, executor 1, partition 75, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,145 INFO scheduler.TaskSetManager: Finished task 70.0 in stage 9.0 (TID 87) in 18 ms on algo-2 (executor 1) (71/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,155 INFO scheduler.TaskSetManager: Starting task 76.0 in stage 9.0 (TID 93, algo-2, executor 1, partition 76, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,155 INFO scheduler.TaskSetManager: Finished task 71.0 in stage 9.0 (TID 88) in 21 ms on algo-2 (executor 1) (72/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,158 INFO scheduler.TaskSetManager: Starting task 77.0 in stage 9.0 (TID 94, algo-2, executor 1, partition 77, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,159 INFO scheduler.TaskSetManager: Finished task 72.0 in stage 9.0 (TID 89) in 17 ms on algo-2 (executor 1) (73/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,159 INFO scheduler.TaskSetManager: Starting task 78.0 in stage 9.0 (TID 95, algo-2, executor 1, partition 78, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,159 INFO scheduler.TaskSetManager: Finished task 73.0 in stage 9.0 (TID 90) in 17 ms on algo-2 (executor 1) (74/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,165 INFO scheduler.TaskSetManager: Starting task 79.0 in stage 9.0 (TID 96, algo-2, executor 1, partition 79, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,166 INFO scheduler.TaskSetManager: Finished task 75.0 in stage 9.0 (TID 92) in 22 ms on algo-2 (executor 1) (75/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,167 INFO scheduler.TaskSetManager: Starting task 80.0 in stage 9.0 (TID 97, algo-2, executor 1, partition 80, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,167 INFO scheduler.TaskSetManager: Finished task 74.0 in stage 9.0 (TID 91) in 24 ms on algo-2 (executor 1) (76/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,175 INFO scheduler.TaskSetManager: Starting task 81.0 in stage 9.0 (TID 98, algo-2, executor 1, partition 81, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,176 INFO scheduler.TaskSetManager: Finished task 78.0 in stage 9.0 (TID 95) in 17 ms on algo-2 (executor 1) (77/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,176 INFO scheduler.TaskSetManager: Starting task 82.0 in stage 9.0 (TID 99, algo-2, executor 1, partition 82, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,176 INFO scheduler.TaskSetManager: Finished task 76.0 in stage 9.0 (TID 93) in 22 ms on algo-2 (executor 1) (78/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,179 INFO scheduler.TaskSetManager: Starting task 83.0 in stage 9.0 (TID 100, algo-2, executor 1, partition 83, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,179 INFO scheduler.TaskSetManager: Finished task 79.0 in stage 9.0 (TID 96) in 14 ms on algo-2 (executor 1) (79/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,180 INFO scheduler.TaskSetManager: Starting task 84.0 in stage 9.0 (TID 101, algo-2, executor 1, partition 84, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,180 INFO scheduler.TaskSetManager: Finished task 77.0 in stage 9.0 (TID 94) in 22 ms on algo-2 (executor 1) (80/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,187 INFO scheduler.TaskSetManager: Starting task 85.0 in stage 9.0 (TID 102, algo-2, executor 1, partition 85, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,188 INFO scheduler.TaskSetManager: Finished task 80.0 in stage 9.0 (TID 97) in 21 ms on algo-2 (executor 1) (81/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,192 INFO scheduler.TaskSetManager: Starting task 86.0 in stage 9.0 (TID 103, algo-2, executor 1, partition 86, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,192 INFO scheduler.TaskSetManager: Finished task 81.0 in stage 9.0 (TID 98) in 17 ms on algo-2 (executor 1) (82/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,195 INFO scheduler.TaskSetManager: Starting task 87.0 in stage 9.0 (TID 104, algo-2, executor 1, partition 87, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,195 INFO scheduler.TaskSetManager: Finished task 84.0 in stage 9.0 (TID 101) in 15 ms on algo-2 (executor 1) (83/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,198 INFO scheduler.TaskSetManager: Starting task 88.0 in stage 9.0 (TID 105, algo-2, executor 1, partition 88, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,198 INFO scheduler.TaskSetManager: Finished task 83.0 in stage 9.0 (TID 100) in 19 ms on algo-2 (executor 1) (84/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,201 INFO scheduler.TaskSetManager: Starting task 89.0 in stage 9.0 (TID 106, algo-2, executor 1, partition 89, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,201 INFO scheduler.TaskSetManager: Finished task 82.0 in stage 9.0 (TID 99) in 25 ms on algo-2 (executor 1) (85/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,202 INFO scheduler.TaskSetManager: Starting task 90.0 in stage 9.0 (TID 107, algo-2, executor 1, partition 90, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,202 INFO scheduler.TaskSetManager: Finished task 85.0 in stage 9.0 (TID 102) in 15 ms on algo-2 (executor 1) (86/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,210 INFO scheduler.TaskSetManager: Starting task 91.0 in stage 9.0 (TID 108, algo-2, executor 1, partition 91, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,210 INFO scheduler.TaskSetManager: Finished task 86.0 in stage 9.0 (TID 103) in 18 ms on algo-2 (executor 1) (87/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,212 INFO scheduler.TaskSetManager: Starting task 92.0 in stage 9.0 (TID 109, algo-2, executor 1, partition 92, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,212 INFO scheduler.TaskSetManager: Finished task 87.0 in stage 9.0 (TID 104) in 17 ms on algo-2 (executor 1) (88/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,212 INFO scheduler.TaskSetManager: Starting task 93.0 in stage 9.0 (TID 110, algo-2, executor 1, partition 93, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,213 INFO scheduler.TaskSetManager: Finished task 88.0 in stage 9.0 (TID 105) in 15 ms on algo-2 (executor 1) (89/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,213 INFO scheduler.TaskSetManager: Starting task 94.0 in stage 9.0 (TID 111, algo-2, executor 1, partition 94, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,214 INFO scheduler.TaskSetManager: Finished task 89.0 in stage 9.0 (TID 106) in 13 ms on algo-2 (executor 1) (90/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,223 INFO scheduler.TaskSetManager: Starting task 95.0 in stage 9.0 (TID 112, algo-2, executor 1, partition 95, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,223 INFO scheduler.TaskSetManager: Starting task 96.0 in stage 9.0 (TID 113, algo-2, executor 1, partition 96, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,223 INFO scheduler.TaskSetManager: Finished task 90.0 in stage 9.0 (TID 107) in 21 ms on algo-2 (executor 1) (91/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,223 INFO scheduler.TaskSetManager: Finished task 91.0 in stage 9.0 (TID 108) in 13 ms on algo-2 (executor 1) (92/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,225 INFO scheduler.TaskSetManager: Starting task 97.0 in stage 9.0 (TID 114, algo-2, executor 1, partition 97, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,225 INFO scheduler.TaskSetManager: Finished task 92.0 in stage 9.0 (TID 109) in 13 ms on algo-2 (executor 1) (93/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,226 INFO scheduler.TaskSetManager: Starting task 98.0 in stage 9.0 (TID 115, algo-2, executor 1, partition 98, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,227 INFO scheduler.TaskSetManager: Finished task 94.0 in stage 9.0 (TID 111) in 14 ms on algo-2 (executor 1) (94/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,229 INFO scheduler.TaskSetManager: Starting task 99.0 in stage 9.0 (TID 116, algo-2, executor 1, partition 99, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,229 INFO scheduler.TaskSetManager: Finished task 93.0 in stage 9.0 (TID 110) in 17 ms on algo-2 (executor 1) (95/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,243 INFO scheduler.TaskSetManager: Starting task 100.0 in stage 9.0 (TID 117, algo-2, executor 1, partition 100, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,243 INFO scheduler.TaskSetManager: Finished task 97.0 in stage 9.0 (TID 114) in 18 ms on algo-2 (executor 1) (96/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,244 INFO scheduler.TaskSetManager: Starting task 101.0 in stage 9.0 (TID 118, algo-2, executor 1, partition 101, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,244 INFO scheduler.TaskSetManager: Finished task 98.0 in stage 9.0 (TID 115) in 18 ms on algo-2 (executor 1) (97/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,245 INFO scheduler.TaskSetManager: Starting task 102.0 in stage 9.0 (TID 119, algo-2, executor 1, partition 102, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,245 INFO scheduler.TaskSetManager: Finished task 95.0 in stage 9.0 (TID 112) in 23 ms on algo-2 (executor 1) (98/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,246 INFO scheduler.TaskSetManager: Starting task 103.0 in stage 9.0 (TID 120, algo-2, executor 1, partition 103, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,246 INFO scheduler.TaskSetManager: Finished task 99.0 in stage 9.0 (TID 116) in 18 ms on algo-2 (executor 1) (99/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,400 INFO scheduler.TaskSetManager: Starting task 104.0 in stage 9.0 (TID 121, algo-2, executor 1, partition 104, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,400 INFO scheduler.TaskSetManager: Finished task 96.0 in stage 9.0 (TID 113) in 177 ms on algo-2 (executor 1) (100/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,411 INFO scheduler.TaskSetManager: Starting task 105.0 in stage 9.0 (TID 122, algo-2, executor 1, partition 105, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,411 INFO scheduler.TaskSetManager: Finished task 100.0 in stage 9.0 (TID 117) in 169 ms on algo-2 (executor 1) (101/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,411 INFO scheduler.TaskSetManager: Starting task 106.0 in stage 9.0 (TID 123, algo-2, executor 1, partition 106, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,411 INFO scheduler.TaskSetManager: Finished task 103.0 in stage 9.0 (TID 120) in 165 ms on algo-2 (executor 1) (102/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,415 INFO scheduler.TaskSetManager: Starting task 107.0 in stage 9.0 (TID 124, algo-2, executor 1, partition 107, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,415 INFO scheduler.TaskSetManager: Finished task 102.0 in stage 9.0 (TID 119) in 170 ms on algo-2 (executor 1) (103/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,420 INFO scheduler.TaskSetManager: Starting task 108.0 in stage 9.0 (TID 125, algo-2, executor 1, partition 108, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,420 INFO scheduler.TaskSetManager: Finished task 101.0 in stage 9.0 (TID 118) in 176 ms on algo-2 (executor 1) (104/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,424 INFO scheduler.TaskSetManager: Starting task 109.0 in stage 9.0 (TID 126, algo-2, executor 1, partition 109, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,424 INFO scheduler.TaskSetManager: Finished task 106.0 in stage 9.0 (TID 123) in 13 ms on algo-2 (executor 1) (105/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,425 INFO scheduler.TaskSetManager: Starting task 110.0 in stage 9.0 (TID 127, algo-2, executor 1, partition 110, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,425 INFO scheduler.TaskSetManager: Finished task 105.0 in stage 9.0 (TID 122) in 14 ms on algo-2 (executor 1) (106/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,426 INFO scheduler.TaskSetManager: Starting task 111.0 in stage 9.0 (TID 128, algo-2, executor 1, partition 111, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,426 INFO scheduler.TaskSetManager: Finished task 107.0 in stage 9.0 (TID 124) in 11 ms on algo-2 (executor 1) (107/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,427 INFO scheduler.TaskSetManager: Starting task 112.0 in stage 9.0 (TID 129, algo-2, executor 1, partition 112, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,427 INFO scheduler.TaskSetManager: Finished task 104.0 in stage 9.0 (TID 121) in 27 ms on algo-2 (executor 1) (108/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,430 INFO scheduler.TaskSetManager: Starting task 113.0 in stage 9.0 (TID 130, algo-2, executor 1, partition 113, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,430 INFO scheduler.TaskSetManager: Finished task 108.0 in stage 9.0 (TID 125) in 10 ms on algo-2 (executor 1) (109/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,436 INFO scheduler.TaskSetManager: Starting task 114.0 in stage 9.0 (TID 131, algo-2, executor 1, partition 114, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,436 INFO scheduler.TaskSetManager: Finished task 110.0 in stage 9.0 (TID 127) in 11 ms on algo-2 (executor 1) (110/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,440 INFO scheduler.TaskSetManager: Starting task 115.0 in stage 9.0 (TID 132, algo-2, executor 1, partition 115, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,441 INFO scheduler.TaskSetManager: Finished task 109.0 in stage 9.0 (TID 126) in 17 ms on algo-2 (executor 1) (111/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,441 INFO scheduler.TaskSetManager: Starting task 116.0 in stage 9.0 (TID 133, algo-2, executor 1, partition 116, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,441 INFO scheduler.TaskSetManager: Starting task 117.0 in stage 9.0 (TID 134, algo-2, executor 1, partition 117, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,442 INFO scheduler.TaskSetManager: Finished task 112.0 in stage 9.0 (TID 129) in 15 ms on algo-2 (executor 1) (112/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,442 INFO scheduler.TaskSetManager: Finished task 111.0 in stage 9.0 (TID 128) in 16 ms on algo-2 (executor 1) (113/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,443 INFO scheduler.TaskSetManager: Starting task 118.0 in stage 9.0 (TID 135, algo-2, executor 1, partition 118, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,443 INFO scheduler.TaskSetManager: Finished task 113.0 in stage 9.0 (TID 130) in 13 ms on algo-2 (executor 1) (114/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,453 INFO scheduler.TaskSetManager: Starting task 119.0 in stage 9.0 (TID 136, algo-2, executor 1, partition 119, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,453 INFO scheduler.TaskSetManager: Finished task 114.0 in stage 9.0 (TID 131) in 17 ms on algo-2 (executor 1) (115/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,454 INFO scheduler.TaskSetManager: Starting task 120.0 in stage 9.0 (TID 137, algo-2, executor 1, partition 120, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,454 INFO scheduler.TaskSetManager: Finished task 115.0 in stage 9.0 (TID 132) in 14 ms on algo-2 (executor 1) (116/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,454 INFO scheduler.TaskSetManager: Starting task 121.0 in stage 9.0 (TID 138, algo-2, executor 1, partition 121, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,454 INFO scheduler.TaskSetManager: Finished task 117.0 in stage 9.0 (TID 134) in 13 ms on algo-2 (executor 1) (117/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,457 INFO scheduler.TaskSetManager: Starting task 122.0 in stage 9.0 (TID 139, algo-2, executor 1, partition 122, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,457 INFO scheduler.TaskSetManager: Finished task 118.0 in stage 9.0 (TID 135) in 14 ms on algo-2 (executor 1) (118/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,463 INFO scheduler.TaskSetManager: Starting task 123.0 in stage 9.0 (TID 140, algo-2, executor 1, partition 123, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,464 INFO scheduler.TaskSetManager: Finished task 116.0 in stage 9.0 (TID 133) in 23 ms on algo-2 (executor 1) (119/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,467 INFO scheduler.TaskSetManager: Starting task 124.0 in stage 9.0 (TID 141, algo-2, executor 1, partition 124, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,467 INFO scheduler.TaskSetManager: Finished task 119.0 in stage 9.0 (TID 136) in 14 ms on algo-2 (executor 1) (120/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,467 INFO scheduler.TaskSetManager: Starting task 125.0 in stage 9.0 (TID 142, algo-2, executor 1, partition 125, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,467 INFO scheduler.TaskSetManager: Finished task 121.0 in stage 9.0 (TID 138) in 13 ms on algo-2 (executor 1) (121/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,469 INFO scheduler.TaskSetManager: Starting task 126.0 in stage 9.0 (TID 143, algo-2, executor 1, partition 126, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,469 INFO scheduler.TaskSetManager: Finished task 120.0 in stage 9.0 (TID 137) in 15 ms on algo-2 (executor 1) (122/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,474 INFO scheduler.TaskSetManager: Starting task 127.0 in stage 9.0 (TID 144, algo-2, executor 1, partition 127, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,475 INFO scheduler.TaskSetManager: Finished task 123.0 in stage 9.0 (TID 140) in 11 ms on algo-2 (executor 1) (123/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,479 INFO scheduler.TaskSetManager: Starting task 128.0 in stage 9.0 (TID 145, algo-2, executor 1, partition 128, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,479 INFO scheduler.TaskSetManager: Finished task 125.0 in stage 9.0 (TID 142) in 12 ms on algo-2 (executor 1) (124/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,479 INFO scheduler.TaskSetManager: Starting task 129.0 in stage 9.0 (TID 146, algo-2, executor 1, partition 129, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,480 INFO scheduler.TaskSetManager: Starting task 130.0 in stage 9.0 (TID 147, algo-2, executor 1, partition 130, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,480 INFO scheduler.TaskSetManager: Finished task 124.0 in stage 9.0 (TID 141) in 14 ms on algo-2 (executor 1) (125/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,480 INFO scheduler.TaskSetManager: Finished task 122.0 in stage 9.0 (TID 139) in 23 ms on algo-2 (executor 1) (126/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,484 INFO scheduler.TaskSetManager: Starting task 131.0 in stage 9.0 (TID 148, algo-2, executor 1, partition 131, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,484 INFO scheduler.TaskSetManager: Finished task 126.0 in stage 9.0 (TID 143) in 15 ms on algo-2 (executor 1) (127/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,488 INFO scheduler.TaskSetManager: Starting task 132.0 in stage 9.0 (TID 149, algo-2, executor 1, partition 132, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,488 INFO scheduler.TaskSetManager: Finished task 127.0 in stage 9.0 (TID 144) in 14 ms on algo-2 (executor 1) (128/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,490 INFO scheduler.TaskSetManager: Starting task 133.0 in stage 9.0 (TID 150, algo-2, executor 1, partition 133, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,490 INFO scheduler.TaskSetManager: Finished task 128.0 in stage 9.0 (TID 145) in 11 ms on algo-2 (executor 1) (129/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,492 INFO scheduler.TaskSetManager: Starting task 134.0 in stage 9.0 (TID 151, algo-2, executor 1, partition 134, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,492 INFO scheduler.TaskSetManager: Finished task 129.0 in stage 9.0 (TID 146) in 13 ms on algo-2 (executor 1) (130/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,495 INFO scheduler.TaskSetManager: Starting task 135.0 in stage 9.0 (TID 152, algo-2, executor 1, partition 135, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,496 INFO scheduler.TaskSetManager: Starting task 136.0 in stage 9.0 (TID 153, algo-2, executor 1, partition 136, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,496 INFO scheduler.TaskSetManager: Finished task 130.0 in stage 9.0 (TID 147) in 16 ms on algo-2 (executor 1) (131/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,496 INFO scheduler.TaskSetManager: Finished task 131.0 in stage 9.0 (TID 148) in 13 ms on algo-2 (executor 1) (132/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,502 INFO scheduler.TaskSetManager: Starting task 137.0 in stage 9.0 (TID 154, algo-2, executor 1, partition 137, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,502 INFO scheduler.TaskSetManager: Finished task 132.0 in stage 9.0 (TID 149) in 14 ms on algo-2 (executor 1) (133/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,503 INFO scheduler.TaskSetManager: Starting task 138.0 in stage 9.0 (TID 155, algo-2, executor 1, partition 138, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,503 INFO scheduler.TaskSetManager: Finished task 133.0 in stage 9.0 (TID 150) in 13 ms on algo-2 (executor 1) (134/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,504 INFO scheduler.TaskSetManager: Starting task 139.0 in stage 9.0 (TID 156, algo-2, executor 1, partition 139, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,504 INFO scheduler.TaskSetManager: Finished task 134.0 in stage 9.0 (TID 151) in 12 ms on algo-2 (executor 1) (135/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,507 INFO scheduler.TaskSetManager: Starting task 140.0 in stage 9.0 (TID 157, algo-2, executor 1, partition 140, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,508 INFO scheduler.TaskSetManager: Finished task 135.0 in stage 9.0 (TID 152) in 13 ms on algo-2 (executor 1) (136/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,508 INFO scheduler.TaskSetManager: Starting task 141.0 in stage 9.0 (TID 158, algo-2, executor 1, partition 141, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,508 INFO scheduler.TaskSetManager: Finished task 136.0 in stage 9.0 (TID 153) in 12 ms on algo-2 (executor 1) (137/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,513 INFO scheduler.TaskSetManager: Starting task 142.0 in stage 9.0 (TID 159, algo-2, executor 1, partition 142, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,513 INFO scheduler.TaskSetManager: Finished task 137.0 in stage 9.0 (TID 154) in 11 ms on algo-2 (executor 1) (138/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,516 INFO scheduler.TaskSetManager: Starting task 143.0 in stage 9.0 (TID 160, algo-2, executor 1, partition 143, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,516 INFO scheduler.TaskSetManager: Finished task 139.0 in stage 9.0 (TID 156) in 13 ms on algo-2 (executor 1) (139/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,517 INFO scheduler.TaskSetManager: Starting task 144.0 in stage 9.0 (TID 161, algo-2, executor 1, partition 144, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,517 INFO scheduler.TaskSetManager: Finished task 141.0 in stage 9.0 (TID 158) in 9 ms on algo-2 (executor 1) (140/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,518 INFO scheduler.TaskSetManager: Starting task 145.0 in stage 9.0 (TID 162, algo-2, executor 1, partition 145, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,518 INFO scheduler.TaskSetManager: Finished task 138.0 in stage 9.0 (TID 155) in 15 ms on algo-2 (executor 1) (141/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,526 INFO scheduler.TaskSetManager: Starting task 146.0 in stage 9.0 (TID 163, algo-2, executor 1, partition 146, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,526 INFO scheduler.TaskSetManager: Finished task 140.0 in stage 9.0 (TID 157) in 19 ms on algo-2 (executor 1) (142/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,527 INFO scheduler.TaskSetManager: Starting task 147.0 in stage 9.0 (TID 164, algo-2, executor 1, partition 147, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,527 INFO scheduler.TaskSetManager: Finished task 142.0 in stage 9.0 (TID 159) in 14 ms on algo-2 (executor 1) (143/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,527 INFO scheduler.TaskSetManager: Starting task 148.0 in stage 9.0 (TID 165, algo-2, executor 1, partition 148, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,527 INFO scheduler.TaskSetManager: Finished task 143.0 in stage 9.0 (TID 160) in 12 ms on algo-2 (executor 1) (144/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,528 INFO scheduler.TaskSetManager: Starting task 149.0 in stage 9.0 (TID 166, algo-2, executor 1, partition 149, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,529 INFO scheduler.TaskSetManager: Finished task 144.0 in stage 9.0 (TID 161) in 12 ms on algo-2 (executor 1) (145/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,529 INFO scheduler.TaskSetManager: Starting task 150.0 in stage 9.0 (TID 167, algo-2, executor 1, partition 150, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,529 INFO scheduler.TaskSetManager: Finished task 145.0 in stage 9.0 (TID 162) in 11 ms on algo-2 (executor 1) (146/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,539 INFO scheduler.TaskSetManager: Starting task 151.0 in stage 9.0 (TID 168, algo-2, executor 1, partition 151, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,539 INFO scheduler.TaskSetManager: Finished task 148.0 in stage 9.0 (TID 165) in 12 ms on algo-2 (executor 1) (147/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,540 INFO scheduler.TaskSetManager: Starting task 152.0 in stage 9.0 (TID 169, algo-2, executor 1, partition 152, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,540 INFO scheduler.TaskSetManager: Finished task 146.0 in stage 9.0 (TID 163) in 14 ms on algo-2 (executor 1) (148/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,540 INFO scheduler.TaskSetManager: Starting task 153.0 in stage 9.0 (TID 170, algo-2, executor 1, partition 153, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,540 INFO scheduler.TaskSetManager: Finished task 147.0 in stage 9.0 (TID 164) in 13 ms on algo-2 (executor 1) (149/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,543 INFO scheduler.TaskSetManager: Starting task 154.0 in stage 9.0 (TID 171, algo-2, executor 1, partition 154, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,543 INFO scheduler.TaskSetManager: Finished task 149.0 in stage 9.0 (TID 166) in 15 ms on algo-2 (executor 1) (150/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,548 INFO scheduler.TaskSetManager: Starting task 155.0 in stage 9.0 (TID 172, algo-2, executor 1, partition 155, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,548 INFO scheduler.TaskSetManager: Finished task 150.0 in stage 9.0 (TID 167) in 19 ms on algo-2 (executor 1) (151/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,552 INFO scheduler.TaskSetManager: Starting task 156.0 in stage 9.0 (TID 173, algo-2, executor 1, partition 156, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,552 INFO scheduler.TaskSetManager: Finished task 153.0 in stage 9.0 (TID 170) in 12 ms on algo-2 (executor 1) (152/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,553 INFO scheduler.TaskSetManager: Starting task 157.0 in stage 9.0 (TID 174, algo-2, executor 1, partition 157, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,553 INFO scheduler.TaskSetManager: Finished task 151.0 in stage 9.0 (TID 168) in 14 ms on algo-2 (executor 1) (153/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,554 INFO scheduler.TaskSetManager: Starting task 158.0 in stage 9.0 (TID 175, algo-2, executor 1, partition 158, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,554 INFO scheduler.TaskSetManager: Finished task 152.0 in stage 9.0 (TID 169) in 15 ms on algo-2 (executor 1) (154/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,555 INFO scheduler.TaskSetManager: Starting task 159.0 in stage 9.0 (TID 176, algo-2, executor 1, partition 159, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,556 INFO scheduler.TaskSetManager: Finished task 154.0 in stage 9.0 (TID 171) in 13 ms on algo-2 (executor 1) (155/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,561 INFO scheduler.TaskSetManager: Starting task 160.0 in stage 9.0 (TID 177, algo-2, executor 1, partition 160, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,561 INFO scheduler.TaskSetManager: Finished task 155.0 in stage 9.0 (TID 172) in 13 ms on algo-2 (executor 1) (156/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,566 INFO scheduler.TaskSetManager: Starting task 161.0 in stage 9.0 (TID 178, algo-2, executor 1, partition 161, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,566 INFO scheduler.TaskSetManager: Finished task 157.0 in stage 9.0 (TID 174) in 13 ms on algo-2 (executor 1) (157/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,567 INFO scheduler.TaskSetManager: Starting task 162.0 in stage 9.0 (TID 179, algo-2, executor 1, partition 162, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,567 INFO scheduler.TaskSetManager: Finished task 156.0 in stage 9.0 (TID 173) in 15 ms on algo-2 (executor 1) (158/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,567 INFO scheduler.TaskSetManager: Starting task 163.0 in stage 9.0 (TID 180, algo-2, executor 1, partition 163, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,568 INFO scheduler.TaskSetManager: Finished task 159.0 in stage 9.0 (TID 176) in 13 ms on algo-2 (executor 1) (159/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,569 INFO scheduler.TaskSetManager: Starting task 164.0 in stage 9.0 (TID 181, algo-2, executor 1, partition 164, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,569 INFO scheduler.TaskSetManager: Finished task 158.0 in stage 9.0 (TID 175) in 15 ms on algo-2 (executor 1) (160/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,575 INFO scheduler.TaskSetManager: Starting task 165.0 in stage 9.0 (TID 182, algo-2, executor 1, partition 165, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,575 INFO scheduler.TaskSetManager: Finished task 160.0 in stage 9.0 (TID 177) in 14 ms on algo-2 (executor 1) (161/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,580 INFO scheduler.TaskSetManager: Starting task 166.0 in stage 9.0 (TID 183, algo-2, executor 1, partition 166, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,580 INFO scheduler.TaskSetManager: Finished task 161.0 in stage 9.0 (TID 178) in 14 ms on algo-2 (executor 1) (162/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,582 INFO scheduler.TaskSetManager: Starting task 167.0 in stage 9.0 (TID 184, algo-2, executor 1, partition 167, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,582 INFO scheduler.TaskSetManager: Finished task 162.0 in stage 9.0 (TID 179) in 16 ms on algo-2 (executor 1) (163/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,583 INFO scheduler.TaskSetManager: Starting task 168.0 in stage 9.0 (TID 185, algo-2, executor 1, partition 168, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,583 INFO scheduler.TaskSetManager: Finished task 163.0 in stage 9.0 (TID 180) in 16 ms on algo-2 (executor 1) (164/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,587 INFO scheduler.TaskSetManager: Starting task 169.0 in stage 9.0 (TID 186, algo-2, executor 1, partition 169, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,587 INFO scheduler.TaskSetManager: Finished task 165.0 in stage 9.0 (TID 182) in 12 ms on algo-2 (executor 1) (165/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,590 INFO scheduler.TaskSetManager: Starting task 170.0 in stage 9.0 (TID 187, algo-2, executor 1, partition 170, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,590 INFO scheduler.TaskSetManager: Finished task 166.0 in stage 9.0 (TID 183) in 10 ms on algo-2 (executor 1) (166/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,590 INFO scheduler.TaskSetManager: Starting task 171.0 in stage 9.0 (TID 188, algo-2, executor 1, partition 171, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,591 INFO scheduler.TaskSetManager: Finished task 164.0 in stage 9.0 (TID 181) in 21 ms on algo-2 (executor 1) (167/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,592 INFO scheduler.TaskSetManager: Starting task 172.0 in stage 9.0 (TID 189, algo-2, executor 1, partition 172, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,592 INFO scheduler.TaskSetManager: Finished task 167.0 in stage 9.0 (TID 184) in 10 ms on algo-2 (executor 1) (168/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,597 INFO scheduler.TaskSetManager: Starting task 173.0 in stage 9.0 (TID 190, algo-2, executor 1, partition 173, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,597 INFO scheduler.TaskSetManager: Finished task 168.0 in stage 9.0 (TID 185) in 14 ms on algo-2 (executor 1) (169/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,597 INFO scheduler.TaskSetManager: Starting task 174.0 in stage 9.0 (TID 191, algo-2, executor 1, partition 174, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,598 INFO scheduler.TaskSetManager: Finished task 169.0 in stage 9.0 (TID 186) in 10 ms on algo-2 (executor 1) (170/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,601 INFO scheduler.TaskSetManager: Starting task 175.0 in stage 9.0 (TID 192, algo-2, executor 1, partition 175, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,601 INFO scheduler.TaskSetManager: Finished task 171.0 in stage 9.0 (TID 188) in 11 ms on algo-2 (executor 1) (171/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,607 INFO scheduler.TaskSetManager: Starting task 176.0 in stage 9.0 (TID 193, algo-2, executor 1, partition 176, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,607 INFO scheduler.TaskSetManager: Finished task 173.0 in stage 9.0 (TID 190) in 10 ms on algo-2 (executor 1) (172/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,608 INFO scheduler.TaskSetManager: Starting task 177.0 in stage 9.0 (TID 194, algo-2, executor 1, partition 177, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,608 INFO scheduler.TaskSetManager: Finished task 174.0 in stage 9.0 (TID 191) in 11 ms on algo-2 (executor 1) (173/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,608 INFO scheduler.TaskSetManager: Starting task 178.0 in stage 9.0 (TID 195, algo-2, executor 1, partition 178, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,609 INFO scheduler.TaskSetManager: Finished task 172.0 in stage 9.0 (TID 189) in 17 ms on algo-2 (executor 1) (174/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,609 INFO scheduler.TaskSetManager: Starting task 179.0 in stage 9.0 (TID 196, algo-2, executor 1, partition 179, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,609 INFO scheduler.TaskSetManager: Finished task 170.0 in stage 9.0 (TID 187) in 20 ms on algo-2 (executor 1) (175/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,610 INFO scheduler.TaskSetManager: Starting task 180.0 in stage 9.0 (TID 197, algo-2, executor 1, partition 180, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,610 INFO scheduler.TaskSetManager: Finished task 175.0 in stage 9.0 (TID 192) in 9 ms on algo-2 (executor 1) (176/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,616 INFO scheduler.TaskSetManager: Starting task 181.0 in stage 9.0 (TID 198, algo-2, executor 1, partition 181, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,617 INFO scheduler.TaskSetManager: Finished task 176.0 in stage 9.0 (TID 193) in 9 ms on algo-2 (executor 1) (177/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,618 INFO scheduler.TaskSetManager: Starting task 182.0 in stage 9.0 (TID 199, algo-2, executor 1, partition 182, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,618 INFO scheduler.TaskSetManager: Finished task 177.0 in stage 9.0 (TID 194) in 11 ms on algo-2 (executor 1) (178/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,618 INFO scheduler.TaskSetManager: Starting task 183.0 in stage 9.0 (TID 200, algo-2, executor 1, partition 183, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,618 INFO scheduler.TaskSetManager: Finished task 179.0 in stage 9.0 (TID 196) in 9 ms on algo-2 (executor 1) (179/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,619 INFO scheduler.TaskSetManager: Starting task 184.0 in stage 9.0 (TID 201, algo-2, executor 1, partition 184, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,619 INFO scheduler.TaskSetManager: Finished task 180.0 in stage 9.0 (TID 197) in 9 ms on algo-2 (executor 1) (180/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,627 INFO scheduler.TaskSetManager: Starting task 185.0 in stage 9.0 (TID 202, algo-2, executor 1, partition 185, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,627 INFO scheduler.TaskSetManager: Finished task 181.0 in stage 9.0 (TID 198) in 11 ms on algo-2 (executor 1) (181/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,628 INFO scheduler.TaskSetManager: Starting task 186.0 in stage 9.0 (TID 203, algo-2, executor 1, partition 186, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,628 INFO scheduler.TaskSetManager: Finished task 183.0 in stage 9.0 (TID 200) in 10 ms on algo-2 (executor 1) (182/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,629 INFO scheduler.TaskSetManager: Starting task 187.0 in stage 9.0 (TID 204, algo-2, executor 1, partition 187, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,629 INFO scheduler.TaskSetManager: Finished task 178.0 in stage 9.0 (TID 195) in 21 ms on algo-2 (executor 1) (183/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,632 INFO scheduler.TaskSetManager: Starting task 188.0 in stage 9.0 (TID 205, algo-2, executor 1, partition 188, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,632 INFO scheduler.TaskSetManager: Finished task 184.0 in stage 9.0 (TID 201) in 13 ms on algo-2 (executor 1) (184/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,635 INFO scheduler.TaskSetManager: Starting task 189.0 in stage 9.0 (TID 206, algo-2, executor 1, partition 189, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,635 INFO scheduler.TaskSetManager: Finished task 182.0 in stage 9.0 (TID 199) in 17 ms on algo-2 (executor 1) (185/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,638 INFO scheduler.TaskSetManager: Starting task 190.0 in stage 9.0 (TID 207, algo-2, executor 1, partition 190, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,638 INFO scheduler.TaskSetManager: Finished task 185.0 in stage 9.0 (TID 202) in 11 ms on algo-2 (executor 1) (186/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,638 INFO scheduler.TaskSetManager: Starting task 191.0 in stage 9.0 (TID 208, algo-2, executor 1, partition 191, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,639 INFO scheduler.TaskSetManager: Finished task 187.0 in stage 9.0 (TID 204) in 10 ms on algo-2 (executor 1) (187/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,640 INFO scheduler.TaskSetManager: Starting task 192.0 in stage 9.0 (TID 209, algo-2, executor 1, partition 192, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,640 INFO scheduler.TaskSetManager: Finished task 186.0 in stage 9.0 (TID 203) in 12 ms on algo-2 (executor 1) (188/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,644 INFO scheduler.TaskSetManager: Starting task 193.0 in stage 9.0 (TID 210, algo-2, executor 1, partition 193, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,644 INFO scheduler.TaskSetManager: Finished task 189.0 in stage 9.0 (TID 206) in 9 ms on algo-2 (executor 1) (189/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,649 INFO scheduler.TaskSetManager: Starting task 194.0 in stage 9.0 (TID 211, algo-2, executor 1, partition 194, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,649 INFO scheduler.TaskSetManager: Finished task 188.0 in stage 9.0 (TID 205) in 17 ms on algo-2 (executor 1) (190/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,650 INFO scheduler.TaskSetManager: Starting task 195.0 in stage 9.0 (TID 212, algo-2, executor 1, partition 195, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,650 INFO scheduler.TaskSetManager: Finished task 190.0 in stage 9.0 (TID 207) in 12 ms on algo-2 (executor 1) (191/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,658 INFO scheduler.TaskSetManager: Starting task 196.0 in stage 9.0 (TID 213, algo-2, executor 1, partition 196, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,659 INFO scheduler.TaskSetManager: Finished task 192.0 in stage 9.0 (TID 209) in 19 ms on algo-2 (executor 1) (192/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,660 INFO scheduler.TaskSetManager: Starting task 197.0 in stage 9.0 (TID 214, algo-2, executor 1, partition 197, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,660 INFO scheduler.TaskSetManager: Finished task 194.0 in stage 9.0 (TID 211) in 11 ms on algo-2 (executor 1) (193/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,661 INFO scheduler.TaskSetManager: Starting task 198.0 in stage 9.0 (TID 215, algo-2, executor 1, partition 198, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,661 INFO scheduler.TaskSetManager: Finished task 193.0 in stage 9.0 (TID 210) in 17 ms on algo-2 (executor 1) (194/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,661 INFO scheduler.TaskSetManager: Starting task 199.0 in stage 9.0 (TID 216, algo-2, executor 1, partition 199, NODE_LOCAL, 7767 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,661 INFO scheduler.TaskSetManager: Finished task 195.0 in stage 9.0 (TID 212) in 11 ms on algo-2 (executor 1) (195/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,669 INFO scheduler.TaskSetManager: Finished task 191.0 in stage 9.0 (TID 208) in 31 ms on algo-2 (executor 1) (196/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,669 INFO scheduler.TaskSetManager: Finished task 196.0 in stage 9.0 (TID 213) in 11 ms on algo-2 (executor 1) (197/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,671 INFO scheduler.TaskSetManager: Finished task 199.0 in stage 9.0 (TID 216) in 10 ms on algo-2 (executor 1) (198/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,671 INFO scheduler.TaskSetManager: Finished task 197.0 in stage 9.0 (TID 214) in 11 ms on algo-2 (executor 1) (199/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,678 INFO scheduler.TaskSetManager: Finished task 198.0 in stage 9.0 (TID 215) in 18 ms on algo-2 (executor 1) (200/200)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,679 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,679 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (collect at AnalysisRunner.scala:499) finished in 0.915 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,679 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,679 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,679 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,679 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,679 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:499), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,681 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 8.5 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,683 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 4.4 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,683 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.176.27:41597 (size: 4.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,683 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,684 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:499) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,684 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,684 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 217, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,692 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-2:44167 (size: 4.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,694 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,727 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 217) in 43 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,727 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,728 INFO scheduler.DAGScheduler: ResultStage 10 (collect at AnalysisRunner.scala:499) finished in 0.048 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,728 INFO scheduler.DAGScheduler: Job 4 finished: collect at AnalysisRunner.scala:499, took 2.738171 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,789 INFO codegen.CodeGenerator: Code generated in 15.273027 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,816 INFO codegen.CodeGenerator: Code generated in 6.072303 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,824 INFO codegen.CodeGenerator: Code generated in 5.972833 ms\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|check       |check_level|check_status|constraint                                                                                                                                         |constraint_status|constraint_message|\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |SizeConstraint(Size(None))                                                                                                                         |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |MinimumConstraint(Minimum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |MaximumConstraint(Maximum(star_rating,None))                                                                                                       |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |CompletenessConstraint(Completeness(review_id,None))                                                                                               |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |UniquenessConstraint(Uniqueness(List(review_id)))                                                                                                  |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |CompletenessConstraint(Completeness(marketplace,None))                                                                                             |Success          |                  |\u001b[0m\n",
      "\u001b[34m|Review Check|Error      |Success     |ComplianceConstraint(Compliance(marketplace contained in US,UK,DE,JP,FR,`marketplace` IS NULL OR `marketplace` IN ('US','UK','DE','JP','FR'),None))|Success          |                  |\u001b[0m\n",
      "\u001b[34m+------------+-----------+------------+---------------------------------------------------------------------------------------------------------------------------------------------------+-----------------+------------------+\n",
      "\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,898 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,898 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,898 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,898 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200919193413_0000}; taskId=attempt_20200919193413_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@2704e5be}; outputPath=s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-checks, workPath=s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-checks/_temporary/0/_temporary/attempt_20200919193413_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-checks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:13,898 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,220 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:110\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,221 INFO scheduler.DAGScheduler: Registering RDD 36 (csv at preprocess-deequ.scala:110) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,221 INFO scheduler.DAGScheduler: Got job 5 (csv at preprocess-deequ.scala:110) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,221 INFO scheduler.DAGScheduler: Final stage: ResultStage 12 (csv at preprocess-deequ.scala:110)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,221 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,221 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 11)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,222 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[36] at csv at preprocess-deequ.scala:110), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,224 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 5.2 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,225 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 3.1 KB, free 364.8 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,225 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.176.27:41597 (size: 3.1 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,225 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,226 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[36] at csv at preprocess-deequ.scala:110) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,226 INFO cluster.YarnScheduler: Adding task set 11.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,226 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 218, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8156 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,227 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 11.0 (TID 219, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8172 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,227 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 11.0 (TID 220, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8341 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,227 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 11.0 (TID 221, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8180 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,227 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 11.0 (TID 222, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8448 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,234 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-2:44167 (size: 3.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,238 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 218) in 12 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,238 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 11.0 (TID 220) in 11 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,238 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 11.0 (TID 222) in 11 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,238 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 11.0 (TID 219) in 12 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,238 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 11.0 (TID 221) in 11 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,239 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,239 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (csv at preprocess-deequ.scala:110) finished in 0.017 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,239 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,239 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,239 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 12)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,239 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,239 INFO scheduler.DAGScheduler: Submitting ResultStage 12 (ShuffledRowRDD[37] at csv at preprocess-deequ.scala:110), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,260 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 245.3 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,262 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 90.5 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,262 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.176.27:41597 (size: 90.5 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,262 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,263 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (ShuffledRowRDD[37] at csv at preprocess-deequ.scala:110) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,263 INFO cluster.YarnScheduler: Adding task set 12.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,263 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 12.0 (TID 223, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,270 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-2:44167 (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:14,281 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:15,865 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 12.0 (TID 223) in 1602 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:15,865 INFO cluster.YarnScheduler: Removed TaskSet 12.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:15,866 INFO scheduler.DAGScheduler: ResultStage 12 (csv at preprocess-deequ.scala:110) finished in 1.627 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:15,866 INFO scheduler.DAGScheduler: Job 5 finished: csv at preprocess-deequ.scala:110, took 1.645815 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,398 INFO datasources.FileFormatWriter: Write Job 3af7c0fb-0743-4f5d-be08-3083b64cb0ce committed.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,398 INFO datasources.FileFormatWriter: Finished processing stats for write job 3af7c0fb-0743-4f5d-be08-3083b64cb0ce.\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|entity |instance                               |name        |value   |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m|Column |review_id                              |Uniqueness  |1.0     |\u001b[0m\n",
      "\u001b[34m|Dataset|*                                      |Size        |247515.0|\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Maximum     |5.0     |\u001b[0m\n",
      "\u001b[34m|Column |star_rating                            |Minimum     |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace contained in US,UK,DE,JP,FR|Compliance  |1.0     |\u001b[0m\n",
      "\u001b[34m|Column |marketplace                            |Completeness|1.0     |\u001b[0m\n",
      "\u001b[34m+-------+---------------------------------------+------------+--------+\n",
      "\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,513 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,513 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,513 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,514 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200919193416_0000}; taskId=attempt_20200919193416_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@68f74bb}; outputPath=s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/success-metrics, workPath=s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/success-metrics/_temporary/0/_temporary/attempt_20200919193416_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/success-metrics\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,514 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,899 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:122\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,900 INFO scheduler.DAGScheduler: Registering RDD 42 (csv at preprocess-deequ.scala:122) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,901 INFO scheduler.DAGScheduler: Got job 6 (csv at preprocess-deequ.scala:122) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,901 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (csv at preprocess-deequ.scala:122)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,901 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,901 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 13)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,901 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[42] at csv at preprocess-deequ.scala:122), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,902 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,903 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KB, free 364.5 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,904 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.176.27:41597 (size: 3.0 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,904 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,904 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[42] at csv at preprocess-deequ.scala:122) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,904 INFO cluster.YarnScheduler: Adding task set 13.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,905 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 224, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,905 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 13.0 (TID 225, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8108 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,905 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 13.0 (TID 226, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8181 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,905 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 13.0 (TID 227, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8100 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,905 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 13.0 (TID 228, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8229 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,914 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-2:44167 (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,927 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 224) in 22 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,927 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 13.0 (TID 228) in 22 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,927 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 13.0 (TID 227) in 22 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,927 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 13.0 (TID 226) in 22 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 13.0 (TID 225) in 22 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO scheduler.DAGScheduler: ShuffleMapStage 13 (csv at preprocess-deequ.scala:122) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 14)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (ShuffledRowRDD[43] at csv at preprocess-deequ.scala:122), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 362\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 312\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 202\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 297\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 234\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 239\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 250\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 195\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 260\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,928 INFO spark.ContextCleaner: Cleaned accumulator 340\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,929 INFO spark.ContextCleaner: Cleaned shuffle 6\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,929 INFO spark.ContextCleaner: Cleaned accumulator 412\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 308\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 322\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 339\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 367\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 255\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 310\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 301\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 404\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 327\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 372\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 216\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 218\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 351\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 232\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 233\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 237\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 394\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 316\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 419\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 206\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 247\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 346\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 295\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 375\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 315\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 246\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,930 INFO spark.ContextCleaner: Cleaned accumulator 336\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,931 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.176.27:41597 in memory (size: 3.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,933 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-2:44167 in memory (size: 3.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,935 INFO spark.ContextCleaner: Cleaned accumulator 402\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,936 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.176.27:41597 in memory (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,938 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-2:44167 in memory (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 245\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 401\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 376\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 215\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 390\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 302\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 304\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 378\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 199\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 243\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 226\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 227\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 319\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 244\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 254\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 357\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 263\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 294\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 323\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 222\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 259\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 374\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 391\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 423\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 343\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 373\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 275\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 204\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 230\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 331\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 400\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 284\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 261\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 303\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 338\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 387\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 332\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 267\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 421\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,940 INFO spark.ContextCleaner: Cleaned accumulator 291\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,941 INFO spark.ContextCleaner: Cleaned shuffle 5\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,945 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-2:44167 in memory (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,949 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.176.27:41597 in memory (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,950 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 245.2 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,951 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 90.5 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,952 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.176.27:41597 (size: 90.5 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,952 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,952 INFO spark.ContextCleaner: Cleaned accumulator 200\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,952 INFO spark.ContextCleaner: Cleaned accumulator 361\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,952 INFO spark.ContextCleaner: Cleaned accumulator 383\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,952 INFO spark.ContextCleaner: Cleaned accumulator 214\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,952 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (ShuffledRowRDD[43] at csv at preprocess-deequ.scala:122) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,952 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,953 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 229, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,953 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.176.27:41597 in memory (size: 90.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,955 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-2:44167 in memory (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,958 INFO spark.ContextCleaner: Cleaned accumulator 258\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,958 INFO spark.ContextCleaner: Cleaned accumulator 326\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,958 INFO spark.ContextCleaner: Cleaned accumulator 253\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,958 INFO spark.ContextCleaner: Cleaned accumulator 211\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,958 INFO spark.ContextCleaner: Cleaned accumulator 298\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,959 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.176.27:41597 in memory (size: 4.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,961 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-2:44167 in memory (size: 4.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,961 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-2:44167 (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,963 INFO spark.ContextCleaner: Cleaned accumulator 427\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,963 INFO spark.ContextCleaner: Cleaned accumulator 278\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,963 INFO spark.ContextCleaner: Cleaned accumulator 366\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,963 INFO spark.ContextCleaner: Cleaned accumulator 257\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,963 INFO spark.ContextCleaner: Cleaned accumulator 201\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,963 INFO spark.ContextCleaner: Cleaned accumulator 241\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,963 INFO spark.ContextCleaner: Cleaned accumulator 269\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,963 INFO spark.ContextCleaner: Cleaned accumulator 410\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,963 INFO spark.ContextCleaner: Cleaned accumulator 277\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,964 INFO spark.ContextCleaner: Cleaned shuffle 3\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,964 INFO spark.ContextCleaner: Cleaned accumulator 273\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,964 INFO spark.ContextCleaner: Cleaned accumulator 306\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,964 INFO spark.ContextCleaner: Cleaned accumulator 314\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,964 INFO spark.ContextCleaner: Cleaned accumulator 289\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,965 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.176.27:41597 in memory (size: 6.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,966 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-2:44167 in memory (size: 6.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 248\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 225\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 392\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 425\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 324\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 352\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 279\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 369\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 220\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 381\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 219\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 270\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 393\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 221\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 281\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 330\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 288\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 203\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 252\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 198\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 242\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,970 INFO spark.ContextCleaner: Cleaned accumulator 335\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 398\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 334\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 411\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 205\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 333\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 424\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 210\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 380\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 229\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 349\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 262\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 307\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 292\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 363\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 417\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 290\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 350\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 382\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 212\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 196\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 300\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 266\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 317\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 305\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 256\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 388\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 396\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 405\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 311\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 293\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 409\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 209\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 353\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 283\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 407\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 264\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 360\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 415\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 280\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 240\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,971 INFO spark.ContextCleaner: Cleaned accumulator 341\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,972 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.176.27:41597 in memory (size: 3.1 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,973 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-2:44167 in memory (size: 3.1 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,976 INFO spark.ContextCleaner: Cleaned accumulator 414\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,976 INFO spark.ContextCleaner: Cleaned accumulator 395\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,976 INFO spark.ContextCleaner: Cleaned accumulator 313\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,976 INFO spark.ContextCleaner: Cleaned accumulator 368\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,977 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.176.27:41597 in memory (size: 14.2 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,978 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,978 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-2:44167 in memory (size: 14.2 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 397\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 370\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 299\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 408\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 207\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 217\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 251\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 347\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 364\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 285\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 377\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 249\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 328\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 355\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 265\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 320\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 337\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,980 INFO spark.ContextCleaner: Cleaned accumulator 321\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,981 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.176.27:41597 in memory (size: 12.7 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,982 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-2:44167 in memory (size: 12.7 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 325\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 389\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 271\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 386\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 208\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 238\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 276\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 286\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 342\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 371\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 358\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 213\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 399\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 422\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 274\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 228\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 418\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 197\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 426\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 231\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 379\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,986 INFO spark.ContextCleaner: Cleaned accumulator 318\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned shuffle 4\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 345\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 224\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 348\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 356\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 420\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 354\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 385\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 428\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 223\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 344\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 235\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 236\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 329\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 296\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 282\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 365\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 384\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 309\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 403\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 268\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 272\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 413\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 406\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 359\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:16,987 INFO spark.ContextCleaner: Cleaned accumulator 287\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,277 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 229) in 1324 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,277 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,278 INFO scheduler.DAGScheduler: ResultStage 14 (csv at preprocess-deequ.scala:122) finished in 1.349 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,278 INFO scheduler.DAGScheduler: Job 6 finished: csv at preprocess-deequ.scala:122, took 1.378541 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,791 INFO datasources.FileFormatWriter: Write Job 01f0ded1-4879-4dfb-b2be-bf6b4fddb0aa committed.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,791 INFO datasources.FileFormatWriter: Finished processing stats for write job 01f0ded1-4879-4dfb-b2be-bf6b4fddb0aa.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,953 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,954 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,954 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,954 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:18,993 INFO codegen.CodeGenerator: Code generated in 11.240193 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,000 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 400.9 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,011 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 42.9 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,012 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.176.27:41597 (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,012 INFO spark.SparkContext: Created broadcast 19 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,012 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,076 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,077 INFO scheduler.DAGScheduler: Registering RDD 49 (collect at AnalysisRunner.scala:303) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,077 INFO scheduler.DAGScheduler: Got job 7 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,077 INFO scheduler.DAGScheduler: Final stage: ResultStage 16 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,077 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,077 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 15)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,077 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,083 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 143.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,084 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 45.5 KB, free 364.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,085 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.176.27:41597 (size: 45.5 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,085 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,086 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[49] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,086 INFO cluster.YarnScheduler: Adding task set 15.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,086 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 230, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,086 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 15.0 (TID 231, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,095 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-2:44167 (size: 45.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:19,131 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-2:44167 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-09-19 19:34:22,642 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 15.0 (TID 231) in 3556 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,398 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 230) in 4312 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,398 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,399 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:303) finished in 4.320 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,399 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,399 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,399 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 16)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,399 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,399 INFO scheduler.DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,403 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 174.5 KB, free 364.7 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,405 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 56.4 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,406 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.176.27:41597 (size: 56.4 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,406 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,406 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[52] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,406 INFO cluster.YarnScheduler: Adding task set 16.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,407 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 232, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,413 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-2:44167 (size: 56.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,418 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,594 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 232) in 187 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,594 INFO cluster.YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,595 INFO scheduler.DAGScheduler: ResultStage 16 (collect at AnalysisRunner.scala:303) finished in 0.196 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,595 INFO scheduler.DAGScheduler: Job 7 finished: collect at AnalysisRunner.scala:303, took 4.518966 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,737 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,737 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,737 INFO datasources.FileSourceStrategy: Output Data Schema: struct<customer_id: string, product_parent: string, star_rating: int, helpful_votes: int, total_votes: int ... 3 more fields>\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,737 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,792 INFO codegen.CodeGenerator: Code generated in 23.815701 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,804 INFO codegen.CodeGenerator: Code generated in 7.240912 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,809 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 400.9 KB, free 364.3 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,820 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 42.9 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,820 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.176.27:41597 (size: 42.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,821 INFO spark.SparkContext: Created broadcast 22 from collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,821 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,842 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:303\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,843 INFO scheduler.DAGScheduler: Registering RDD 56 (collect at AnalysisRunner.scala:303) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,843 INFO scheduler.DAGScheduler: Got job 8 (collect at AnalysisRunner.scala:303) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,843 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collect at AnalysisRunner.scala:303)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,843 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,843 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,843 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 17 (MapPartitionsRDD[56] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,845 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.2 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,846 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 13.9 KB, free 364.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,846 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.176.27:41597 (size: 13.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,847 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,847 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 17 (MapPartitionsRDD[56] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,847 INFO cluster.YarnScheduler: Adding task set 17.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,847 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 233, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,848 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 17.0 (TID 234, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,854 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-2:44167 (size: 13.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:23,889 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-2:44167 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:25,670 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 17.0 (TID 234) in 1823 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,127 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 233) in 2280 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,127 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,127 INFO scheduler.DAGScheduler: ShuffleMapStage 17 (collect at AnalysisRunner.scala:303) finished in 2.283 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,127 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,127 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,127 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 18)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,127 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,127 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[59] at collect at AnalysisRunner.scala:303), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,129 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 38.2 KB, free 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,130 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 15.5 KB, free 364.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,131 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.176.27:41597 (size: 15.5 KB, free: 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,131 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,131 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[59] at collect at AnalysisRunner.scala:303) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,131 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,132 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 235, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,138 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-2:44167 (size: 15.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,142 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,243 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 235) in 111 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,243 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,243 INFO scheduler.DAGScheduler: ResultStage 18 (collect at AnalysisRunner.scala:303) finished in 0.115 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,243 INFO scheduler.DAGScheduler: Job 8 finished: collect at AnalysisRunner.scala:303, took 2.401390 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,281 INFO datasources.FileSourceStrategy: Pruning directories with: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,282 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,282 INFO datasources.FileSourceStrategy: Output Data Schema: struct<marketplace: string, customer_id: string, review_id: string, product_id: string, product_parent: string ... 13 more fields>\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,282 INFO execution.FileSourceScanExec: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,287 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 400.9 KB, free 363.7 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,302 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 42.9 KB, free 363.7 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,303 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.176.27:41597 (size: 42.9 KB, free: 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,303 INFO spark.SparkContext: Created broadcast 25 from rdd at ColumnProfiler.scala:533\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,303 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 10965763 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,324 INFO spark.ContextCleaner: Cleaned accumulator 513\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,324 INFO spark.ContextCleaner: Cleaned accumulator 441\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,326 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.176.27:41597 in memory (size: 3.0 KB, free: 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,328 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-2:44167 in memory (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,330 INFO spark.ContextCleaner: Cleaned accumulator 462\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,330 INFO spark.ContextCleaner: Cleaned accumulator 551\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,330 INFO spark.ContextCleaner: Cleaned accumulator 460\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,330 INFO spark.ContextCleaner: Cleaned accumulator 589\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,330 INFO spark.ContextCleaner: Cleaned accumulator 520\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,330 INFO spark.ContextCleaner: Cleaned accumulator 459\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,331 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.176.27:41597 in memory (size: 45.5 KB, free: 365.9 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,332 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-2:44167 in memory (size: 45.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 544\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 466\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 538\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 451\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 481\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 535\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 565\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 583\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 495\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 607\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 611\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 609\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 482\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 501\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 557\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 490\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 554\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,335 INFO spark.ContextCleaner: Cleaned accumulator 523\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,338 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.176.27:41597 in memory (size: 42.9 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,338 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-2:44167 in memory (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,340 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:547\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,342 INFO scheduler.DAGScheduler: Registering RDD 66 (countByKey at ColumnProfiler.scala:547) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,342 INFO scheduler.DAGScheduler: Got job 9 (countByKey at ColumnProfiler.scala:547) with 2 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,342 INFO scheduler.DAGScheduler: Final stage: ResultStage 20 (countByKey at ColumnProfiler.scala:547)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,342 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,343 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 19)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,343 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 19 (MapPartitionsRDD[66] at countByKey at ColumnProfiler.scala:547), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 540\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 512\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 453\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 605\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 465\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 432\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 599\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 558\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 519\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 447\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 600\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 572\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 587\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 585\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 511\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 569\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 612\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 604\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 596\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 561\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 442\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 507\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 503\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 528\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 594\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 570\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 552\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 469\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 603\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,345 INFO spark.ContextCleaner: Cleaned accumulator 514\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,347 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.176.27:41597 in memory (size: 15.5 KB, free: 366.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,348 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-2:44167 in memory (size: 15.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,352 INFO spark.ContextCleaner: Cleaned accumulator 559\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,352 INFO spark.ContextCleaner: Cleaned accumulator 498\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,352 INFO spark.ContextCleaner: Cleaned accumulator 602\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,352 INFO spark.ContextCleaner: Cleaned accumulator 531\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,352 INFO spark.ContextCleaner: Cleaned accumulator 530\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,352 INFO spark.ContextCleaner: Cleaned accumulator 534\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,352 INFO spark.ContextCleaner: Cleaned accumulator 476\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,352 INFO spark.ContextCleaner: Cleaned accumulator 430\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,354 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.176.27:41597 in memory (size: 56.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,354 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-2:44167 in memory (size: 56.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,354 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 18.5 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,355 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 9.4 KB, free 364.6 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 473\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 553\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 480\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 574\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.176.27:41597 (size: 9.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 580\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 509\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 499\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 575\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 493\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 435\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned shuffle 8\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,356 INFO spark.ContextCleaner: Cleaned accumulator 489\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,362 INFO spark.ContextCleaner: Cleaned shuffle 9\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,362 INFO spark.ContextCleaner: Cleaned accumulator 525\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 527\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 455\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 487\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 471\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 488\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 522\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 543\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 436\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 472\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 563\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 429\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 505\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 608\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 529\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 461\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 581\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 568\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 19 (MapPartitionsRDD[66] at countByKey at ColumnProfiler.scala:547) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 591\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 492\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 483\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 504\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 541\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO cluster.YarnScheduler: Adding task set 19.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 537\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 578\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 440\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 586\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 533\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 550\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 438\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 458\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 437\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 443\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 518\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 524\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 431\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 606\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 445\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 577\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 456\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 446\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 485\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,363 INFO spark.ContextCleaner: Cleaned accumulator 560\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,364 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 236, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8328 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,364 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 19.0 (TID 237, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8325 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,365 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.176.27:41597 in memory (size: 13.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,365 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-2:44167 in memory (size: 13.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 546\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 474\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 500\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 526\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 477\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 496\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 479\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 449\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 478\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 564\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 593\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 439\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,368 INFO spark.ContextCleaner: Cleaned accumulator 454\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,369 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.176.27:41597 in memory (size: 42.9 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,369 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-2:44167 (size: 9.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,370 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-2:44167 in memory (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 610\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 467\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 542\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 516\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 502\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 506\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 548\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 595\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 579\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 450\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 457\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 590\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 521\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 515\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 491\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 433\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 555\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 592\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 464\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 484\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 576\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 582\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 517\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 547\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 510\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 532\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 588\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 470\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 566\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 536\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 486\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 508\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 545\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 497\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,371 INFO spark.ContextCleaner: Cleaned accumulator 584\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,372 INFO spark.ContextCleaner: Cleaned accumulator 549\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,373 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.176.27:41597 in memory (size: 90.5 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,373 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-2:44167 in memory (size: 90.5 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 452\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 475\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 448\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 562\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 598\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 494\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 539\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 601\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 434\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 463\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned shuffle 7\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 468\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 567\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 556\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 573\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 597\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 571\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:26,376 INFO spark.ContextCleaner: Cleaned accumulator 444\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:27,198 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-2:44167 (size: 42.9 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,038 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 19.0 (TID 237) in 2674 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,696 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 236) in 3333 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,696 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,696 INFO scheduler.DAGScheduler: ShuffleMapStage 19 (countByKey at ColumnProfiler.scala:547) finished in 3.353 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,696 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,696 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,696 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 20)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,696 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,697 INFO scheduler.DAGScheduler: Submitting ResultStage 20 (ShuffledRDD[67] at countByKey at ColumnProfiler.scala:547), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,697 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 3.0 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,698 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1795.0 B, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,699 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.176.27:41597 (size: 1795.0 B, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,699 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,699 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 20 (ShuffledRDD[67] at countByKey at ColumnProfiler.scala:547) (first 15 tasks are for partitions Vector(0, 1))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,699 INFO cluster.YarnScheduler: Adding task set 20.0 with 2 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,700 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 238, algo-2, executor 1, partition 0, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,700 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 20.0 (TID 239, algo-2, executor 1, partition 1, NODE_LOCAL, 7673 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,707 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-2:44167 (size: 1795.0 B, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,712 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,724 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 238) in 24 ms on algo-2 (executor 1) (1/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,724 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 20.0 (TID 239) in 24 ms on algo-2 (executor 1) (2/2)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,724 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,724 INFO scheduler.DAGScheduler: ResultStage 20 (countByKey at ColumnProfiler.scala:547) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,725 INFO scheduler.DAGScheduler: Job 9 finished: countByKey at ColumnProfiler.scala:547, took 3.384301 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,810 INFO codegen.CodeGenerator: Code generated in 11.198407 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,828 INFO codegen.CodeGenerator: Code generated in 6.069118 ms\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,834 INFO codegen.CodeGenerator: Code generated in 4.313357 ms\u001b[0m\n",
      "\u001b[34m+----------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|_1              |_2                                                                          |_3                                                                                  |\u001b[0m\n",
      "\u001b[34m+----------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34m|review_id       |'review_id' is not null                                                     |.isComplete(\"review_id\")                                                            |\u001b[0m\n",
      "\u001b[34m|customer_id     |'customer_id' is not null                                                   |.isComplete(\"customer_id\")                                                          |\u001b[0m\n",
      "\u001b[34m|customer_id     |'customer_id' has type Integral                                             |.hasDataType(\"customer_id\", ConstrainableDataTypes.Integral)                        |\u001b[0m\n",
      "\u001b[34m|customer_id     |'customer_id' has no negative values                                        |.isNonNegative(\"customer_id\")                                                       |\u001b[0m\n",
      "\u001b[34m|review_date     |'review_date' is not null                                                   |.isComplete(\"review_date\")                                                          |\u001b[0m\n",
      "\u001b[34m|helpful_votes   |'helpful_votes' is not null                                                 |.isComplete(\"helpful_votes\")                                                        |\u001b[0m\n",
      "\u001b[34m|helpful_votes   |'helpful_votes' has no negative values                                      |.isNonNegative(\"helpful_votes\")                                                     |\u001b[0m\n",
      "\u001b[34m|star_rating     |'star_rating' is not null                                                   |.isComplete(\"star_rating\")                                                          |\u001b[0m\n",
      "\u001b[34m|star_rating     |'star_rating' has no negative values                                        |.isNonNegative(\"star_rating\")                                                       |\u001b[0m\n",
      "\u001b[34m|product_title   |'product_title' is not null                                                 |.isComplete(\"product_title\")                                                        |\u001b[0m\n",
      "\u001b[34m|review_headline |'review_headline' is not null                                               |.isComplete(\"review_headline\")                                                      |\u001b[0m\n",
      "\u001b[34m|product_id      |'product_id' is not null                                                    |.isComplete(\"product_id\")                                                           |\u001b[0m\n",
      "\u001b[34m|total_votes     |'total_votes' is not null                                                   |.isComplete(\"total_votes\")                                                          |\u001b[0m\n",
      "\u001b[34m|total_votes     |'total_votes' has no negative values                                        |.isNonNegative(\"total_votes\")                                                       |\u001b[0m\n",
      "\u001b[34m|product_category|'product_category' is not null                                              |.isComplete(\"product_category\")                                                     |\u001b[0m\n",
      "\u001b[34m|product_category|'product_category' has value range 'Digital_Video_Games', 'Digital_Software'|.isContainedIn(\"product_category\", Array(\"Digital_Video_Games\", \"Digital_Software\"))|\u001b[0m\n",
      "\u001b[34m|product_parent  |'product_parent' is not null                                                |.isComplete(\"product_parent\")                                                       |\u001b[0m\n",
      "\u001b[34m|product_parent  |'product_parent' has type Integral                                          |.hasDataType(\"product_parent\", ConstrainableDataTypes.Integral)                     |\u001b[0m\n",
      "\u001b[34m|product_parent  |'product_parent' has no negative values                                     |.isNonNegative(\"product_parent\")                                                    |\u001b[0m\n",
      "\u001b[34m|review_body     |'review_body' has less than 1% missing values                               |.hasCompleteness(\"review_body\", _ >= 0.99, Some(\"It should be above 0.99!\"))        |\u001b[0m\n",
      "\u001b[34m+----------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[34monly showing top 20 rows\n",
      "\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,954 WARN commit.AbstractS3ACommitterFactory: Using standard FileOutputCommitter to commit work. This is slow and potentially unsafe.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,955 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,955 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,955 INFO commit.AbstractS3ACommitterFactory: Using Commmitter FileOutputCommitter{PathOutputCommitter{context=TaskAttemptContextImpl{JobContextImpl{jobId=job_20200919193429_0000}; taskId=attempt_20200919193429_0000_m_000000_0, status=''}; org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter@680c363}; outputPath=s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-suggestions, workPath=s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-suggestions/_temporary/0/_temporary/attempt_20200919193429_0000_m_000000_0, algorithmVersion=2, skipCleanup=false, ignoreCleanupFailures=false} for s3a://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-suggestions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:29,955 INFO datasources.SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,339 INFO spark.SparkContext: Starting job: csv at preprocess-deequ.scala:151\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,339 INFO scheduler.DAGScheduler: Registering RDD 70 (csv at preprocess-deequ.scala:151) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,340 INFO scheduler.DAGScheduler: Got job 10 (csv at preprocess-deequ.scala:151) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,340 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (csv at preprocess-deequ.scala:151)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,340 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,340 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,340 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 21 (MapPartitionsRDD[70] at csv at preprocess-deequ.scala:151), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,341 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.0 KB, free 365.4 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,342 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.0 KB, free 365.3 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,342 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.176.27:41597 (size: 3.0 KB, free: 366.2 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,343 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,343 INFO scheduler.DAGScheduler: Submitting 5 missing tasks from ShuffleMapStage 21 (MapPartitionsRDD[70] at csv at preprocess-deequ.scala:151) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,343 INFO cluster.YarnScheduler: Adding task set 21.0 with 5 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,344 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 21.0 (TID 240, algo-2, executor 1, partition 0, PROCESS_LOCAL, 8680 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,344 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 21.0 (TID 241, algo-2, executor 1, partition 1, PROCESS_LOCAL, 8672 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,344 INFO scheduler.TaskSetManager: Starting task 2.0 in stage 21.0 (TID 242, algo-2, executor 1, partition 2, PROCESS_LOCAL, 8656 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,344 INFO scheduler.TaskSetManager: Starting task 3.0 in stage 21.0 (TID 243, algo-2, executor 1, partition 3, PROCESS_LOCAL, 8872 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,344 INFO scheduler.TaskSetManager: Starting task 4.0 in stage 21.0 (TID 244, algo-2, executor 1, partition 4, PROCESS_LOCAL, 8841 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,353 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-2:44167 (size: 3.0 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,356 INFO scheduler.TaskSetManager: Finished task 1.0 in stage 21.0 (TID 241) in 12 ms on algo-2 (executor 1) (1/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,357 INFO scheduler.TaskSetManager: Finished task 3.0 in stage 21.0 (TID 243) in 13 ms on algo-2 (executor 1) (2/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,357 INFO scheduler.TaskSetManager: Finished task 2.0 in stage 21.0 (TID 242) in 13 ms on algo-2 (executor 1) (3/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,357 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 21.0 (TID 240) in 14 ms on algo-2 (executor 1) (4/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,357 INFO scheduler.TaskSetManager: Finished task 4.0 in stage 21.0 (TID 244) in 13 ms on algo-2 (executor 1) (5/5)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,357 INFO cluster.YarnScheduler: Removed TaskSet 21.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,358 INFO scheduler.DAGScheduler: ShuffleMapStage 21 (csv at preprocess-deequ.scala:151) finished in 0.016 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,358 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,358 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,358 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 22)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,358 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,358 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (ShuffledRowRDD[71] at csv at preprocess-deequ.scala:151), which has no missing parents\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,378 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 245.1 KB, free 365.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,380 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 90.4 KB, free 365.0 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,380 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.176.27:41597 (size: 90.4 KB, free: 366.1 MB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,380 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1163\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,381 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (ShuffledRowRDD[71] at csv at preprocess-deequ.scala:151) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,381 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,381 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 245, algo-2, executor 1, partition 0, NODE_LOCAL, 7778 bytes)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,387 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-2:44167 (size: 90.4 KB, free: 24.1 GB)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:30,397 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.131.52:52416\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:31,517 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 245) in 1136 ms on algo-2 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:31,517 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:31,518 INFO scheduler.DAGScheduler: ResultStage 22 (csv at preprocess-deequ.scala:151) finished in 1.160 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:31,518 INFO scheduler.DAGScheduler: Job 10 finished: csv at preprocess-deequ.scala:151, took 1.179066 s\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,175 INFO datasources.FileFormatWriter: Write Job 234f7f6f-785c-4106-903a-c901ab503a4d committed.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,175 INFO datasources.FileFormatWriter: Finished processing stats for write job 234f7f6f-785c-4106-903a-c901ab503a4d.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m2020-09-19 19:34:32,207 INFO spark.SparkContext: Invoking stop() from shutdown hook\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,211 INFO server.AbstractConnector: Stopped Spark@74f1bae5{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,213 INFO ui.SparkUI: Stopped Spark web UI at http://10.0.176.27:4040\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,217 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,234 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,234 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,237 INFO cluster.SchedulerExtensionServices: Stopping SchedulerExtensionServices\u001b[0m\n",
      "\u001b[34m(serviceOption=None,\n",
      " services=List(),\n",
      " started=false)\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,238 INFO cluster.YarnClientSchedulerBackend: Stopped\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,242 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,250 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,252 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,253 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,260 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,267 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,267 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,268 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f327819a-d20f-4813-9ae7-c1a9a1bedb14\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,270 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-de9dbabb-2257-4de0-a0df-f3ecaa0e5c72\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,272 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f327819a-d20f-4813-9ae7-c1a9a1bedb14/pyspark-fe20d329-7c1c-4999-8de7-c2debd1de414\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,275 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,276 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\u001b[0m\n",
      "\u001b[34m2020-09-19 19:34:32,276 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\u001b[0m\n",
      "\u001b[35m2020-09-19 19:34:33\u001b[0m\n",
      "\u001b[35mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\u001b[35mReceived end of job signal, exiting...\u001b[0m\n",
      "\u001b[34mFinished Yarn configuration files setup.\n",
      "\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "running_processor.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the Processed Output \n",
    "\n",
    "## These are the quality checks on our dataset.\n",
    "\n",
    "## _The next cells will not work properly until the job completes above._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-09-19 19:34:17          0 amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-checks/_SUCCESS\r\n",
      "2020-09-19 19:34:16        768 amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-checks/part-00000-22187485-4b51-47ac-b112-ce04b05728ee-c000.csv\r\n",
      "2020-09-19 19:34:32          0 amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-suggestions/_SUCCESS\r\n",
      "2020-09-19 19:34:32       2289 amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-suggestions/part-00000-fe1487a3-8f4c-4db9-a821-462ddc759542-c000.csv\r\n",
      "2020-09-19 19:34:08          0 amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/dataset-metrics/_SUCCESS\r\n",
      "2020-09-19 19:34:08        364 amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/dataset-metrics/part-00000-6bfff2a4-c345-43cc-bb20-41c9020e523a-c000.csv\r\n",
      "2020-09-19 19:34:19          0 amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/success-metrics/_SUCCESS\r\n",
      "2020-09-19 19:34:18        277 amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/success-metrics/part-00000-6331d7fd-ba89-4f89-8191-73b8603a521a-c000.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls --recursive $s3_output_analyze_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Output from S3 to Local\n",
    "* dataset-metrics/\n",
    "* constraint-checks/\n",
    "* success-metrics/\n",
    "* constraint-suggestions/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-checks/part-00000-22187485-4b51-47ac-b112-ce04b05728ee-c000.csv to amazon-reviews-spark-analyzer/constraint-checks/part-00000-22187485-4b51-47ac-b112-ce04b05728ee-c000.csv\n",
      "download: s3://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/constraint-suggestions/part-00000-fe1487a3-8f4c-4db9-a821-462ddc759542-c000.csv to amazon-reviews-spark-analyzer/constraint-suggestions/part-00000-fe1487a3-8f4c-4db9-a821-462ddc759542-c000.csv\n",
      "download: s3://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/success-metrics/part-00000-6331d7fd-ba89-4f89-8191-73b8603a521a-c000.csv to amazon-reviews-spark-analyzer/success-metrics/part-00000-6331d7fd-ba89-4f89-8191-73b8603a521a-c000.csv\n",
      "download: s3://sagemaker-us-east-1-889926741212/amazon-reviews-spark-analyzer-2020-09-19-19-29-30/output/dataset-metrics/part-00000-6bfff2a4-c345-43cc-bb20-41c9020e523a-c000.csv to amazon-reviews-spark-analyzer/dataset-metrics/part-00000-6bfff2a4-c345-43cc-bb20-41c9020e523a-c000.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $s3_output_analyze_data ./amazon-reviews-spark-analyzer/ --exclude=\"*\" --include=\"*.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_dataset(path, sep, header):\n",
    "    data = pd.concat([pd.read_csv(f, sep=sep, header=header) for f in glob.glob('{}/*.csv'.format(path))], ignore_index = True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>check</th>\n",
       "      <th>constraint</th>\n",
       "      <th>constraint_status</th>\n",
       "      <th>constraint_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>SizeConstraint(Size(None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>MinimumConstraint(Minimum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>MaximumConstraint(Maximum(star_rating,None))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>CompletenessConstraint(Completeness(review_id,...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>UniquenessConstraint(Uniqueness(List(review_id)))</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>CompletenessConstraint(Completeness(marketplac...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Review Check</td>\n",
       "      <td>ComplianceConstraint(Compliance(marketplace co...</td>\n",
       "      <td>Success</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          check                                         constraint  \\\n",
       "0  Review Check                         SizeConstraint(Size(None))   \n",
       "1  Review Check       MinimumConstraint(Minimum(star_rating,None))   \n",
       "2  Review Check       MaximumConstraint(Maximum(star_rating,None))   \n",
       "3  Review Check  CompletenessConstraint(Completeness(review_id,...   \n",
       "4  Review Check  UniquenessConstraint(Uniqueness(List(review_id)))   \n",
       "5  Review Check  CompletenessConstraint(Completeness(marketplac...   \n",
       "6  Review Check  ComplianceConstraint(Compliance(marketplace co...   \n",
       "\n",
       "  constraint_status  constraint_message  \n",
       "0           Success                 NaN  \n",
       "1           Success                 NaN  \n",
       "2           Success                 NaN  \n",
       "3           Success                 NaN  \n",
       "4           Success                 NaN  \n",
       "5           Success                 NaN  \n",
       "6           Success                 NaN  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_constraint_checks = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-checks/', sep='\\t', header=0)\n",
    "df_constraint_checks[['check', 'constraint', 'constraint_status', 'constraint_message']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Dataset Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>ApproxCountDistinct</td>\n",
       "      <td>238027.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,star_rating</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>-0.080881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>247515.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Mean</td>\n",
       "      <td>3.723706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>top star_rating</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>0.663338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mutlicolumn</td>\n",
       "      <td>total_votes,helpful_votes</td>\n",
       "      <td>Correlation</td>\n",
       "      <td>0.980529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        entity                   instance                 name          value\n",
       "0       Column                  review_id         Completeness       1.000000\n",
       "1       Column                  review_id  ApproxCountDistinct  238027.000000\n",
       "2  Mutlicolumn    total_votes,star_rating          Correlation      -0.080881\n",
       "3      Dataset                          *                 Size  247515.000000\n",
       "4       Column                star_rating                 Mean       3.723706\n",
       "5       Column            top star_rating           Compliance       0.663338\n",
       "6  Mutlicolumn  total_votes,helpful_votes          Correlation       0.980529"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/dataset-metrics/', sep='\\t', header=0)\n",
    "df_dataset_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Success Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>instance</th>\n",
       "      <th>name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Column</td>\n",
       "      <td>review_id</td>\n",
       "      <td>Uniqueness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dataset</td>\n",
       "      <td>*</td>\n",
       "      <td>Size</td>\n",
       "      <td>247515.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Maximum</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Column</td>\n",
       "      <td>star_rating</td>\n",
       "      <td>Minimum</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace contained in US,UK,DE,JP,FR</td>\n",
       "      <td>Compliance</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Column</td>\n",
       "      <td>marketplace</td>\n",
       "      <td>Completeness</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    entity                                 instance          name     value\n",
       "0   Column                                review_id  Completeness       1.0\n",
       "1   Column                                review_id    Uniqueness       1.0\n",
       "2  Dataset                                        *          Size  247515.0\n",
       "3   Column                              star_rating       Maximum       5.0\n",
       "4   Column                              star_rating       Minimum       1.0\n",
       "5   Column  marketplace contained in US,UK,DE,JP,FR    Compliance       1.0\n",
       "6   Column                              marketplace  Completeness       1.0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_success_metrics = load_dataset(path='./amazon-reviews-spark-analyzer/success-metrics/', sep='\\t', header=0)\n",
    "df_success_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Constraint Suggestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column_name</th>\n",
       "      <th>description</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>review_id</td>\n",
       "      <td>'review_id' is not null</td>\n",
       "      <td>.isComplete(\\review_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' is not null</td>\n",
       "      <td>.isComplete(\\customer_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' has type Integral</td>\n",
       "      <td>.hasDataType(\\customer_id\\\", ConstrainableData...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>customer_id</td>\n",
       "      <td>'customer_id' has no negative values</td>\n",
       "      <td>.isNonNegative(\\customer_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>review_date</td>\n",
       "      <td>'review_date' is not null</td>\n",
       "      <td>.isComplete(\\review_date\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>'helpful_votes' is not null</td>\n",
       "      <td>.isComplete(\\helpful_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>helpful_votes</td>\n",
       "      <td>'helpful_votes' has no negative values</td>\n",
       "      <td>.isNonNegative(\\helpful_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>star_rating</td>\n",
       "      <td>'star_rating' is not null</td>\n",
       "      <td>.isComplete(\\star_rating\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>star_rating</td>\n",
       "      <td>'star_rating' has no negative values</td>\n",
       "      <td>.isNonNegative(\\star_rating\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>product_title</td>\n",
       "      <td>'product_title' is not null</td>\n",
       "      <td>.isComplete(\\product_title\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>review_headline</td>\n",
       "      <td>'review_headline' is not null</td>\n",
       "      <td>.isComplete(\\review_headline\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>product_id</td>\n",
       "      <td>'product_id' is not null</td>\n",
       "      <td>.isComplete(\\product_id\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>total_votes</td>\n",
       "      <td>'total_votes' is not null</td>\n",
       "      <td>.isComplete(\\total_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>total_votes</td>\n",
       "      <td>'total_votes' has no negative values</td>\n",
       "      <td>.isNonNegative(\\total_votes\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>product_category</td>\n",
       "      <td>'product_category' is not null</td>\n",
       "      <td>.isComplete(\\product_category\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>product_category</td>\n",
       "      <td>'product_category' has value range 'Digital_Vi...</td>\n",
       "      <td>.isContainedIn(\\product_category\\\", Array(\\\"Di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' is not null</td>\n",
       "      <td>.isComplete(\\product_parent\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' has type Integral</td>\n",
       "      <td>.hasDataType(\\product_parent\\\", ConstrainableD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>product_parent</td>\n",
       "      <td>'product_parent' has no negative values</td>\n",
       "      <td>.isNonNegative(\\product_parent\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>review_body</td>\n",
       "      <td>'review_body' has less than 1% missing values</td>\n",
       "      <td>.hasCompleteness(\\review_body\\\", _ &gt;= 0.99, So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>vine</td>\n",
       "      <td>'vine' is not null</td>\n",
       "      <td>.isComplete(\\vine\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>vine</td>\n",
       "      <td>'vine' has value range 'N'</td>\n",
       "      <td>.isContainedIn(\\vine\\\", Array(\\\"N\\\"))\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>marketplace</td>\n",
       "      <td>'marketplace' is not null</td>\n",
       "      <td>.isComplete(\\marketplace\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>marketplace</td>\n",
       "      <td>'marketplace' has value range 'US'</td>\n",
       "      <td>.isContainedIn(\\marketplace\\\", Array(\\\"US\\\"))\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>'verified_purchase' is not null</td>\n",
       "      <td>.isComplete(\\verified_purchase\\\")\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>verified_purchase</td>\n",
       "      <td>'verified_purchase' has value range 'Y', 'N'</td>\n",
       "      <td>.isContainedIn(\\verified_purchase\\\", Array(\\\"Y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          column_name                                        description  \\\n",
       "0           review_id                            'review_id' is not null   \n",
       "1         customer_id                          'customer_id' is not null   \n",
       "2         customer_id                    'customer_id' has type Integral   \n",
       "3         customer_id               'customer_id' has no negative values   \n",
       "4         review_date                          'review_date' is not null   \n",
       "5       helpful_votes                        'helpful_votes' is not null   \n",
       "6       helpful_votes             'helpful_votes' has no negative values   \n",
       "7         star_rating                          'star_rating' is not null   \n",
       "8         star_rating               'star_rating' has no negative values   \n",
       "9       product_title                        'product_title' is not null   \n",
       "10    review_headline                      'review_headline' is not null   \n",
       "11         product_id                           'product_id' is not null   \n",
       "12        total_votes                          'total_votes' is not null   \n",
       "13        total_votes               'total_votes' has no negative values   \n",
       "14   product_category                     'product_category' is not null   \n",
       "15   product_category  'product_category' has value range 'Digital_Vi...   \n",
       "16     product_parent                       'product_parent' is not null   \n",
       "17     product_parent                 'product_parent' has type Integral   \n",
       "18     product_parent            'product_parent' has no negative values   \n",
       "19        review_body      'review_body' has less than 1% missing values   \n",
       "20               vine                                 'vine' is not null   \n",
       "21               vine                         'vine' has value range 'N'   \n",
       "22        marketplace                          'marketplace' is not null   \n",
       "23        marketplace                 'marketplace' has value range 'US'   \n",
       "24  verified_purchase                    'verified_purchase' is not null   \n",
       "25  verified_purchase       'verified_purchase' has value range 'Y', 'N'   \n",
       "\n",
       "                                                 code  \n",
       "0                          .isComplete(\\review_id\\\")\"  \n",
       "1                        .isComplete(\\customer_id\\\")\"  \n",
       "2   .hasDataType(\\customer_id\\\", ConstrainableData...  \n",
       "3                     .isNonNegative(\\customer_id\\\")\"  \n",
       "4                        .isComplete(\\review_date\\\")\"  \n",
       "5                      .isComplete(\\helpful_votes\\\")\"  \n",
       "6                   .isNonNegative(\\helpful_votes\\\")\"  \n",
       "7                        .isComplete(\\star_rating\\\")\"  \n",
       "8                     .isNonNegative(\\star_rating\\\")\"  \n",
       "9                      .isComplete(\\product_title\\\")\"  \n",
       "10                   .isComplete(\\review_headline\\\")\"  \n",
       "11                        .isComplete(\\product_id\\\")\"  \n",
       "12                       .isComplete(\\total_votes\\\")\"  \n",
       "13                    .isNonNegative(\\total_votes\\\")\"  \n",
       "14                  .isComplete(\\product_category\\\")\"  \n",
       "15  .isContainedIn(\\product_category\\\", Array(\\\"Di...  \n",
       "16                    .isComplete(\\product_parent\\\")\"  \n",
       "17  .hasDataType(\\product_parent\\\", ConstrainableD...  \n",
       "18                 .isNonNegative(\\product_parent\\\")\"  \n",
       "19  .hasCompleteness(\\review_body\\\", _ >= 0.99, So...  \n",
       "20                              .isComplete(\\vine\\\")\"  \n",
       "21             .isContainedIn(\\vine\\\", Array(\\\"N\\\"))\"  \n",
       "22                       .isComplete(\\marketplace\\\")\"  \n",
       "23     .isContainedIn(\\marketplace\\\", Array(\\\"US\\\"))\"  \n",
       "24                 .isComplete(\\verified_purchase\\\")\"  \n",
       "25  .isContainedIn(\\verified_purchase\\\", Array(\\\"Y...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_constraint_suggestions = load_dataset(path='./amazon-reviews-spark-analyzer/constraint-suggestions/', sep='\\t', header=0)\n",
    "df_constraint_suggestions.columns=['column_name', 'description', 'code']\n",
    "df_constraint_suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save for the Next Notebook(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'df_dataset_metrics' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store df_dataset_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
