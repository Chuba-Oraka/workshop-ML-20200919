{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# SPARK VERSION FOR DEEQU\n",
    "***\n",
    "\n",
    "# Feature transformation with Amazon SageMaker Processing and SparkML\n",
    "\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Spark in a managed SageMaker environment to run our preprocessing workload. Then, we'll take our preprocessed dataset and train a regression model using XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  amazon-reviews-spark-analyzer-2020-02-25-21-49-56\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "# TODO:  Clean these up\n",
    "#input_raw_prefix = 'sagemaker/spark-preprocess-reviews-demo/input/raw/reviews'\n",
    "output_prefix = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "#model_prefix = 'sagemaker/spark-preprocess-reviews-demo/model'\n",
    "processing_job_name = 'amazon-reviews-spark-analyzer-{}'.format(timestamp_prefix)\n",
    "\n",
    "print('Processing job name:  {}'.format(processing_job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Amazon SageMaker Processing to execute a SparkML job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset and uploading to Amazon Simple Storage Service (Amazon S3)\n",
    "\n",
    "Show the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE product_category=Apparel/\r\n",
      "                           PRE product_category=Automotive/\r\n",
      "                           PRE product_category=Baby/\r\n",
      "                           PRE product_category=Beauty/\r\n",
      "                           PRE product_category=Books/\r\n",
      "                           PRE product_category=Camera/\r\n",
      "                           PRE product_category=Digital_Ebook_Purchase/\r\n",
      "                           PRE product_category=Digital_Music_Purchase/\r\n",
      "                           PRE product_category=Digital_Software/\r\n",
      "                           PRE product_category=Digital_Video_Download/\r\n",
      "                           PRE product_category=Digital_Video_Games/\r\n",
      "                           PRE product_category=Electronics/\r\n",
      "                           PRE product_category=Furniture/\r\n",
      "                           PRE product_category=Gift_Card/\r\n",
      "                           PRE product_category=Grocery/\r\n",
      "                           PRE product_category=Health_&_Personal_Care/\r\n",
      "                           PRE product_category=Home/\r\n",
      "                           PRE product_category=Home_Entertainment/\r\n",
      "                           PRE product_category=Home_Improvement/\r\n",
      "                           PRE product_category=Jewelry/\r\n",
      "                           PRE product_category=Kitchen/\r\n",
      "                           PRE product_category=Lawn_and_Garden/\r\n",
      "                           PRE product_category=Luggage/\r\n",
      "                           PRE product_category=Major_Appliances/\r\n",
      "                           PRE product_category=Mobile_Apps/\r\n",
      "                           PRE product_category=Mobile_Electronics/\r\n",
      "                           PRE product_category=Music/\r\n",
      "                           PRE product_category=Musical_Instruments/\r\n",
      "                           PRE product_category=Office_Products/\r\n",
      "                           PRE product_category=Outdoors/\r\n",
      "                           PRE product_category=PC/\r\n",
      "                           PRE product_category=Personal_Care_Appliances/\r\n",
      "                           PRE product_category=Pet_Products/\r\n",
      "                           PRE product_category=Shoes/\r\n",
      "                           PRE product_category=Software/\r\n",
      "                           PRE product_category=Sports/\r\n",
      "                           PRE product_category=Tools/\r\n",
      "                           PRE product_category=Toys/\r\n",
      "                           PRE product_category=Video/\r\n",
      "                           PRE product_category=Video_DVD/\r\n",
      "                           PRE product_category=Video_Games/\r\n",
      "                           PRE product_category=Watches/\r\n",
      "                           PRE product_category=Wireless/\r\n"
     ]
    }
   ],
   "source": [
    "#!aws s3 ls s3://$bucket/$input_raw_prefix/\n",
    "!aws s3 ls s3://amazon-reviews-pds/parquet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Spark container for running the preprocessing job\n",
    "\n",
    "An example Spark container is included in the `./container` directory of this example. The container handles the bootstrapping of all Spark configuration, and serves as a wrapper around the `spark-submit` CLI. At a high level the container provides:\n",
    "* A set of default Spark/YARN/Hadoop configurations\n",
    "* A bootstrapping script for configuring and starting up Spark master/worker nodes\n",
    "* A wrapper around the `spark-submit` CLI to submit a Spark application\n",
    "\n",
    "\n",
    "After the container build and push process is complete, use the Amazon SageMaker Python SDK to submit a managed, distributed Spark application that performs our dataset preprocessing.\n",
    "\n",
    "Build the example Spark container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM openjdk:8-jre-slim\r\n",
      "\r\n",
      "RUN apt-get update\r\n",
      "RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\r\n",
      "RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\r\n",
      "RUN apt-get clean\r\n",
      "RUN rm -rf /var/lib/apt/lists/*\r\n",
      "\r\n",
      "# http://blog.stuart.axelbrooke.com/python-3-on-spark-return-of-the-pythonhashseed\r\n",
      "ENV PYTHONHASHSEED 0\r\n",
      "ENV PYTHONIOENCODING UTF-8\r\n",
      "ENV PIP_DISABLE_PIP_VERSION_CHECK 1\r\n",
      "\r\n",
      "# Install Hadoop\r\n",
      "ENV HADOOP_VERSION 3.0.0\r\n",
      "ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\r\n",
      "ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\r\n",
      "ENV PATH $PATH:$HADOOP_HOME/bin\r\n",
      "RUN curl -sL --retry 3 \\\r\n",
      "  \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\" \\\r\n",
      "  | gunzip \\\r\n",
      "  | tar -x -C /usr/ \\\r\n",
      " && rm -rf $HADOOP_HOME/share/doc \\\r\n",
      " && chown -R root:root $HADOOP_HOME\r\n",
      "\r\n",
      "# Install Spark\r\n",
      "ENV SPARK_VERSION 2.4.5\r\n",
      "ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\r\n",
      "ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\r\n",
      "ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\r\n",
      "ENV PATH $PATH:${SPARK_HOME}/bin\r\n",
      "RUN curl -sL --retry 3 \\\r\n",
      "  \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\" \\\r\n",
      "  | gunzip \\\r\n",
      "  | tar x -C /usr/ \\\r\n",
      " && mv /usr/$SPARK_PACKAGE $SPARK_HOME \\\r\n",
      " && chown -R root:root $SPARK_HOME\r\n",
      " \r\n",
      "# Point Spark at proper python binary\r\n",
      "ENV PYSPARK_PYTHON=/usr/bin/python3\r\n",
      "\r\n",
      "# Setup Spark/Yarn/HDFS user as root\r\n",
      "ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\r\n",
      "ENV YARN_RESOURCEMANAGER_USER=\"root\"\r\n",
      "ENV YARN_NODEMANAGER_USER=\"root\"\r\n",
      "ENV HDFS_NAMENODE_USER=\"root\"\r\n",
      "ENV HDFS_DATANODE_USER=\"root\"\r\n",
      "ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\r\n",
      "\r\n",
      "# Set up bootstrapping program and Spark configuration\r\n",
      "COPY program /opt/program\r\n",
      "RUN chmod +x /opt/program/submit\r\n",
      "COPY hadoop-config /opt/hadoop-config\r\n",
      "\r\n",
      "COPY jars /usr/jars\r\n",
      "\r\n",
      "WORKDIR $SPARK_HOME\r\n",
      "\r\n",
      "ENTRYPOINT [\"/opt/program/submit\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat container/Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_repo = 'amazon-reviews-spark-analyzer'\n",
    "docker_tag = 'latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  3.022MB\n",
      "Step 1/33 : FROM openjdk:8-jre-slim\n",
      " ---> 9c82c74fbc96\n",
      "Step 2/33 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> ca0a6099c443\n",
      "Step 3/33 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> cc1fd71bd88c\n",
      "Step 4/33 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> 376699e73ced\n",
      "Step 5/33 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 1ea11fb14632\n",
      "Step 6/33 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> ea880623fd34\n",
      "Step 7/33 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 7d1b53453a5e\n",
      "Step 8/33 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> 12cfee88f392\n",
      "Step 9/33 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> 8010c768a0e5\n",
      "Step 10/33 : ENV HADOOP_VERSION 3.0.0\n",
      " ---> Using cache\n",
      " ---> 0e10f67d8992\n",
      "Step 11/33 : ENV HADOOP_HOME /usr/hadoop-$HADOOP_VERSION\n",
      " ---> Using cache\n",
      " ---> 19f1a57f792c\n",
      "Step 12/33 : ENV HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop\n",
      " ---> Using cache\n",
      " ---> 26efdd691c69\n",
      "Step 13/33 : ENV PATH $PATH:$HADOOP_HOME/bin\n",
      " ---> Using cache\n",
      " ---> 20b3f1df2938\n",
      "Step 14/33 : RUN curl -sL --retry 3   \"http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz\"   | gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root $HADOOP_HOME\n",
      " ---> Using cache\n",
      " ---> 99bf890b2a4b\n",
      "Step 15/33 : ENV SPARK_VERSION 2.4.5\n",
      " ---> Using cache\n",
      " ---> 4c8bd605f733\n",
      "Step 16/33 : ENV SPARK_PACKAGE spark-${SPARK_VERSION}-bin-without-hadoop\n",
      " ---> Using cache\n",
      " ---> 95570132b791\n",
      "Step 17/33 : ENV SPARK_HOME /usr/spark-${SPARK_VERSION}\n",
      " ---> Using cache\n",
      " ---> 46f3c6ef3e2e\n",
      "Step 18/33 : ENV SPARK_DIST_CLASSPATH=\"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*\"\n",
      " ---> Using cache\n",
      " ---> 0edbc32cb627\n",
      "Step 19/33 : ENV PATH $PATH:${SPARK_HOME}/bin\n",
      " ---> Using cache\n",
      " ---> 6faefe1a7164\n",
      "Step 20/33 : RUN curl -sL --retry 3   \"https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/${SPARK_PACKAGE}.tgz\"   | gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R root:root $SPARK_HOME\n",
      " ---> Using cache\n",
      " ---> 393fc9f21f4b\n",
      "Step 21/33 : ENV PYSPARK_PYTHON=/usr/bin/python3\n",
      " ---> Using cache\n",
      " ---> 9ae69f1ec4d2\n",
      "Step 22/33 : ENV PATH=\"/usr/bin:/opt/program:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 84ff81959b4b\n",
      "Step 23/33 : ENV YARN_RESOURCEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 851718c619e5\n",
      "Step 24/33 : ENV YARN_NODEMANAGER_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> dba307054ba1\n",
      "Step 25/33 : ENV HDFS_NAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> c6e12165467e\n",
      "Step 26/33 : ENV HDFS_DATANODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 6064e2aed768\n",
      "Step 27/33 : ENV HDFS_SECONDARYNAMENODE_USER=\"root\"\n",
      " ---> Using cache\n",
      " ---> 35de5dfe5d32\n",
      "Step 28/33 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> 1a609344ed01\n",
      "Step 29/33 : RUN chmod +x /opt/program/submit\n",
      " ---> Using cache\n",
      " ---> 3da7df9a080c\n",
      "Step 30/33 : COPY hadoop-config /opt/hadoop-config\n",
      " ---> Using cache\n",
      " ---> 627c02067218\n",
      "Step 31/33 : COPY jars /usr/jars\n",
      " ---> 336c821fc868\n",
      "Step 32/33 : WORKDIR $SPARK_HOME\n",
      " ---> Running in 7d3b03f5efe0\n",
      "Removing intermediate container 7d3b03f5efe0\n",
      " ---> edc7240135c5\n",
      "Step 33/33 : ENTRYPOINT [\"/opt/program/submit\"]\n",
      " ---> Running in 27cc9257dc22\n",
      "Removing intermediate container 27cc9257dc22\n",
      " ---> 3a06159a0962\n",
      "Successfully built 3a06159a0962\n",
      "Successfully tagged amazon-reviews-spark-analyzer:latest\n"
     ]
    }
   ],
   "source": [
    "!docker build -t $docker_repo:$docker_tag -f container/Dockerfile ./container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Amazon Elastic Container Registry (Amazon ECR) repository for the Spark container and push the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "835319576252.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-analyzer:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "image_uri = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account_id, region, docker_repo, docker_tag)\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create ECR repository and push docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"repositories\": [\r\n",
      "        {\r\n",
      "            \"repositoryArn\": \"arn:aws:ecr:us-east-1:835319576252:repository/amazon-reviews-spark-analyzer\",\r\n",
      "            \"registryId\": \"835319576252\",\r\n",
      "            \"repositoryName\": \"amazon-reviews-spark-analyzer\",\r\n",
      "            \"repositoryUri\": \"835319576252.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-analyzer\",\r\n",
      "            \"createdAt\": 1581738843.0,\r\n",
      "            \"imageTagMutability\": \"MUTABLE\",\r\n",
      "            \"imageScanningConfiguration\": {\r\n",
      "                \"scanOnPush\": false\r\n",
      "            }\r\n",
      "        }\r\n",
      "    ]\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!aws ecr describe-repositories --repository-names $docker_repo || aws ecr create-repository --repository-name $docker_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker tag $docker_repo:$docker_tag $image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The push refers to repository [835319576252.dkr.ecr.us-east-1.amazonaws.com/amazon-reviews-spark-analyzer]\n",
      "\n",
      "\u001b[1B4377c451: Preparing \n",
      "\u001b[1B268138d8: Preparing \n",
      "\u001b[1B35559781: Preparing \n",
      "\u001b[1B6b36b7a4: Preparing \n",
      "\u001b[1B782557b4: Preparing \n",
      "\u001b[1B8eb2663a: Preparing \n",
      "\u001b[1B5ae17e8e: Preparing \n",
      "\u001b[1B89b8c28a: Preparing \n",
      "\u001b[1Bd604f04b: Preparing \n",
      "\u001b[1Ba936c4d8: Preparing \n",
      "\u001b[1Bf855c32d: Preparing \n",
      "\u001b[1B964f7673: Preparing \n",
      "\u001b[1B0d7e7b4a: Preparing \n",
      "\u001b[1Ba6e6c92c: Preparing \n",
      "\u001b[15B377c451: Pushed       3MBists 5MB5A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[K\u001b[15A\u001b[1K\u001b[Klatest: digest: sha256:f6fe6ddad0942f0ba99e9298a5e5f39235b3de24ee169a250ac4d51a880f3669 size: 3472\n"
     ]
    }
   ],
   "source": [
    "!docker push $image_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the preprocessing job using Amazon SageMaker Processing\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built, and a SparkML script for preprocessing in the job configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from __future__ import print_function\r\n",
      "from __future__ import unicode_literals\r\n",
      "\r\n",
      "import time\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import shutil\r\n",
      "import csv\r\n",
      "\r\n",
      "import pyspark\r\n",
      "from pyspark.sql import SparkSession\r\n",
      "from pyspark.sql.functions import *\r\n",
      "\r\n",
      "def main():\r\n",
      "    args_iter = iter(sys.argv[1:])\r\n",
      "    args = dict(zip(args_iter, args_iter))\r\n",
      "    \r\n",
      "    # Retrieve the args and replace 's3://' with 's3a://' (used by Spark)\r\n",
      "    s3_input_data = args['s3_input_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_input_data)\r\n",
      "    s3_output_analyze_data = args['s3_output_analyze_data'].replace('s3://', 's3a://')\r\n",
      "    print(s3_output_analyze_data)\r\n",
      "    \r\n",
      "    spark = SparkSession.builder \\\r\n",
      "        .appName(\"Amazon_Reviews_Spark_Analyzer\") \\\r\n",
      "        .getOrCreate()\r\n",
      "\r\n",
      "    # Invoke Main from preprocess-deequ.jar\r\n",
      "    getattr(spark._jvm.SparkAmazonReviewsAnalyzer, \"run\")(s3_input_data, s3_output_analyze_data)\r\n",
      "\r\n",
      "if __name__ == \"__main__\":\r\n",
      "    main()\r\n"
     ]
    }
   ],
   "source": [
    "cat preprocess-deequ.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a processing job.  You specify the command (`/opt/program/submit` for this Spark processor.)  You also need to specify one `ProcessingInput` with the `source` argument of the Amazon S3 bucket and `destination` is where the script reads this data from `/opt/ml/processing/input` (inside the Docker container.)  All local paths inside the processing container must begin with `/opt/ml/processing/`.\n",
    "\n",
    "Also give the `run()` method a `ProcessingOutput`, where the `source` is the path the script writes output data to.  For outputs, the `destination` defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name>/`.  You also give the `ProcessingOutput` value for `output_name`, to make it easier to retrieve these output artifacts after the job is run.\n",
    "\n",
    "The arguments parameter in the `run()` method are command-line arguments in our `preprocess.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "processor = ScriptProcessor(base_job_name='spark-amazon-reviews-analyzer',\n",
    "                            image_uri=image_uri,\n",
    "                            command=['/opt/program/submit'],\n",
    "                            role=role,\n",
    "                            instance_count=10, # instance_count needs to be > 1 or you will see the following error:  \"INFO yarn.Client: Application report for application_ (state: ACCEPTED)\"\n",
    "                            instance_type='ml.r5.8xlarge',\n",
    "                            max_runtime_in_seconds=600,\n",
    "                            env={\n",
    "                                'mode': 'jar',\n",
    "                                'main_class': 'Main'\n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://amazon-reviews-pds/parquet/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://amazon-reviews-pds/parquet/'\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           PRE product_category=Apparel/\r\n",
      "                           PRE product_category=Automotive/\r\n",
      "                           PRE product_category=Baby/\r\n",
      "                           PRE product_category=Beauty/\r\n",
      "                           PRE product_category=Books/\r\n",
      "                           PRE product_category=Camera/\r\n",
      "                           PRE product_category=Digital_Ebook_Purchase/\r\n",
      "                           PRE product_category=Digital_Music_Purchase/\r\n",
      "                           PRE product_category=Digital_Software/\r\n",
      "                           PRE product_category=Digital_Video_Download/\r\n",
      "                           PRE product_category=Digital_Video_Games/\r\n",
      "                           PRE product_category=Electronics/\r\n",
      "                           PRE product_category=Furniture/\r\n",
      "                           PRE product_category=Gift_Card/\r\n",
      "                           PRE product_category=Grocery/\r\n",
      "                           PRE product_category=Health_&_Personal_Care/\r\n",
      "                           PRE product_category=Home/\r\n",
      "                           PRE product_category=Home_Entertainment/\r\n",
      "                           PRE product_category=Home_Improvement/\r\n",
      "                           PRE product_category=Jewelry/\r\n",
      "                           PRE product_category=Kitchen/\r\n",
      "                           PRE product_category=Lawn_and_Garden/\r\n",
      "                           PRE product_category=Luggage/\r\n",
      "                           PRE product_category=Major_Appliances/\r\n",
      "                           PRE product_category=Mobile_Apps/\r\n",
      "                           PRE product_category=Mobile_Electronics/\r\n",
      "                           PRE product_category=Music/\r\n",
      "                           PRE product_category=Musical_Instruments/\r\n",
      "                           PRE product_category=Office_Products/\r\n",
      "                           PRE product_category=Outdoors/\r\n",
      "                           PRE product_category=PC/\r\n",
      "                           PRE product_category=Personal_Care_Appliances/\r\n",
      "                           PRE product_category=Pet_Products/\r\n",
      "                           PRE product_category=Shoes/\r\n",
      "                           PRE product_category=Software/\r\n",
      "                           PRE product_category=Sports/\r\n",
      "                           PRE product_category=Tools/\r\n",
      "                           PRE product_category=Toys/\r\n",
      "                           PRE product_category=Video/\r\n",
      "                           PRE product_category=Video_DVD/\r\n",
      "                           PRE product_category=Video_Games/\r\n",
      "                           PRE product_category=Watches/\r\n",
      "                           PRE product_category=Wireless/\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-02-25-21-49-56/analyze_data\n"
     ]
    }
   ],
   "source": [
    "# Outputs\n",
    "s3_output_analyze_data = 's3://{}/{}/analyze_data'.format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_analyze_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  spark-amazon-reviews-analyzer-2020-02-26-01-05-05-890\n",
      "Inputs:  [{'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/spark-amazon-reviews-analyzer-2020-02-26-01-05-05-890/input/code/preprocess-deequ.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  []\n"
     ]
    }
   ],
   "source": [
    "# Note:  We can specify the local `preprocess.py` because we're using the SageMaker SDK.\n",
    "#\n",
    "#    Notes on Invoking from Lambda:\n",
    "#      * However, if we use the boto3 SDK (ie. with a Lambda), we need to copy the `preprocess.py` file to S3 and specify the everything include --py-files, etc.\n",
    "#      * We would need to do the following before invoking the Lambda:\n",
    "#          !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/code/preprocess.py\n",
    "#          !aws s3 cp preprocess.py s3://<location>/sagemaker/spark-preprocess-reviews-demo/py_files/preprocess.py\n",
    "#      * Then reference the s3://<location> above in the --py-files, etc.\n",
    "#      * See Lambda example code in this same project for more details.\n",
    "#\n",
    "# Note:  See https://github.com/awslabs/amazon-sagemaker-examples/issues/994 for issues related to using /opt/ml/processing/input/ and output/\n",
    "processor.run(code='preprocess-deequ.py',\n",
    "                    arguments=['s3_input_data', s3_input_data,\n",
    "                               's3_output_analyze_data', s3_output_analyze_data,\n",
    "                              ],\n",
    "                    logs=True,\n",
    "                    wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=spark-amazon-reviews-analyzer-2020-02-26-01-05-05-890;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############\n",
    "# TODO:  CHANGE THE processing_job_name BELOW\n",
    "#############\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "processing_job_name = 'spark-amazon-reviews-analyzer-2020-02-26-01-05-05-890'\n",
    "\n",
    "\n",
    "#############\n",
    "# TODO:  CHANGE THE processing_job_name ABOVE ^^\n",
    "#############\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, processing_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws sagemaker list-processing-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n",
    "#                                                                            sagemaker_session=sagemaker_session)\n",
    "#running_processor.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the processed dataset\n",
    "Take a look at a few rows of the transformed dataset to make sure the preprocessing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/amazon-reviews-spark-analyzer-2020-02-25-21-49-56/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, output_prefix, region)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $s3_output_analyze_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
