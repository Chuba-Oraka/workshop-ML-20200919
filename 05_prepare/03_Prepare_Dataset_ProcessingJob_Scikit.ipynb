{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature transformation with Amazon SageMaker Processing and SparkML\n",
    "\n",
    "Typically a machine learning (ML) process consists of few steps. First, gathering data with various ETL jobs, then pre-processing the data, featurizing the dataset by incorporating standard techniques or prior knowledge, and finally training an ML model using an algorithm.\n",
    "\n",
    "Often, distributed data processing frameworks such as Spark are used to pre-process data sets in order to prepare them for training. In this notebook we'll use Amazon SageMaker Processing, and leverage the power of Spark in a managed SageMaker environment to run our preprocessing workload. Then, we'll take our preprocessed dataset and train a regression model using XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/processing.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Objective](#Objective:-predict-the-age-of-an-Abalone-from-its-physical-measurement)\n",
    "1. [Setup](#Setup)\n",
    "1. [Using Amazon SageMaker Processing to execute a SparkML Job](#Using-Amazon-SageMaker-Processing-to-execute-a-SparkML-Job)\n",
    "  1. [Downloading dataset and uploading to S3](#Downloading-dataset-and-uploading-to-S3)\n",
    "  1. [Build a Spark container for running the preprocessing job](#Build-a-Spark-container-for-running-the-preprocessing-job)\n",
    "  1. [Run the preprocessing job using Amazon SageMaker Processing](#Run-the-preprocessing-job-using-Amazon-SageMaker-Processing)\n",
    "    1. [Inspect the preprocessed dataset](#Inspect-the-preprocessed-dataset)\n",
    "1. [Train a regression model using the Amazon SageMaker XGBoost algorithm](#Train-a-regression-model-using-the-SageMaker-XGBoost-algorithm)\n",
    "  1. [Retrieve the XGBoost algorithm image](#Retrieve-the-XGBoost-algorithm-image)\n",
    "  1. [Set XGBoost model parameters and dataset details](#Set-XGBoost-model-parameters-and-dataset-details)\n",
    "  1. [Train the XGBoost model](#Train-the-XGBoost-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's start by specifying:\n",
    "* The S3 bucket and prefixes that you use for training and model data. Use the default bucket specified by the Amazon SageMaker session.\n",
    "* The IAM role ARN used to give processing and training access to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.12.15)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.15 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3) (1.15.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3) (0.9.5)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.15->boto3) (0.15.2)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.15->boto3) (1.25.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.15->boto3) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.16.0,>=1.15.15->boto3) (1.14.0)\n",
      "\u001b[31mtensorflow 2.0.0 requires opt-einsum>=2.3.2, which is not installed.\u001b[0m\n",
      "\u001b[31mtensorflow 2.0.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.18.11 has requirement botocore==1.15.11, but you'll have botocore 1.15.15 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.18.11 has requirement PyYAML<5.3,>=3.10, but you'll have pyyaml 5.3 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "import boto3\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Amazon SageMaker Processing to execute a SparkML job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading dataset and uploading to Amazon Simple Storage Service (Amazon S3)\n",
    "\n",
    "Show the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the preprocessing job using Amazon SageMaker Processing\n",
    "\n",
    "Next, use the Amazon SageMaker Python SDK to submit a processing job. Use the Spark container that was just built, and a SparkML script for preprocessing in the job configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the Spark preprocessing script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import argparse\r\n",
      "import json\r\n",
      "import os\r\n",
      "\r\n",
      "def list_arg(raw_value):\r\n",
      "    \"\"\"argparse type for a list of strings\"\"\"\r\n",
      "    return str(raw_value).split(\",\")\r\n",
      "\r\n",
      "\r\n",
      "def parse_args():\r\n",
      "    # Unlike SageMaker training jobs (which have `SM_HOSTS` and `SM_CURRENT_HOST` env vars), processing jobs to need to parse the resource config file directly\r\n",
      "    resconfig = {}\r\n",
      "    try:\r\n",
      "        with open(\"/opt/ml/config/resourceconfig.json\", \"r\") as cfgfile:\r\n",
      "            resconfig = json.load(cfgfile)\r\n",
      "    except FileNotFoundError:\r\n",
      "        print(\"/opt/ml/config/resourceconfig.json not found.  current_host is unknown.\")\r\n",
      "        pass # Ignore\r\n",
      "\r\n",
      "    # Local testing with CLI args\r\n",
      "    parser = argparse.ArgumentParser(description=\"Process\")\r\n",
      "\r\n",
      "    parser.add_argument(\"--hosts\", type=list_arg,\r\n",
      "        default=resconfig.get(\"hosts\", [\"unknown\"]),\r\n",
      "        help=\"Comma-separated list of host names running the job\"\r\n",
      "    )\r\n",
      "    parser.add_argument(\"--current-host\", type=str,\r\n",
      "        default=resconfig.get(\"current_host\", \"unknown\"),\r\n",
      "        help=\"Name of this host running the job\"\r\n",
      "    )\r\n",
      "    \r\n",
      "    return parser.parse_args()\r\n",
      "\r\n",
      "\r\n",
      "def process(args):\r\n",
      "    print('Current host: {}'.format(args.current_host))\r\n",
      "\r\n",
      "    print('Creating directory /opt/ml/processing/output/train'.format(args.current_host))\r\n",
      "    os.makedirs('/opt/ml/processing/output/train/', exist_ok=True)\r\n",
      "\r\n",
      "    print('Writing to /opt/ml/processing/output/train/{}.csv'.format(args.current_host))\r\n",
      "    with open('/opt/ml/processing/output/train/{}.csv'.format(args.current_host), 'w') as fd:\r\n",
      "        fd.write('host{},thanks,andre,and,alex!'.format(args.current_host))\r\n",
      "        fd.close()\r\n",
      "        \r\n",
      "    print('Listing contents of /opt/ml/processing/output/train/')\r\n",
      "    dirs_output_train = os.listdir('/opt/ml/processing/output/train/')\r\n",
      "\r\n",
      "    # This would print all the files and directories\r\n",
      "    for file in dirs_output_train:\r\n",
      "        print(file)\r\n",
      "\r\n",
      "    print('Listing contents of /opt/ml/processing/input/data')\r\n",
      "    dirs_input = os.listdir('/opt/ml/processing/input/data')\r\n",
      "\r\n",
      "    # This would print all the files and directories\r\n",
      "    for file in dirs_input:\r\n",
      "        print(file)\r\n",
      " \r\n",
      "    print('Complete')\r\n",
      "    \r\n",
      "    \r\n",
      "if __name__ == \"__main__\":\r\n",
      "    args = parse_args()\r\n",
      "    print(\"Loaded arguments:\")\r\n",
      "    print(args)\r\n",
      "    \r\n",
      "    print(\"Environment variables:\")\r\n",
      "    print(os.environ)\r\n",
      "\r\n",
      "    process(args)\r\n"
     ]
    }
   ],
   "source": [
    "cat preprocess-scikit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this script as a processing job.  You specify the command (`/opt/program/submit` for this Spark processor.)  You also need to specify one `ProcessingInput` with the `source` argument of the Amazon S3 bucket and `destination` is where the script reads this data from `/opt/ml/processing/input` (inside the Docker container.)  All local paths inside the processing container must begin with `/opt/ml/processing/`.\n",
    "\n",
    "Also give the `run()` method a `ProcessingOutput`, where the `source` is the path the script writes output data to.  For outputs, the `destination` defaults to an S3 bucket that the Amazon SageMaker Python SDK creates for you, following the format `s3://sagemaker-<region>-<account_id>/<processing_job_name>/output/<output_name>/`.  You also give the `ProcessingOutput` value for `output_name`, to make it easier to retrieve these output artifacts after the job is run.\n",
    "\n",
    "The arguments parameter in the `run()` method are command-line arguments in our `preprocess.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing job name:  amazon-reviews-scikit-processor-2020-03-06-23-50-36\n"
     ]
    }
   ],
   "source": [
    "timestamp_prefix = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "\n",
    "output_prefix = 'amazon-reviews-scikit-processor-{}'.format(timestamp_prefix)\n",
    "processing_job_name = 'amazon-reviews-scikit-processor-{}'.format(timestamp_prefix)\n",
    "\n",
    "print('Processing job name:  {}'.format(processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    #base_job_name='amazon-reviews-processor-scikit',\n",
    "                                     framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.m5.xlarge',\n",
    "                                     instance_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://amazon-reviews-pds/tsv/\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "s3_input_data = 's3://amazon-reviews-pds/tsv/'\n",
    "print(s3_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $s3_input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/amazon-reviews-scikit-processor-2020-03-06-23-50-36/train\n",
      "s3://sagemaker-us-east-1-835319576252/amazon-reviews-scikit-processor-2020-03-06-23-50-36/validation\n",
      "s3://sagemaker-us-east-1-835319576252/amazon-reviews-scikit-processor-2020-03-06-23-50-36/test\n"
     ]
    }
   ],
   "source": [
    "# Outputs\n",
    "s3_output_train_data = 's3://{}/{}/train'.format(bucket, output_prefix)\n",
    "s3_output_validation_data = 's3://{}/{}/validation'.format(bucket, output_prefix)\n",
    "s3_output_test_data = 's3://{}/{}/test'.format(bucket, output_prefix)\n",
    "\n",
    "print(s3_output_train_data)\n",
    "print(s3_output_validation_data)\n",
    "print(s3_output_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sagemaker-scikit-learn-2020-03-06-23-50-51-679\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://amazon-reviews-pds/tsv/', 'LocalPath': '/opt/ml/processing/input/data/', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'ShardedByS3Key', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-06-23-50-51-679/input/code/preprocess-scikit.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-06-23-50-51-679/output/train_data', 'LocalPath': '/opt/ml/processing/output/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'validation_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-06-23-50-51-679/output/validation_data', 'LocalPath': '/opt/ml/processing/output/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-06-23-50-51-679/output/test_data', 'LocalPath': '/opt/ml/processing/output/test', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "# ShardedS3Key to spread the transformations across all nodes\n",
    "sklearn_processor.run(code='preprocess-scikit.py',\n",
    "                      inputs=[ProcessingInput(source=s3_input_data,\n",
    "                                              destination='/opt/ml/processing/input/data/',\n",
    "                                              s3_data_distribution_type='ShardedByS3Key')],\n",
    "                      outputs=[ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/output/train'),\n",
    "                               ProcessingOutput(output_name='validation_data',\n",
    "                                                source='/opt/ml/processing/output/validation'),\n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/output/test')],\n",
    "                      logs=True,\n",
    "                      wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/ProcessingJobs;prefix=sagemaker-scikit-learn-2020-03-06-23-50-51-679;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#############\n",
    "# TODO:  CHANGE THE processing_job_name BELOW\n",
    "#############\n",
    "\n",
    "processing_job_name = 'sagemaker-scikit-learn-2020-03-06-23-50-51-679'\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, processing_job_name)))\n",
    "\n",
    "#############\n",
    "# TODO:  CHANGE THE processing_job_name ABOVE ^^\n",
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!aws sagemaker list-processing-jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#running_processor = sagemaker.processing.ProcessingJob.from_processing_name(processing_job_name=processing_job_name,\n",
    "#                                                                            sagemaker_session=sagemaker_session)\n",
    "#running_processor.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the processed dataset\n",
    "Take a look at a few rows of the transformed dataset to make sure the preprocessing was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-06-23-50-51-679/output/train_data\n",
      "s3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-06-23-50-51-679/output/validation_data\n",
      "s3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-06-23-50-51-679/output/test_data\n"
     ]
    }
   ],
   "source": [
    "preprocessing_job_description = sklearn_processor.jobs[-1].describe()\n",
    "\n",
    "output_config = preprocessing_job_description['ProcessingOutputConfig']\n",
    "for output in output_config['Outputs']:\n",
    "    if output['OutputName'] == 'train_data':\n",
    "        preprocessed_train_data = output['S3Output']['S3Uri']\n",
    "    if output['OutputName'] == 'validation_data':\n",
    "        preprocessed_validation_data = output['S3Output']['S3Uri']        \n",
    "    if output['OutputName'] == 'test_data':\n",
    "        preprocessed_test_data = output['S3Output']['S3Uri']\n",
    "        \n",
    "print(preprocessed_train_data)\n",
    "print(preprocessed_validation_data)\n",
    "print(preprocessed_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-06 23:56:32         33 algo-1.csv\r\n",
      "2020-03-06 23:56:32         33 algo-2.csv\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 ls $preprocessed_train_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $preprocessed_validation_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $preprocessed_test_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-06-23-50-51-679/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Spark Job Has Completed</b>'.format(bucket, processing_job_name, region)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
