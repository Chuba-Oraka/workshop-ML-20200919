{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the S3 Location of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r scikit_processing_job_s3_output_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Scikit Processing Job Name: sagemaker-scikit-learn-2020-03-25-05-52-04-408\n"
     ]
    }
   ],
   "source": [
    "print('Previous Scikit Processing Job Name: {}'.format(scikit_processing_job_s3_output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-25-05-52-04-408/output/bert-train', 'S3DataDistributionType': 'FullyReplicated'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-25-05-52-04-408/output/bert-validation', 'S3DataDistributionType': 'FullyReplicated'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-25-05-52-04-408/output/bert-test', 'S3DataDistributionType': 'FullyReplicated'}}}\n"
     ]
    }
   ],
   "source": [
    "prefix_train = '{}/output/bert-train'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_validation = '{}/output/bert-validation'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_test = '{}/output/bert-test'.format(scikit_processing_job_s3_output_prefix)\n",
    "\n",
    "path_train = './{}'.format(prefix_train)\n",
    "path_validation = './{}'.format(prefix_validation)\n",
    "path_test = './{}'.format(prefix_test)\n",
    "\n",
    "train_s3_uri = 's3://{}/{}'.format(bucket, prefix_train)\n",
    "validation_s3_uri = 's3://{}/{}'.format(bucket, prefix_validation)\n",
    "test_s3_uri = 's3://{}/{}'.format(bucket, prefix_test)\n",
    "\n",
    "s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri) #, content_type='text/csv')\n",
    "s3_input_validation_data = sagemaker.s3_input(s3_data=validation_s3_uri) #, content_type='text/csv')\n",
    "s3_input_test_data = sagemaker.s3_input(s3_data=test_s3_uri) #, content_type='text/csv')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import argparse\r\n",
      "import csv\r\n",
      "import pickle as pkl\r\n",
      "import pandas as pd\r\n",
      "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\r\n",
      "import sklearn\r\n",
      "from sklearn import metrics\r\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
      "import re\r\n",
      "import glob\r\n",
      "import json\r\n",
      "import numpy as np\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'simpletransformers'])\r\n",
      "import torch\r\n",
      "import torch.distributed as dist\r\n",
      "import torch.utils.data\r\n",
      "import torch.utils.data.distributed\r\n",
      "\r\n",
      "import simpletransformers\r\n",
      "from simpletransformers.classification import ClassificationModel\r\n",
      "\r\n",
      "def load_dataset(path, sep, header):\r\n",
      "    data = pd.concat([pd.read_csv(f, sep=sep, header=header) for f in glob.glob('{}/*.csv'.format(path))], ignore_index = True)\r\n",
      "\r\n",
      "    labels = data.iloc[:,0]\r\n",
      "    features = data.drop(data.columns[0], axis=1)\r\n",
      "\r\n",
      "    if header==None:\r\n",
      "        # Adjust the column names after dropped the 0th column above\r\n",
      "        # New column names are 0 (inclusive) to len(features.columns) (exclusive)\r\n",
      "        new_column_names = list(range(0, len(features.columns)))\r\n",
      "        features.columns = new_column_names\r\n",
      "\r\n",
      "    return features, labels\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    parser.add_argument('--model-type', type=str, default='bert')\r\n",
      "    parser.add_argument('--model-name', type=str, default='bert-base-cased')\r\n",
      "    parser.add_argument('--backend', type=str, default='gloo')\r\n",
      "    parser.add_argument('--train-data', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\r\n",
      "    parser.add_argument('--validation-data', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\r\n",
      "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\r\n",
      "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\r\n",
      "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\r\n",
      "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\r\n",
      "\r\n",
      "    args, _ = parser.parse_known_args()   \r\n",
      "    model_type = args.model_type\r\n",
      "    model_name = args.model_name\r\n",
      "    backend = args.backend\r\n",
      "    train_data = args.train_data\r\n",
      "    validation_data = args.validation_data\r\n",
      "    model_dir = args.model_dir\r\n",
      "    hosts = args.hosts\r\n",
      "    current_host = args.current_host\r\n",
      "    num_gpus = args.num_gpus\r\n",
      "\r\n",
      "    # TODO:  Convert to distributed data loader\r\n",
      "    #        https://pytorch.org/tutorials/beginner/aws_distributed_training_tutorial.html\r\n",
      "    #        https://github.com/aws/sagemaker-python-sdk/issues/1110\r\n",
      "    is_distributed = len(args.hosts) > 1 and args.backend is not None\r\n",
      "    print('Distributed training - {}'.format(is_distributed))\r\n",
      "    use_cuda = args.num_gpus > 0\r\n",
      "    print('Number of gpus available - {}'.format(args.num_gpus))\r\n",
      "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\r\n",
      "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n",
      "\r\n",
      "    if is_distributed:\r\n",
      "        # Initialize the distributed environment.\r\n",
      "        world_size = len(args.hosts)\r\n",
      "        os.environ['WORLD_SIZE'] = str(world_size)\r\n",
      "        host_rank = args.hosts.index(args.current_host)\r\n",
      "        os.environ['RANK'] = str(host_rank)\r\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\r\n",
      "        print('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\r\n",
      "            args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\r\n",
      "            dist.get_rank(), args.num_gpus))\r\n",
      "\r\n",
      "    # TODO:  Change this to use SM_CHANNEL_TRAIN and DistributedDataLoader, etc\r\n",
      "    # X_train, y_train = load_dataset(train_data, ',', header=0)\r\n",
      "    # X_validation, y_validation = load_dataset(validation_data, ',', header=0)\r\n",
      "\r\n",
      "    df1 = pd.read_csv('./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz', \r\n",
      "                 delimiter='\\t', \r\n",
      "                 quoting=csv.QUOTE_NONE,\r\n",
      "                 compression='gzip',\r\n",
      "                 header=0)\r\n",
      "    print(df1.shape)\r\n",
      "\r\n",
      "    df2 = pd.read_csv('./data/amazon_reviews_us_Video_Games_v1_00.tsv.gz',\r\n",
      "                 delimiter='\\t', \r\n",
      "                 quoting=csv.QUOTE_NONE,\r\n",
      "                 compression='gzip', \r\n",
      "                 header=0)\r\n",
      "    print(df2.shape)\r\n",
      "\r\n",
      "    df = pd.concat([df1, df2])\r\n",
      "\r\n",
      "    print('YES: {}'.format(df.isna().values.any()))\r\n",
      "    df = df.dropna()\r\n",
      "    df = df.reset_index(drop=True)\r\n",
      "\r\n",
      "    # Enrich the data\r\n",
      "    df['is_positive_sentiment'] = (df['star_rating'] >= 4).astype(int)\r\n",
      "\r\n",
      "    df_bert = df[['review_body', 'is_positive_sentiment']]\r\n",
      "    df_bert.columns = ['text', 'labels']\r\n",
      "    df_bert.head(5)\r\n",
      "\r\n",
      "    print(df_bert.shape)\r\n",
      "\r\n",
      "    df_bert = df_bert #[:200100]\r\n",
      "    df_bert.shape\r\n",
      "\r\n",
      "    from sklearn.model_selection import train_test_split\r\n",
      "\r\n",
      "    df_bert_train, df_bert_holdout = train_test_split(df_bert, test_size=0.10)\r\n",
      "    df_bert_validation, df_bert_test = train_test_split(df_bert_holdout, test_size=0.50)\r\n",
      "\r\n",
      "    print(df_bert_train.shape)\r\n",
      "    print(df_bert_validation.shape)\r\n",
      "    print(df_bert_test.shape)\r\n",
      "\r\n",
      "    # TODO:  change output_dir to SM_model_dir or output_path\r\n",
      "    bert_args = {\r\n",
      "       'output_dir': model_dir, \r\n",
      "       'cache_dir': 'cache/',\r\n",
      "       'fp16': False,\r\n",
      "       'max_seq_length': 128,\r\n",
      "       'train_batch_size': 8,\r\n",
      "       'eval_batch_size': 8,\r\n",
      "       'gradient_accumulation_steps': 1,\r\n",
      "       'num_train_epochs': 1,\r\n",
      "       'weight_decay': 0,\r\n",
      "       'learning_rate': 3e-5,\r\n",
      "       'adam_epsilon': 1e-8,\r\n",
      "       'warmup_ratio': 0.06,\r\n",
      "       'warmup_steps': 0,\r\n",
      "       'max_grad_norm': 1.0,\r\n",
      "       'logging_steps': 50,\r\n",
      "       'evaluate_during_training': False,\r\n",
      "       'save_steps': 2000,\r\n",
      "       'eval_all_checkpoints': True,\r\n",
      "       'use_tensorboard': True,\r\n",
      "       'tensorboard_dir': 'tensorboard',\r\n",
      "       'overwrite_output_dir': True,\r\n",
      "       'reprocess_input_data': False,\r\n",
      "    }\r\n",
      "\r\n",
      "    bert_model = ClassificationModel(model_type='distilbert', # bert, distilbert, etc, etc.\r\n",
      "                                     model_name='distilbert-base-cased',\r\n",
      "                                     args=bert_args,\r\n",
      "                                     use_cuda=use_cuda)\r\n",
      "\r\n",
      "    bert_model.train_model(train_df=df_bert_train,\r\n",
      "                           eval_df=df_bert_validation,\r\n",
      "                           show_running_loss=True)\r\n",
      "\r\n",
      "    # TODO:  use the model_dir that is passed in through args\r\n",
      "    #        (currently SM_MODEL_DIR)\r\n",
      "#    os.makedirs(model_dir, exist_ok=True)\r\n",
      "#    model_path = os.path.join(model_dir, 'bert-model')\r\n",
      "\r\n",
      "#    pkl.dump(bert_model, open(model_path, 'wb'))\r\n",
      "#    print('Wrote model to {}'.format(model_path))\r\n",
      "   \r\n",
      "#    result, model_outputs, wrong_predictions = bert_model.eval_model(eval_df=df_bert_test, acc=sklearn.metrics.accuracy_score)\r\n",
      "\r\n",
      "#    print(result)\r\n",
      "\r\n",
      "    # Show bad predictions\r\n",
      "#    print('Number of wrong predictions: {}'.format(len(wrong_predictions)))\r\n",
      "#    print('\\n')\r\n",
      "\r\n",
      "#    for prediction in wrong_predictions:\r\n",
      "#        print(prediction.text_a)\r\n",
      "#        print('\\n')\r\n",
      "\r\n",
      "#    predictions, raw_outputs = bert_model.predict([\"\"\"I really enjoyed this item.  I highly recommend it.\"\"\"])\r\n",
      "\r\n",
      "#    print('Predictions: {}'.format(predictions))\r\n",
      "#    print('Raw outputs: {}'.format(raw_outputs))\r\n",
      "\r\n",
      "#    predictions, raw_outputs = bert_model.predict([\"\"\"This item is awful and terrible.\"\"\"])\r\n",
      "\r\n",
      "#    print('Predictions: {}'.format(predictions))\r\n",
      "#    print('Raw outputs: {}'.format(raw_outputs))\r\n"
     ]
    }
   ],
   "source": [
    "!cat src_bert/bert_reviews.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model_output_path = 's3://{}/models/tf-bert/script-mode/training-runs'.format(bucket)\n",
    "\n",
    "bert_estimator = TensorFlow(entry_point='tf_bert_reviews.py',\n",
    "                         source_dir='src_bert_tf',\n",
    "                         role=role,\n",
    "                         train_instance_count=1, # 1 is actually faster due to communication overhead with >1\n",
    "                         train_instance_type='ml.c5.18xlarge',\n",
    "                         py_version='py3',\n",
    "                         framework_version='1.15.2',\n",
    "                         output_path=model_output_path,\n",
    "#                         hyperparameters={'model_type':'bert',\n",
    "#                                          'model_name': 'bert-base-cased'},\n",
    "                         enable_cloudwatch_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_estimator.fit(inputs={'train': s3_input_train_data, \n",
    "                           'validation': s3_input_validation_data,}, \n",
    "                   wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_job_name:  tensorflow-training-2020-03-25-07-01-00-916\n"
     ]
    }
   ],
   "source": [
    "training_job_name = bert_estimator.latest_training_job.name\n",
    "print('training_job_name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "# bert_estimator = TensorFlow.attach(training_job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/tensorflow-training-2020-03-25-07-01-00-916\">Training Job</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(region, training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=tensorflow-training-2020-03-25-07-01-00-916;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/models/tf-bert/script-mode/training-runs/tensorflow-training-2020-03-25-07-01-00-916/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# This is different than the job name because we are not using ProcessingOutput's in this Spark ML case.\n",
    "training_job_s3_output_prefix = 'models/tf-bert/script-mode/training-runs/{}'.format(training_job_name)\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket, training_job_s3_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal error: An error occurred (404) when calling the HeadObject operation: Key \"models/tf-bert/script-mode/training-runs/tensorflow-training-2020-03-25-06-07-46-309/output/model.tar.gz\" does not exist\r\n"
     ]
    }
   ],
   "source": [
    "# download the model artifact from AWS S3\n",
    "!aws s3 cp $model_output_path/$training_job_name/output/model.tar.gz ./models/bert-tf/\n",
    "\n",
    "#!aws s3 cp s3://sagemaker-us-east-1-835319576252/models/tf-bert/script-mode/training-runs/tensorflow-training-2020-03-24-04-41-39-405/output/model.tar.gz ./models/bert-tf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "tar = tarfile.open('./models/bert-tf/model.tar.gz')\n",
    "tar.extractall(path='./models/bert-tf-model')\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -al ./models/bert-tf-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  We need to install tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow==1.15.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from bert import run_classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #import tensorflow as tf\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#     model = tf.train.import_meta_graph('./models/bert-tf-model/model.ckpt-100.meta')\n",
    "#     model.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "#     print(model)\n",
    "#     #print(sess.run('w1:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saver = tf.train.import_meta_graph('./models/bert-tf-model/model.ckpt-100.meta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Load the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_prediction(in_sentences):\n",
    "#   labels = [\"1\", \"2\", \"3\", \"4\", \"5\"]\n",
    "#   input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "#   input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "#   predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "#   predictions = estimator.predict(predict_input_fn)\n",
    "#   return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_sentences = [\n",
    "#   \"That movie was absolutely awful\",\n",
    "#   \"The acting was a bit lacking\",\n",
    "#   \"The film was creative and surprising\",\n",
    "#   \"Absolutely fantastic!\"\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = get_prediction(pred_sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
