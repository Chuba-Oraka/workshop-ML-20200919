{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q smdebug==0.7.2\n",
    "!pip install -q sagemaker-experiments==0.1.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the S3 Location of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%store -r scikit_processing_job_s3_output_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "scikit_processing_job_s3_output_prefix = 'TT0310826689'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(scikit_processing_job_s3_output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Scikit Processing Job Name: TT0310826689\n"
     ]
    }
   ],
   "source": [
    "# HACK:  Remove this once we test the new `05_prepare/` and remove the `is_real_example`\n",
    "\n",
    "#scikit_processing_job_s3_output_prefix = 'sagemaker-scikit-learn-2020-04-10-04-44-09-647'\n",
    "\n",
    "print('Previous Scikit Processing Job Name: {}'.format(scikit_processing_job_s3_output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_train = '{}/output/bert-train'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_validation = '{}/output/bert-validation'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_test = '{}/output/bert-test'.format(scikit_processing_job_s3_output_prefix)\n",
    "\n",
    "train_s3_uri = 's3://{}/{}'.format(bucket, prefix_train)\n",
    "validation_s3_uri = 's3://{}/{}'.format(bucket, prefix_validation)\n",
    "test_s3_uri = 's3://{}/{}'.format(bucket, prefix_test)\n",
    "\n",
    "# HACK:  TODO\n",
    "\n",
    "# train_s3_uri = 's3://data-science-on-aws/TT0310826689/bert-train'\n",
    "# validation_s3_uri = 's3://data-science-on-aws/TT0310826689/bert-validation'\n",
    "# test_s3_uri = 's3://data-science-on-aws/TT0310826689/bert-test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp --recursive s3://data-science-on-aws/TT0310826689/bert-train/ s3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-train/\n",
    "# !aws s3 cp --recursive s3://data-science-on-aws/TT0310826689/bert-validation/ s3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-validation/\n",
    "# !aws s3 cp --recursive s3://data-science-on-aws/TT0310826689/bert-test/ s3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-test/\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-train\n",
      "2020-04-13 02:48:52 2842017956 part-algo-1-amazon_reviews_us_Apparel_v1_00.tfrecord\n",
      "2020-04-13 02:48:52 3256644636 part-algo-1-amazon_reviews_us_Books_v1_01.tfrecord\n",
      "2020-04-13 02:48:52  817388348 part-algo-1-amazon_reviews_us_Digital_Music_Purchase_v1_00.tfrecord\n",
      "2020-04-13 02:48:52  392412003 part-algo-1-amazon_reviews_us_Furniture_v1_00.tfrecord\n",
      "2020-04-13 02:48:52 1293434433 part-algo-1-amazon_reviews_us_Home_Improvement_v1_00.tfrecord\n",
      "2020-04-13 02:49:01  171699748 part-algo-1-amazon_reviews_us_Luggage_v1_00.tfrecord\n",
      "2020-04-13 02:49:05  450276402 part-algo-1-amazon_reviews_us_Musical_Instruments_v1_00.tfrecord\n",
      "2020-04-13 02:49:10 1313804795 part-algo-1-amazon_reviews_us_Pet_Products_v1_00.tfrecord\n",
      "2020-04-13 02:49:15 2379851198 part-algo-1-amazon_reviews_us_Toys_v1_00.tfrecord\n",
      "2020-04-13 02:49:21 4395944875 part-algo-1-amazon_reviews_us_Wireless_v1_00.tfrecord\n",
      "2020-04-13 02:49:47 1702370740 part-algo-2-amazon_reviews_us_Automotive_v1_00.tfrecord\n",
      "2020-04-13 02:50:05 1681960843 part-algo-2-amazon_reviews_us_Books_v1_02.tfrecord\n",
      "2020-04-13 02:50:15   50969090 part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2020-04-13 02:50:17   70316784 part-algo-2-amazon_reviews_us_Gift_Card_v1_00.tfrecord\n",
      "2020-04-13 02:50:20 3030381387 part-algo-2-amazon_reviews_us_Home_v1_00.tfrecord\n",
      "2020-04-13 02:50:20   48926304 part-algo-2-amazon_reviews_us_Major_Appliances_v1_00.tfrecord\n",
      "2020-04-13 02:50:22 1297907882 part-algo-2-amazon_reviews_us_Office_Products_v1_00.tfrecord\n",
      "2020-04-13 02:50:32 2125453822 part-algo-2-amazon_reviews_us_Shoes_v1_00.tfrecord\n",
      "2020-04-13 02:50:50 2569817433 part-algo-2-amazon_reviews_us_Video_DVD_v1_00.tfrecord\n",
      "2020-04-13 02:50:57  879004721 part-algo-3-amazon_reviews_us_Baby_v1_00.tfrecord\n",
      "2020-04-13 02:51:21  904382714 part-algo-3-amazon_reviews_us_Camera_v1_00.tfrecord\n",
      "2020-04-13 02:51:22 1935776473 part-algo-3-amazon_reviews_us_Digital_Video_Download_v1_00.tfrecord\n",
      "2020-04-13 02:51:30 1178550706 part-algo-3-amazon_reviews_us_Grocery_v1_00.tfrecord\n",
      "2020-04-13 02:51:41  850161526 part-algo-3-amazon_reviews_us_Jewelry_v1_00.tfrecord\n",
      "2020-04-13 02:51:45 2402943768 part-algo-3-amazon_reviews_us_Mobile_Apps_v1_00.tfrecord\n",
      "2020-04-13 02:51:57 1139168876 part-algo-3-amazon_reviews_us_Outdoors_v1_00.tfrecord\n",
      "2020-04-13 02:51:59  175529858 part-algo-3-amazon_reviews_us_Software_v1_00.tfrecord\n",
      "2020-04-13 02:52:02  900310265 part-algo-3-amazon_reviews_us_Video_Games_v1_00.tfrecord\n",
      "2020-04-13 02:52:04 2510200513 part-algo-4-amazon_reviews_us_Beauty_v1_00.tfrecord\n",
      "2020-04-13 02:52:10 6210395292 part-algo-4-amazon_reviews_us_Digital_Ebook_Purchase_v1_00.tfrecord\n",
      "2020-04-13 02:52:21   71730527 part-algo-4-amazon_reviews_us_Digital_Video_Games_v1_00.tfrecord\n",
      "2020-04-13 02:52:23 2620777353 part-algo-4-amazon_reviews_us_Health_Personal_Care_v1_00.tfrecord\n",
      "2020-04-13 02:52:24 2413444587 part-algo-4-amazon_reviews_us_Kitchen_v1_00.tfrecord\n",
      "2020-04-13 02:52:43   52423605 part-algo-4-amazon_reviews_us_Mobile_Electronics_v1_00.tfrecord\n",
      "2020-04-13 02:52:45 3430205662 part-algo-4-amazon_reviews_us_PC_v1_00.tfrecord\n",
      "2020-04-13 02:53:07 2372618152 part-algo-4-amazon_reviews_us_Sports_v1_00.tfrecord\n",
      "2020-04-13 02:53:29  202050384 part-algo-4-amazon_reviews_us_Video_v1_00.tfrecord\n",
      "2020-04-13 02:53:33 5157162534 part-algo-5-amazon_reviews_us_Books_v1_00.tfrecord\n",
      "2020-04-13 02:53:34 2594939522 part-algo-5-amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tfrecord\n",
      "2020-04-13 02:54:04 1544626099 part-algo-5-amazon_reviews_us_Electronics_v1_00.tfrecord\n",
      "2020-04-13 02:54:11  360162781 part-algo-5-amazon_reviews_us_Home_Entertainment_v1_00.tfrecord\n",
      "2020-04-13 02:54:20 1256250570 part-algo-5-amazon_reviews_us_Lawn_and_Garden_v1_00.tfrecord\n",
      "2020-04-13 02:54:34 2464401663 part-algo-5-amazon_reviews_us_Music_v1_00.tfrecord\n",
      "2020-04-13 02:54:41   43089602 part-algo-5-amazon_reviews_us_Personal_Care_Appliances_v1_00.tfrecord\n",
      "2020-04-13 02:54:41  857024888 part-algo-5-amazon_reviews_us_Tools_v1_00.tfrecord\n",
      "2020-04-13 02:54:42  470796922 part-algo-5-amazon_reviews_us_Watches_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(train_s3_uri)\n",
    "!aws s3 ls $train_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-train', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-validation', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-test', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n"
     ]
    }
   ],
   "source": [
    "s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri, distribution='ShardedByS3Key') \n",
    "s3_input_validation_data = sagemaker.s3_input(s3_data=validation_s3_uri, distribution='ShardedByS3Key')\n",
    "s3_input_test_data = sagemaker.s3_input(s3_data=test_s3_uri, distribution='ShardedByS3Key')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import time\r\n",
      "import random\r\n",
      "import pandas as pd\r\n",
      "from glob import glob\r\n",
      "import pprint\r\n",
      "import argparse\r\n",
      "import json\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers==2.8.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.1.0.1.0.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'smdebug==0.7.2'])\r\n",
      "from transformers import DistilBertTokenizer\r\n",
      "from transformers import TFDistilBertForSequenceClassification\r\n",
      "from transformers import TextClassificationPipeline\r\n",
      "from transformers.configuration_distilbert import DistilBertConfig\r\n",
      "\r\n",
      "CLASSES = [1, 2, 3, 4, 5]\r\n",
      "\r\n",
      "def select_data_and_label_from_record(record):\r\n",
      "    x = {\r\n",
      "        'input_ids': record['input_ids'],\r\n",
      "        'input_mask': record['input_mask'],\r\n",
      "        'segment_ids': record['segment_ids']\r\n",
      "    }\r\n",
      "\r\n",
      "    y = record['label_ids']\r\n",
      "\r\n",
      "    return (x, y)\r\n",
      "\r\n",
      "\r\n",
      "def file_based_input_dataset_builder(channel,\r\n",
      "                                     input_filenames,\r\n",
      "                                     pipe_mode,\r\n",
      "                                     is_training,\r\n",
      "                                     drop_remainder,\r\n",
      "                                     batch_size,\r\n",
      "                                     epochs,\r\n",
      "                                     steps_per_epoch,\r\n",
      "                                     max_seq_length):\r\n",
      "\r\n",
      "    # For training, we want a lot of parallel reading and shuffling.\r\n",
      "    # For eval, we want no shuffling and parallel reading doesn't matter.\r\n",
      "\r\n",
      "    if pipe_mode:\r\n",
      "        print('***** Using pipe_mode with channel {}'.format(channel))\r\n",
      "        from sagemaker_tensorflow import PipeModeDataset\r\n",
      "        dataset = PipeModeDataset(channel=channel,\r\n",
      "                                  record_format='TFRecord')\r\n",
      "    else:\r\n",
      "        print('***** Using input_filenames {}'.format(input_filenames))\r\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\r\n",
      "\r\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch)\r\n",
      "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
      "\r\n",
      "    name_to_features = {\r\n",
      "      \"input_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"input_mask\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"segment_ids\": tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\r\n",
      "    }\r\n",
      "\r\n",
      "    def _decode_record(record, name_to_features):\r\n",
      "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\r\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\r\n",
      "        # TODO:  wip/bert/bert_attention_head_view/train.py\r\n",
      "        # Convert input_ids into input_tokens with DistilBert vocabulary \r\n",
      "        #  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\r\n",
      "#              hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\r\n",
      "        return record\r\n",
      "    \r\n",
      "    dataset = dataset.apply(\r\n",
      "        tf.data.experimental.map_and_batch(\r\n",
      "          lambda record: _decode_record(record, name_to_features),\r\n",
      "          batch_size=batch_size,\r\n",
      "          drop_remainder=drop_remainder,\r\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\r\n",
      "\r\n",
      "    dataset.cache()\r\n",
      "\r\n",
      "    if is_training:\r\n",
      "        dataset = dataset.shuffle(seed=42,\r\n",
      "                                  buffer_size=1000,\r\n",
      "                                  reshuffle_each_iteration=True)\r\n",
      "\r\n",
      "    return dataset\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument('--train-data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_TRAIN'])\r\n",
      "    parser.add_argument('--validation-data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_VALIDATION'])\r\n",
      "    parser.add_argument('--test-data',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_CHANNEL_TEST'])\r\n",
      "    parser.add_argument('--model-dir', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_MODEL_DIR'])\r\n",
      "    parser.add_argument('--output-data-dir',\r\n",
      "                        type=str,\r\n",
      "                        default=os.environ['SM_OUTPUT_DATA_DIR'])\r\n",
      "    parser.add_argument('--hosts', \r\n",
      "                        type=list, \r\n",
      "                        default=json.loads(os.environ['SM_HOSTS']))\r\n",
      "    parser.add_argument('--current-host', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CURRENT_HOST'])    \r\n",
      "    parser.add_argument('--num-gpus', \r\n",
      "                        type=int, \r\n",
      "                        default=os.environ['SM_NUM_GPUS'])\r\n",
      "    parser.add_argument('--use-xla',\r\n",
      "                        type=bool,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--use-amp',\r\n",
      "                        type=bool,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--max-seq-length',\r\n",
      "                        type=int,\r\n",
      "                        default=128)\r\n",
      "    parser.add_argument('--train-batch-size',\r\n",
      "                        type=int,\r\n",
      "                        default=128)\r\n",
      "    parser.add_argument('--validation-batch-size',\r\n",
      "                        type=int,\r\n",
      "                        default=256)\r\n",
      "    parser.add_argument('--test-batch-size',\r\n",
      "                        type=int,\r\n",
      "                        default=256)\r\n",
      "    parser.add_argument('--epochs',\r\n",
      "                        type=int,\r\n",
      "                        default=2)\r\n",
      "    parser.add_argument('--learning-rate',\r\n",
      "                        type=float,\r\n",
      "                        default=0.00003)\r\n",
      "    parser.add_argument('--epsilon',\r\n",
      "                        type=float,\r\n",
      "                        default=0.00000001)\r\n",
      "    parser.add_argument('--train-steps-per-epoch',\r\n",
      "                        type=int,\r\n",
      "                        default=1000)\r\n",
      "    parser.add_argument('--validation-steps',\r\n",
      "                        type=int,\r\n",
      "                        default=1000)\r\n",
      "    parser.add_argument('--test-steps',\r\n",
      "                        type=int,\r\n",
      "                        default=1000)\r\n",
      "    parser.add_argument('--freeze-bert-layer',\r\n",
      "                        type=bool,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--enable-sagemaker-debugger',\r\n",
      "                        type=bool,\r\n",
      "                        default=False)\r\n",
      "    parser.add_argument('--run-validation',\r\n",
      "                        type=bool,\r\n",
      "                        default=False)    \r\n",
      "    parser.add_argument('--run-test',\r\n",
      "                        type=bool,\r\n",
      "                        default=False)    \r\n",
      "    parser.add_argument('--run-sample-predictions',\r\n",
      "                        type=bool,\r\n",
      "                        default=False)\r\n",
      "    \r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    print(\"Args:\") \r\n",
      "    print(args)\r\n",
      "    \r\n",
      "    env_var = os.environ \r\n",
      "    print(\"Environment Variables:\") \r\n",
      "    pprint.pprint(dict(env_var), width = 1) \r\n",
      "\r\n",
      "    train_data = args.train_data\r\n",
      "    validation_data = args.validation_data\r\n",
      "    test_data = args.test_data\r\n",
      "    model_dir = args.model_dir\r\n",
      "    output_data_dir = args.output_data_dir\r\n",
      "    hosts = args.hosts\r\n",
      "    current_host = args.current_host\r\n",
      "    num_gpus = args.num_gpus\r\n",
      "    use_xla = args.use_xla\r\n",
      "    use_amp = args.use_amp\r\n",
      "    max_seq_length = args.max_seq_length\r\n",
      "    train_batch_size = args.train_batch_size\r\n",
      "    validation_batch_size = args.validation_batch_size\r\n",
      "    test_batch_size = args.test_batch_size\r\n",
      "    epochs = args.epochs\r\n",
      "    learning_rate = args.learning_rate\r\n",
      "    epsilon = args.epsilon\r\n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\r\n",
      "    validation_steps = args.validation_steps\r\n",
      "    test_steps = args.test_steps\r\n",
      "    freeze_bert_layer = args.freeze_bert_layer\r\n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\r\n",
      "    run_validation = args.run_validation\r\n",
      "    run_test = args.run_test\r\n",
      "    run_sample_predictions = args.run_sample_predictions\r\n",
      "\r\n",
      "    # Determine if PipeMode is enabled \r\n",
      "    pipe_mode_str = os.environ.get('SM_INPUT_DATA_CONFIG', '')\r\n",
      "    pipe_mode = (pipe_mode_str.find('Pipe') >= 0)\r\n",
      "    print('Using pipe_mode: {}'.format(pipe_mode))\r\n",
      " \r\n",
      "    # Model Output \r\n",
      "    transformer_pretrained_model_path = os.path.join(model_dir, 'transformer/pretrained')\r\n",
      "    os.makedirs(transformer_pretrained_model_path, exist_ok=True)\r\n",
      "\r\n",
      "    # SavedModel Output\r\n",
      "    tensorflow_saved_model_path = os.path.join(model_dir, 'saved_model/0')\r\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=True)\r\n",
      "\r\n",
      "    # Tensorboard Logs \r\n",
      "    tensorboard_logs_path = os.path.join(output_data_dir, 'tensorboard') \r\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=True)\r\n",
      "\r\n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\r\n",
      "    # smdebug currently (0.7.2) does not support MultiWorkerMirroredStrategy()\r\n",
      "    # distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\r\n",
      "    with distributed_strategy.scope():\r\n",
      "        tf.config.optimizer.set_jit(use_xla)\r\n",
      "        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": use_amp})\r\n",
      "\r\n",
      "        train_data_filenames = glob(os.path.join(train_data, '*.tfrecord'))\r\n",
      "        print('train_data_filenames {}'.format(train_data_filenames))\r\n",
      "        train_dataset = file_based_input_dataset_builder(\r\n",
      "            channel='train',\r\n",
      "            input_filenames=train_data_filenames,\r\n",
      "            pipe_mode=pipe_mode,\r\n",
      "            is_training=True,\r\n",
      "            drop_remainder=False,\r\n",
      "            batch_size=train_batch_size,\r\n",
      "            epochs=epochs,\r\n",
      "            steps_per_epoch=train_steps_per_epoch,\r\n",
      "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "        tokenizer = None\r\n",
      "        config = None\r\n",
      "        model = None\r\n",
      "\r\n",
      "        # This is required when launching many instances at once...  the urllib request seems to get denied periodically\r\n",
      "        successful_download = False\r\n",
      "        retries = 0\r\n",
      "        while (retries < 5 and not successful_download):\r\n",
      "            try:\r\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "                config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\r\n",
      "                                                          num_labels=len(CLASSES))\r\n",
      "                model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\r\n",
      "                                                                              config=config)\r\n",
      "                successful_download = True\r\n",
      "                print('Sucessfully downloaded after {} retries.'.format(retries))\r\n",
      "            except:\r\n",
      "                retries = retries + 1\r\n",
      "                random_sleep = random.randint(1, 30)\r\n",
      "                print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\r\n",
      "                time.sleep(random_sleep)\r\n",
      "\r\n",
      "        if not tokenizer or not model or not config:\r\n",
      "            print('Not properly initialized...')\r\n",
      "\r\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\r\n",
      "        if use_amp:\r\n",
      "            # loss scaling is currently required when using mixed precision\r\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\r\n",
      "\r\n",
      "        callbacks = []\r\n",
      "        print('freeze BERT layer {}'.format(args.freeze_bert_layer))\r\n",
      "        print('enable SM_DEBUGGER {}'.format(args.enable_sagemaker_debugger))\r\n",
      "        if enable_sagemaker_debugger:\r\n",
      "            print('*** DEBUGGING ***')\r\n",
      "            import smdebug.tensorflow as smd\r\n",
      "            # This assumes that we specified debugger_hook_config\r\n",
      "            callback = smd.KerasHook.create_from_json_file()\r\n",
      "            print(callback)\r\n",
      "            print('*** CALLBACK {} ***'.format(callback))\r\n",
      "            callbacks.append(callback)\r\n",
      "            optimizer = callback.wrap_optimizer(optimizer)\r\n",
      "        else:\r\n",
      "            callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\r\n",
      "            callbacks.append(callback)\r\n",
      "            \r\n",
      "        print('*** OPTIMIZER {} ***'.format(optimizer))\r\n",
      "        \r\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\n",
      "\r\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
      "        print('Trained model {}'.format(model))\r\n",
      "\r\n",
      "        if run_validation:\r\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, '*.tfrecord'))\r\n",
      "            print('validation_data_filenames {}'.format(validation_data_filenames))\r\n",
      "            validation_dataset = file_based_input_dataset_builder(\r\n",
      "                channel='validation',\r\n",
      "                input_filenames=validation_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=False,\r\n",
      "                drop_remainder=False,\r\n",
      "                batch_size=validation_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=validation_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "            train_and_validation_history = model.fit(train_dataset,\r\n",
      "                                                     shuffle=True,\r\n",
      "                                                     epochs=epochs,\r\n",
      "                                                     steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                                     validation_data=validation_dataset,\r\n",
      "                                                     validation_steps=validation_steps,\r\n",
      "                                                     callbacks=callbacks)\r\n",
      "            print(train_and_validation_history)\r\n",
      "        else: # Not running validation\r\n",
      "            train_history = model.fit(train_dataset,\r\n",
      "                                      shuffle=True,\r\n",
      "                                      epochs=epochs,\r\n",
      "                                      steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                      callbacks=callbacks)\r\n",
      "            print(train_history)\r\n",
      "\r\n",
      "        if run_test:\r\n",
      "            test_data_filenames = glob(os.path.join(test_data, '*.tfrecord'))\r\n",
      "            print('test_data_filenames {}'.format(test_data_filenames))\r\n",
      "            test_dataset = file_based_input_dataset_builder(\r\n",
      "                channel='test',\r\n",
      "                input_filenames=test_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=False,\r\n",
      "                drop_remainder=False,\r\n",
      "                batch_size=test_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=test_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "            test_history = model.evaluate(test_dataset,\r\n",
      "                                          steps=test_steps,\r\n",
      "                                          callbacks=callbacks)\r\n",
      "            print(test_history)\r\n",
      "\r\n",
      "            \r\n",
      "        # Save the fine-tuned Transformers Model\r\n",
      "        model.save_pretrained(transformer_pretrained_model_path)\r\n",
      "\r\n",
      "        # Save the TensorFlow SavedModel\r\n",
      "        print('tensorflow_saved_model_path {}'.format(tensorflow_saved_model_path))        \r\n",
      "        model.save(tensorflow_saved_model_path, save_format='tf')\r\n",
      "\r\n",
      "    if run_sample_predictions:\r\n",
      "        loaded_model = TFDistilBertForSequenceClassification.from_pretrained(transformer_pretrained_model_path,\r\n",
      "                                                                       id2label={\r\n",
      "                                                                        0: 1,\r\n",
      "                                                                        1: 2,\r\n",
      "                                                                        2: 3,\r\n",
      "                                                                        3: 4,\r\n",
      "                                                                        4: 5\r\n",
      "                                                                       },\r\n",
      "                                                                       label2id={\r\n",
      "                                                                        1: 0,\r\n",
      "                                                                        2: 1,\r\n",
      "                                                                        3: 2,\r\n",
      "                                                                        4: 3,\r\n",
      "                                                                        5: 4\r\n",
      "                                                                       })\r\n",
      "\r\n",
      "        tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "\r\n",
      "        if num_gpus >= 1:\r\n",
      "            inference_device = 0 # GPU 0\r\n",
      "        else:\r\n",
      "            inference_device = -1 # CPU\r\n",
      "        print('inference_device {}'.format(inference_device))\r\n",
      "\r\n",
      "        inference_pipeline = TextClassificationPipeline(model=loaded_model, \r\n",
      "                                                        tokenizer=tokenizer,\r\n",
      "                                                        framework='tf',\r\n",
      "                                                        device=inference_device)  \r\n",
      "\r\n",
      "        print(\"\"\"I loved it!  I will recommend this to everyone.\"\"\", inference_pipeline(\"\"\"I loved it!  I will recommend this to everyone.\"\"\"))\r\n",
      "        print(\"\"\"It's OK.\"\"\", inference_pipeline(\"\"\"It's OK.\"\"\"))\r\n",
      "        print(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\", inference_pipeline(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"))\r\n"
     ]
    }
   ],
   "source": [
    "!cat src/NEW_tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Debugger Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.debugger import Rule\n",
    "from sagemaker.debugger import rule_configs\n",
    "from sagemaker.debugger import CollectionConfig\n",
    " \n",
    "\n",
    "rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                'collection_names': 'losses,metrics',\n",
    "                'use_losses_collection': 'true',\n",
    "                'num_steps': '5',\n",
    "                'diff_percent': '5'\n",
    "            },\n",
    "            collections_to_save=[\n",
    "                CollectionConfig(name='losses',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '100',\n",
    "                                 }),\n",
    "                CollectionConfig(name='metrics',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '100',\n",
    "                                 })\n",
    "            ]\n",
    "        ),\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.overtraining(),\n",
    "            rule_parameters={\n",
    "                'collection_names': 'losses,metrics',\n",
    "                'patience_train': '2',\n",
    "                'patience_validation': '2',\n",
    "                'delta': '0.5'\n",
    "            },\n",
    "            collections_to_save=[\n",
    "                CollectionConfig(name='losses',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '100',\n",
    "                                 }),\n",
    "                CollectionConfig(name='metrics',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '100',\n",
    "                                 })\n",
    "            ]\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "learning_rate=0.00001 # tune from 1e-5 to 5e-5, step by 1e-5\n",
    "epsilon=0.00000001\n",
    "train_batch_size=128\n",
    "validation_batch_size=128\n",
    "test_batch_size=128\n",
    "train_steps_per_epoch=100\n",
    "validation_steps=100\n",
    "test_steps=100\n",
    "train_instance_count=1\n",
    "train_instance_type='ml.p3.2xlarge'\n",
    "train_volume_size=1024\n",
    "use_xla=True\n",
    "use_amp=True\n",
    "max_seq_length=128\n",
    "freeze_bert_layer=False\n",
    "enable_sagemaker_debugger=False                    # Enable SM Debugger\n",
    "#use_parameter_server=(train_instance_count >= 2) # Use Parameter Server if distributed cluster\n",
    "input_mode='File'                                 # 'File' or 'Pipe' Mode\n",
    "run_validation=True\n",
    "run_test=True\n",
    "run_sample_predictions=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track Experiments\n",
    "import time\n",
    "unique_id = '{}-{}'.format(input_mode, int(time.time()))\n",
    "\n",
    "from smexperiments.experiment import Experiment\n",
    "experiment=Experiment.create(\n",
    "    experiment_name='train-reviews-bert-{}'.format(unique_id),\n",
    "    description='Train Reviews BERT', \n",
    "    sagemaker_boto_client=sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-Reviews-BERT-Tracker-File-1586900136\n"
     ]
    }
   ],
   "source": [
    "from smexperiments.tracker import Tracker\n",
    "tracker_display_name='Train-Reviews-BERT-Tracker-{}'.format(unique_id)\n",
    "print(tracker_display_name)\n",
    "\n",
    "tracker = Tracker.create(display_name=tracker_display_name, sagemaker_boto_client=sm)\n",
    "tracker.log_parameters({\n",
    "    'epochs': epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'epsilon': epsilon,\n",
    "    'train_batch_size': train_batch_size,\n",
    "    'validation_batch_size': validation_batch_size,\n",
    "    'test_batch_size': test_batch_size,\n",
    "    'train_steps_per_epoch': train_steps_per_epoch,\n",
    "    'validation_steps': validation_steps,\n",
    "    'test_steps': test_steps,\n",
    "    'train_instance_count': train_instance_count,\n",
    "    'train_instance_type': train_instance_type,\n",
    "    'train_volume_size': train_volume_size,\n",
    "    'use_xla': use_xla,\n",
    "    'use_amp': use_amp,\n",
    "    'max_seq_length': max_seq_length,\n",
    "    'freeze_bert_layer': freeze_bert_layer,\n",
    "    'enable_sagemaker_debugger': enable_sagemaker_debugger,\n",
    "#    'use_parameter_server': use_parameter_server,\n",
    "    'input_mode': input_mode, # 'File' or 'Pipe'\n",
    "    'run_validation': run_validation,\n",
    "    'run_test': run_test,\n",
    "    'run_sample_predictions': run_sample_predictions,    \n",
    "})\n",
    "# we can log the s3 uri to the dataset we just uploaded\n",
    "tracker.log_input(name='reviews_dataset_train', media_type='s3/uri', value=train_s3_uri)\n",
    "tracker.log_input(name='reviews_dataset_validation', media_type='s3/uri', value=validation_s3_uri)\n",
    "tracker.log_input(name='reviews_dataset_test', media_type='s3/uri', value=test_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smexperiments.trial import Trial\n",
    "trial_name='train-reviews-bert-training-job-{}'.format(unique_id)\n",
    "trial = Trial.create(trial_name=trial_name, experiment_name=experiment.experiment_name, sagemaker_boto_client=sm)\n",
    "trial.add_trial_component(tracker.trial_component)\n",
    "trial_component_display_name='Train-Reviews-BERT-Trial-{}'.format(unique_id)\n",
    "experiment_config={'ExperimentName': experiment.experiment_name,\n",
    "                   'TrialName': trial.trial_name,\n",
    "                   'TrialComponentDisplayName': trial_component_display_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.debugger import DebuggerHookConfig\n",
    "\n",
    "metrics_definitions = [\n",
    "     {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "]\n",
    "\n",
    "hook_config = DebuggerHookConfig(\n",
    "    hook_parameters={\n",
    "        \"save_interval\": \"100\", # number of steps\n",
    "        \"export_tensorboard\": \"true\",\n",
    "    })\n",
    "\n",
    "estimator = TensorFlow(entry_point='NEW_tf_bert_reviews.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       train_instance_count=train_instance_count, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_volume_size=train_volume_size,\n",
    "                       py_version='py3',\n",
    "                       framework_version='2.1.0',\n",
    "                       hyperparameters={'epochs': epochs,\n",
    "                                        'learning_rate': learning_rate,\n",
    "                                        'epsilon': epsilon,\n",
    "                                        'train-batch-size': train_batch_size,\n",
    "                                        'validation-batch-size': validation_batch_size,\n",
    "                                        'test-batch-size': test_batch_size,                                             \n",
    "                                        'train-steps-per-epoch': train_steps_per_epoch,\n",
    "                                        'validation-steps': validation_steps,\n",
    "                                        'test-steps': test_steps,\n",
    "                                        'use-xla': use_xla,\n",
    "                                        'use-amp': use_amp,                                             \n",
    "                                        'max-seq-length': max_seq_length,\n",
    "                                        'freeze-bert-layer': freeze_bert_layer,\n",
    "                                        'enable-sagemaker-debugger': enable_sagemaker_debugger,\n",
    "                                        'run-validation': run_validation,\n",
    "                                        'run-test': run_test,\n",
    "                                        'run-sample-predictions': run_sample_predictions},\n",
    "#                       distributions={'parameter_server': {'enabled': use_parameter_server}},\n",
    "                       input_mode=input_mode,\n",
    "                       metric_definitions=metrics_definitions,\n",
    "#                       rules=rules,\n",
    "#                       debugger_hook_config=hook_config,                       \n",
    "                       train_max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 seconds per minute\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: tensorflow-training-2020-04-14-21-32-07-606\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs={'train': s3_input_train_data, \n",
    "                      'validation': s3_input_validation_data,\n",
    "                      'test': s3_input_test_data\n",
    "                     },\n",
    "                     experiment_config=experiment_config,                   \n",
    "                     wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_job_name:  tensorflow-training-2020-04-14-21-32-07-606\n"
     ]
    }
   ],
   "source": [
    "training_job_name = estimator.latest_training_job.name\n",
    "print('training_job_name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/tensorflow-training-2020-04-14-21-32-07-606\">Training Job</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(region, training_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=tensorflow-training-2020-04-14-21-32-07-606;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, training_job_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-828802286385/tensorflow-training-2020-04-14-21-32-07-606/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket, training_job_name, region)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Track Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "# # Might need to convert ' => \"\n",
    "# search_expression = {\n",
    "#     'Filters':[\n",
    "#         {\n",
    "#             'Name': 'DisplayName',\n",
    "#             'Operator': 'Equals',\n",
    "#             'Value': 'Training'\n",
    "#         }\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    experiment_name=experiment.experiment_name,\n",
    "#    search_expression=search_expression,\n",
    "    sort_by='metrics.validation:accuracy.max',\n",
    "    sort_order='Descending',\n",
    "    metric_names=['validation:accuracy'],\n",
    "    parameter_names=['epochs', 'train_batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>epochs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensorflow-training-2020-04-14-19-27-45-657-aw...</td>\n",
       "      <td>Train-Reviews-BERT-Trial-File-1586892463</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:828802286385:train...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TrialComponent-2020-04-14-192743-vlcr</td>\n",
       "      <td>Train-Reviews-BERT-Tracker-File-1586892463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName  \\\n",
       "0  tensorflow-training-2020-04-14-19-27-45-657-aw...   \n",
       "1              TrialComponent-2020-04-14-192743-vlcr   \n",
       "\n",
       "                                  DisplayName  \\\n",
       "0    Train-Reviews-BERT-Trial-File-1586892463   \n",
       "1  Train-Reviews-BERT-Tracker-File-1586892463   \n",
       "\n",
       "                                           SourceArn  epochs  \n",
       "0  arn:aws:sagemaker:us-east-1:828802286385:train...     1.0  \n",
       "1                                                NaN     NaN  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analytics_table = trial_component_analytics.dataframe()\n",
    "analytics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialComponentName</th>\n",
       "      <th>DisplayName</th>\n",
       "      <th>SourceArn</th>\n",
       "      <th>SageMaker.ImageUri</th>\n",
       "      <th>SageMaker.InstanceCount</th>\n",
       "      <th>SageMaker.InstanceType</th>\n",
       "      <th>SageMaker.VolumeSizeInGB</th>\n",
       "      <th>enable-sagemaker-debugger</th>\n",
       "      <th>epochs</th>\n",
       "      <th>epsilon</th>\n",
       "      <th>...</th>\n",
       "      <th>sagemaker_region</th>\n",
       "      <th>sagemaker_submit_directory</th>\n",
       "      <th>test-batch-size</th>\n",
       "      <th>test-steps</th>\n",
       "      <th>train-batch-size</th>\n",
       "      <th>train-steps-per-epoch</th>\n",
       "      <th>use-amp</th>\n",
       "      <th>use-xla</th>\n",
       "      <th>validation-batch-size</th>\n",
       "      <th>validation-steps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TrialComponent-2020-04-14-192743-vlcr</td>\n",
       "      <td>Train-Reviews-BERT-Tracker-File-1586892463</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensorflow-training-2020-04-14-19-27-45-657-aw...</td>\n",
       "      <td>Train-Reviews-BERT-Trial-File-1586892463</td>\n",
       "      <td>arn:aws:sagemaker:us-east-1:828802286385:train...</td>\n",
       "      <td>763104351884.dkr.ecr.us-east-1.amazonaws.com/t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ml.p3.8xlarge</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>true</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>\"us-east-1\"</td>\n",
       "      <td>\"s3://sagemaker-us-east-1-828802286385/tensorf...</td>\n",
       "      <td>128.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "      <td>128.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  TrialComponentName  \\\n",
       "0              TrialComponent-2020-04-14-192743-vlcr   \n",
       "1  tensorflow-training-2020-04-14-19-27-45-657-aw...   \n",
       "\n",
       "                                  DisplayName  \\\n",
       "0  Train-Reviews-BERT-Tracker-File-1586892463   \n",
       "1    Train-Reviews-BERT-Trial-File-1586892463   \n",
       "\n",
       "                                           SourceArn  \\\n",
       "0                                                NaN   \n",
       "1  arn:aws:sagemaker:us-east-1:828802286385:train...   \n",
       "\n",
       "                                  SageMaker.ImageUri  SageMaker.InstanceCount  \\\n",
       "0                                                NaN                      NaN   \n",
       "1  763104351884.dkr.ecr.us-east-1.amazonaws.com/t...                      1.0   \n",
       "\n",
       "  SageMaker.InstanceType  SageMaker.VolumeSizeInGB enable-sagemaker-debugger  \\\n",
       "0                    NaN                       NaN                       NaN   \n",
       "1          ml.p3.8xlarge                    1024.0                      true   \n",
       "\n",
       "   epochs       epsilon  ... sagemaker_region  \\\n",
       "0     NaN           NaN  ...              NaN   \n",
       "1     1.0  1.000000e-08  ...      \"us-east-1\"   \n",
       "\n",
       "                          sagemaker_submit_directory  test-batch-size  \\\n",
       "0                                                NaN              NaN   \n",
       "1  \"s3://sagemaker-us-east-1-828802286385/tensorf...            128.0   \n",
       "\n",
       "  test-steps train-batch-size train-steps-per-epoch use-amp  use-xla  \\\n",
       "0        NaN              NaN                   NaN     NaN      NaN   \n",
       "1      100.0            128.0                 100.0    true     true   \n",
       "\n",
       "  validation-batch-size validation-steps  \n",
       "0                   NaN              NaN  \n",
       "1                 128.0            100.0  \n",
       "\n",
       "[2 rows x 31 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# search_expression={\n",
    "#     'Filters':[{\n",
    "#         'Name': 'Parents.TrialName',\n",
    "#         'Operator': 'Equals',\n",
    "#         'Value': ??\n",
    "#     }]\n",
    "# },\n",
    "\n",
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=experiment.experiment_name,\n",
    "#    search_expression=search_expression,\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")\n",
    "lineage_table.dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving and Analyzing Debugger Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Rule Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'RuleConfigurationName': 'LossNotDecreasing',\n",
       "  'RuleEvaluationStatus': 'InProgress',\n",
       "  'LastModifiedTime': datetime.datetime(2020, 4, 14, 19, 27, 48, 68000, tzinfo=tzlocal())},\n",
       " {'RuleConfigurationName': 'Overtraining',\n",
       "  'RuleEvaluationStatus': 'InProgress',\n",
       "  'LastModifiedTime': datetime.datetime(2020, 4, 14, 19, 27, 48, 68000, tzinfo=tzlocal())}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.latest_training_job.rule_job_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting to analysis, here are some notes on concepts being used in Debugger that help with analysis.\n",
    "* **Trial** - object that is a center piece of Debugger API when it comes to getting access to tensors. It is a top level abstract that represents a single run of a training job. All tensors emitted by training job are associated with its trial.\n",
    "* **Step** - object that represents next level of abstraction. In Debugger - step is a representation of a single batch of a training job. Each trial has multiple steps. Each tensor is associated with multiple steps - having a particular value at each of the steps.\n",
    "* **Tensor** - object that represent actual tensor saved during training job. Note, a tensor can be a 1-D scaler, as well (ie. loss is stored as a scalar).\n",
    "\n",
    "For more details on these concepts as well as on Debugger API in general (including examples) please refer to Debugger Analysis API documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14 19:27:57.857 ip-172-16-9-13:31306 INFO s3_trial.py:42] Loading trial debug-output at path s3://sagemaker-us-east-1-828802286385/tensorflow-training-2020-04-14-19-27-45-657/debug-output\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-54ebcd544841>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# this is where we create a Trial object that allows access to saved tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_job_debugger_artifacts_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/trials/utils.py\u001b[0m in \u001b[0;36mcreate_trial\u001b[0;34m(path, name, **kwargs)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0ms3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_s3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ms3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mS3Trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbucket_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalTrial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/trials/s3_trial.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, bucket_name, prefix_name, range_steps, check, index_mode, cache)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"s3://\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucket_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS3IndexReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/trials/trial.py\u001b[0m in \u001b[0;36m_load_collections\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0m_wait_for_collection_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait for the first collection file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0m_wait_for_collection_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# wait for all the collection files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/trials/trial.py\u001b[0m in \u001b[0;36m_wait_for_collection_files\u001b[0;34m(number_of_collection_file_to_wait_for)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcollection_files\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnumber_of_collection_file_to_wait_for\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0m_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_training_ended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                     \u001b[0;34m\"\"\" _fetch should have returned all the collection files if the training job has ended \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/trials/trial.py\u001b[0m in \u001b[0;36m_fetch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mnonlocal\u001b[0m \u001b[0mcollection_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mnonlocal\u001b[0m \u001b[0mnum_times_before_warning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mcollection_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_collection_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0mnum_times_before_warning\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/trials/s3_trial.py\u001b[0m in \u001b[0;36m_get_collection_files\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mget_path_to_collections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mstart_after_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcollection_files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/core/s3_utils.py\u001b[0m in \u001b[0;36mlist_s3_objects\u001b[0;34m(bucket, prefix, start_after_key, delimiter)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mstart_after_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mListRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPrefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStartAfter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_after_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mobjects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_s3_prefixes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mlast_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/core/s3_utils.py\u001b[0m in \u001b[0;36m_list_s3_prefixes\u001b[0;34m(list_info)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# list_info will be a list of ListRequest objects. Returns list of lists of files for each request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_list_s3_prefixes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mS3Handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_prefixes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/core/access_layer/s3handler.py\u001b[0m in \u001b[0;36mlist_prefixes\u001b[0;34m(list_requests)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_requests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS3Handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_prefix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/smdebug/core/access_layer/s3handler.py\u001b[0m in \u001b[0;36mlist_prefix\u001b[0;34m(lr)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 )\n\u001b[1;32m    100\u001b[0m                 \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CommonPrefixes\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/paginate.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inject_starting_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m             \u001b[0mparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_parsed_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_request\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/paginate.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, current_kwargs)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcurrent_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_parsed_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m             http, parsed_response = self._make_request(\n\u001b[0;32m--> 613\u001b[0;31m                 operation_model, request_dict, request_context)\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m         self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             self.meta.events.emit(\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36mmake_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    100\u001b[0m         logger.debug(\"Making request for %s with params: %s\",\n\u001b[1;32m    101\u001b[0m                      operation_model, request_dict)\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         success_response, exception = self._get_response(\n\u001b[0;32m--> 135\u001b[0;31m             request, operation_model, context)\n\u001b[0m\u001b[1;32m    136\u001b[0m         while self._needs_retry(attempts, operation_model, request_dict,\n\u001b[1;32m    137\u001b[0m                                 success_response, exception):\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_get_response\u001b[0;34m(self, request, operation_model, context)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;31m# If no exception occurs then exception is None.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         success_response, exception = self._do_get_response(\n\u001b[0;32m--> 167\u001b[0;31m             request, operation_model)\n\u001b[0m\u001b[1;32m    168\u001b[0m         kwargs_to_emit = {\n\u001b[1;32m    169\u001b[0m             \u001b[0;34m'response_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_do_get_response\u001b[0;34m(self, request, operation_model)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_non_none_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhttp_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/endpoint.py\u001b[0m in \u001b[0;36m_send\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/httpsession.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0mpreload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m             )\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    670\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    419\u001b[0m                     \u001b[0;31m# Python 3 (including for exceptions like SystemExit).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m                     \u001b[0;31m# Otherwise it looks like a bug in the code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSocketError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1329\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1331\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1332\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1007\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "# this is where we create a Trial object that allows access to saved tensors\n",
    "trial = create_trial(estimator.latest_job_debugger_artifacts_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in trial.tensor_names():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_data(trial, tensor_name, batch_index, steps_range, mode):\n",
    "    tensor = trial.tensor(tensor_name)\n",
    "    vals = []\n",
    "    for step_num in steps_range:\n",
    "        val = tensor.value(step_num=step_num, mode=mode)[batch_index]\n",
    "        vals.append(val)\n",
    "    return pd.DataFrame(columns=['steps', tensor_name], data=list(zip(steps_range, vals)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.tensorflow import modes\n",
    "\n",
    "# Below we select the very first tensor from every batch.\n",
    "# Feel free to modify this and select another tensor from the batch.\n",
    "batch_index = 0\n",
    "\n",
    "# This is a name of a tensor to analyze.\n",
    "tensor_name = 'accuracy'\n",
    "\n",
    "steps = 0\n",
    "while steps == 0:\n",
    "    # trial.steps return all steps that have been downloaded by Debugger to date.\n",
    "    # It doesn't represent all steps that are to be available once training job is complete -\n",
    "    # it is a snapshot of a current state of the training job. If you call it after training job is done\n",
    "    # you will get all tensors available at once.\n",
    "    steps = trial.steps()\n",
    "    print('Waiting for tensors to become available...')\n",
    "    time.sleep(3)\n",
    "print('\\nDone')\n",
    "\n",
    "print('Getting tensors and plotting...')\n",
    "rendered_steps = []\n",
    "\n",
    "# trial.loaded_all_steps is a way to keep monitoring for a state of a training job as seen by Debugger.\n",
    "# When SageMaker completes training job Debugger, and trial, becomes aware of it.\n",
    "\n",
    "loaded_all_steps = False\n",
    "while not loaded_all_steps:\n",
    "    loaded_all_steps = trial.loaded_all_steps\n",
    "    steps = trial.steps()\n",
    "    # show diff between lists\n",
    "    steps_to_render = list(set(steps).symmetric_difference(set(rendered_steps)))\n",
    "    \n",
    "    data = get_data(trial=trial, \n",
    "                    tensor_name=tensor_name, \n",
    "                    batch_index=0, \n",
    "                    steps_range=steps_to_render, \n",
    "                    mode=modes.GLOBAL)\n",
    "    print(data)\n",
    "    data.plot(x='steps', y=tensor_name)\n",
    "    \n",
    "    rendered_steps.extend(steps_to_render)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.tensorflow import modes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Let's visualize weights of the first convolutional layer as they progressively change through training.\n",
    "tensor_name = 'loss'\n",
    "\n",
    "num_batches = trial.tensor(tensor_name).value(step_num=steps[0]).shape[0]\n",
    "for batch_index in range(0, num_batches):\n",
    "    steps_range = trial.tensor(tensor_name).steps()\n",
    "    print(steps_range)\n",
    "    data = get_data(trial=trial, \n",
    "                    tensor_name=tensor_name, \n",
    "                    batch_index=batch_index, \n",
    "                    steps_range=steps_range, \n",
    "                    mode=modes.GLOBAL)\n",
    "    print(data)\n",
    "    data.plot(x='steps', y=tensor_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.tensorflow import modes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "#from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tensor_name = 'accuracy'\n",
    "\n",
    "num_batches = trial.tensor(tensor_name).value(step_num=steps[0]).shape[0]\n",
    "for batch_index in range(0, num_batches):\n",
    "    steps_range = trial.tensor(tensor_name).steps()\n",
    "    print(steps_range)\n",
    "    data = get_data(trial=trial, \n",
    "                    tensor_name=tensor_name, \n",
    "                    batch_index=batch_index, \n",
    "                    steps_range=steps_range, \n",
    "                    mode=modes.GLOBAL)\n",
    "    print(data)\n",
    "    data.plot(x='steps', y=tensor_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Lambda Function to Stop the TrainingJob Early\n",
    "In your AWS console, go to Lambda Management Console,\n",
    "Create a new function by hitting Create Function,\n",
    "Choose the language as Python 3.7 and put in the following sample code for stopping the training job if one of the Rule statuses is \"IssuesFound\".\n",
    "\n",
    "### CloudWatch Events for Rules\n",
    "Rule status changes in a training job trigger CloudWatch Events. These events can be acted upon by configuring a CloudWatch Rule (different from Amazon SageMaker Debugger Rule) to trigger each time a Debugger Rule changes status. In this notebook we'll go through how you can create a CloudWatch Rule to direct Training Job State change events to a lambda function that stops the training job in case a rule triggers and has status \"IssuesFound\".\n",
    "\n",
    "Create a new execution role for the Lambda, and\n",
    "In your IAM console, search for the role and attach \"AmazonSageMakerFullAccess\" policy to the role. This is needed for the code in your Lambda function to stop the training job.\n",
    "\n",
    "\n",
    "### Create a CloudWatch Rule to Trigger a Lamba\n",
    "In your AWS Console, go to CloudWatch and select Rule from the left column,\n",
    "Hit Create Rule. The console will redirect you to the Rule creation page,\n",
    "For the Service Name, select \"SageMaker\".\n",
    "For the Event Type, select \"SageMaker Training Job State Change\".\n",
    "In the Targets select the Lambda function you created above, and\n",
    "For this example notebook, we'll leave everything as is.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "import json\n",
    "import boto3\n",
    "import logging\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    training_job_name = event.get(\"detail\").get(\"TrainingJobName\")\n",
    "    eval_statuses = event.get(\"detail\").get(\"DebugRuleEvaluationStatuses\", None)\n",
    "\n",
    "    if eval_statuses is None or len(eval_statuses) == 0:\n",
    "        logging.info(\"Couldn't find any debug rule statuses, skipping...\")\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps('Nothing to do')\n",
    "        }\n",
    "\n",
    "    client = boto3.client('sagemaker')\n",
    "\n",
    "    for status in eval_statuses:\n",
    "        if status.get(\"RuleEvaluationStatus\") == \"IssuesFound\":\n",
    "            logging.info(\n",
    "                'Evaluation of rule configuration {} resulted in \"IssuesFound\". '\n",
    "                'Attempting to stop training job {}'.format(\n",
    "                    status.get(\"RuleConfigurationName\"), training_job_name\n",
    "                )\n",
    "            )\n",
    "            try:\n",
    "                client.stop_training_job(\n",
    "                    TrainingJobName=training_job_name\n",
    "                )\n",
    "            except Exception as e:\n",
    "                logging.error(\n",
    "                    \"Encountered error while trying to \"\n",
    "                    \"stop training job {}: {}\".format(\n",
    "                        training_job_name, str(e)\n",
    "                    )\n",
    "                )\n",
    "                raise e\n",
    "    return None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a new execution role for the Lambda, and\n",
    "* In your IAM console, search for the role and attach \"AmazonSageMakerFullAccess\" policy to the role. This is needed for the code in your Lambda function to stop the training job.\n",
    "\n",
    "#### Create a CloudWatch Rule\n",
    "\n",
    "* In your AWS Console, go to CloudWatch and select Rule from the left column,\n",
    "* Hit Create Rule. The console will redirect you to the Rule creation page,\n",
    " * For the Service Name, select \"SageMaker\".\n",
    " * For the Event Type, select \"SageMaker Training Job State Change\".\n",
    "* In the Targets select the Lambda function you created above, and\n",
    "* For this example notebook, we'll leave everything as is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker kicked off rule evaluation jobs, one for each of the SageMaker rules - `Overtraining` and `LossNotDecreasing` as specified in the estimator. If we setup a CloudWatch Rule to stop the training job, we would see the `TrainingJobStatus` change to `Stopped` once the `RuleEvaluationStatus` for changes to `IssuesFound`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This utility gives the link to monitor the CW event\n",
    "# def _get_rule_job_name(training_job_name, rule_configuration_name, rule_job_arn):\n",
    "#         \"\"\"Helper function to get the rule job name\"\"\"\n",
    "#         return \"{}-{}-{}\".format(\n",
    "#             training_job_name[:26], rule_configuration_name[:26], rule_job_arn[-8:]\n",
    "#         )\n",
    "    \n",
    "# def _get_cw_url_for_rule_job(rule_job_name, region):\n",
    "#     return \"https://{}.console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/ProcessingJobs;prefix={};streamFilter=typeLogStreamPrefix\".format(region, region, rule_job_name)\n",
    "\n",
    "\n",
    "# def get_rule_jobs_cw_urls(estimator):\n",
    "#     region = boto3.Session().region_name\n",
    "#     training_job = estimator.latest_training_job\n",
    "#     training_job_name = training_job.describe()[\"TrainingJobName\"]\n",
    "#     rule_eval_statuses = training_job.describe()[\"DebugRuleEvaluationStatuses\"]\n",
    "    \n",
    "#     result={}\n",
    "#     for status in rule_eval_statuses:\n",
    "#         if status.get(\"RuleEvaluationJobArn\", None) is not None:\n",
    "#             rule_job_name = _get_rule_job_name(training_job_name, status[\"RuleConfigurationName\"], status[\"RuleEvaluationJobArn\"])\n",
    "#             result[status[\"RuleConfigurationName\"]] = _get_cw_url_for_rule_job(rule_job_name, region)\n",
    "#     return result\n",
    "\n",
    "# get_rule_jobs_cw_urls(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimator.latest_training_job.describe()[\"TrainingJobStatus\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameter Ranges to Explore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note:  enable_sagemaker_debugger must be disabled or these jobs will fail \n",
    "#        on the call to `config_from_jsonfile()`\n",
    "\n",
    "from sagemaker.tuner import IntegerParameter\n",
    "from sagemaker.tuner import ContinuousParameter\n",
    "from sagemaker.tuner import CategoricalParameter\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "                                                \n",
    "hyperparameter_ranges = {\n",
    "#    'use-xla': CategoricalParameter([True, False]),\n",
    "#    'use-amp': CategoricalParameter([True, False]),\n",
    "    'epochs': IntegerParameter(2, 16, scaling_type='Logarithmic'),\n",
    "    'learning_rate': ContinuousParameter(0.00001, 0.00005, scaling_type='Linear'),\n",
    "#    'train-steps-per-epoch': IntegerParameter(10, 1000, scaling_type='Logarithmic'),    \n",
    "    'train-batch-size': CategoricalParameter([128, 256, 512]),\n",
    "    'freeze-bert-layer': CategoricalParameter([True, False])\n",
    "}\n",
    "\n",
    "objective_metric_name = 'validation:accuracy'\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name=objective_metric_name,\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metrics_definitions,\n",
    "    max_jobs=6,\n",
    "    max_parallel_jobs=2,\n",
    "    strategy='Bayesian',\n",
    "    early_stopping_type='Auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Tuning Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:_TuningJob.start_new!!!\n",
      "INFO:sagemaker:Creating hyperparameter tuning job with name: tensorflow-training-200414-2142\n"
     ]
    }
   ],
   "source": [
    "tuner.fit({'train': s3_input_train_data, \n",
    "           'validation': s3_input_validation_data,\n",
    "           'test': s3_input_test_data\n",
    "          }, include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Check Tuning Job Status\n",
    "Re-run this cell to track the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "InProgress\n",
      "\n",
      "\n",
      "{'CreationTime': datetime.datetime(2020, 4, 14, 21, 42, 35, 39000, tzinfo=tzlocal()),\n",
      " 'HyperParameterTuningJobArn': 'arn:aws:sagemaker:us-east-1:828802286385:hyper-parameter-tuning-job/tensorflow-training-200414-2142',\n",
      " 'HyperParameterTuningJobConfig': {'HyperParameterTuningJobObjective': {'MetricName': 'validation:accuracy',\n",
      "                                                                        'Type': 'Maximize'},\n",
      "                                   'ParameterRanges': {'CategoricalParameterRanges': [{'Name': 'train-batch-size',\n",
      "                                                                                       'Values': ['\"128\"',\n",
      "                                                                                                  '\"256\"',\n",
      "                                                                                                  '\"512\"']},\n",
      "                                                                                      {'Name': 'freeze-bert-layer',\n",
      "                                                                                       'Values': ['\"True\"',\n",
      "                                                                                                  '\"False\"']}],\n",
      "                                                       'ContinuousParameterRanges': [{'MaxValue': '5e-05',\n",
      "                                                                                      'MinValue': '1e-05',\n",
      "                                                                                      'Name': 'learning_rate',\n",
      "                                                                                      'ScalingType': 'Linear'}],\n",
      "                                                       'IntegerParameterRanges': [{'MaxValue': '16',\n",
      "                                                                                   'MinValue': '2',\n",
      "                                                                                   'Name': 'epochs',\n",
      "                                                                                   'ScalingType': 'Logarithmic'}]},\n",
      "                                   'ResourceLimits': {'MaxNumberOfTrainingJobs': 6,\n",
      "                                                      'MaxParallelTrainingJobs': 2},\n",
      "                                   'Strategy': 'Bayesian',\n",
      "                                   'TrainingJobEarlyStoppingType': 'Auto'},\n",
      " 'HyperParameterTuningJobName': 'tensorflow-training-200414-2142',\n",
      " 'HyperParameterTuningJobStatus': 'InProgress',\n",
      " 'LastModifiedTime': datetime.datetime(2020, 4, 14, 21, 42, 35, 39000, tzinfo=tzlocal()),\n",
      " 'ObjectiveStatusCounters': {'Failed': 0, 'Pending': 0, 'Succeeded': 0},\n",
      " 'ResponseMetadata': {'HTTPHeaders': {'content-length': '3681',\n",
      "                                      'content-type': 'application/x-amz-json-1.1',\n",
      "                                      'date': 'Tue, 14 Apr 2020 21:42:35 GMT',\n",
      "                                      'x-amzn-requestid': 'e0409cee-608b-478d-9e00-3436f7b2f4c2'},\n",
      "                      'HTTPStatusCode': 200,\n",
      "                      'RequestId': 'e0409cee-608b-478d-9e00-3436f7b2f4c2',\n",
      "                      'RetryAttempts': 0},\n",
      " 'TrainingJobDefinition': {'AlgorithmSpecification': {'MetricDefinitions': [{'Name': 'train:loss',\n",
      "                                                                             'Regex': 'loss: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'train:accuracy',\n",
      "                                                                             'Regex': 'accuracy: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'validation:loss',\n",
      "                                                                             'Regex': 'val_loss: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'validation:accuracy',\n",
      "                                                                             'Regex': 'val_accuracy: '\n",
      "                                                                                      '([0-9\\\\.]+)'},\n",
      "                                                                            {'Name': 'ObjectiveMetric',\n",
      "                                                                             'Regex': 'val_accuracy: '\n",
      "                                                                                      '([0-9\\\\.]+)'}],\n",
      "                                                      'TrainingImage': '763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-gpu-py3',\n",
      "                                                      'TrainingInputMode': 'File'},\n",
      "                           'EnableInterContainerTrafficEncryption': False,\n",
      "                           'EnableManagedSpotTraining': False,\n",
      "                           'EnableNetworkIsolation': False,\n",
      "                           'InputDataConfig': [{'ChannelName': 'train',\n",
      "                                                'DataSource': {'S3DataSource': {'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                                                'S3DataType': 'S3Prefix',\n",
      "                                                                                'S3Uri': 's3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-train'}}},\n",
      "                                               {'ChannelName': 'validation',\n",
      "                                                'DataSource': {'S3DataSource': {'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                                                'S3DataType': 'S3Prefix',\n",
      "                                                                                'S3Uri': 's3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-validation'}}},\n",
      "                                               {'ChannelName': 'test',\n",
      "                                                'DataSource': {'S3DataSource': {'S3DataDistributionType': 'ShardedByS3Key',\n",
      "                                                                                'S3DataType': 'S3Prefix',\n",
      "                                                                                'S3Uri': 's3://sagemaker-us-east-1-828802286385/TT0310826689/output/bert-test'}}}],\n",
      "                           'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-828802286385/'},\n",
      "                           'ResourceConfig': {'InstanceCount': 1,\n",
      "                                              'InstanceType': 'ml.p3.2xlarge',\n",
      "                                              'VolumeSizeInGB': 1024},\n",
      "                           'RoleArn': 'arn:aws:iam::828802286385:role/TeamRole',\n",
      "                           'StaticHyperParameters': {'_tuning_objective_metric': 'validation:accuracy',\n",
      "                                                     'enable-sagemaker-debugger': 'false',\n",
      "                                                     'epsilon': '1e-08',\n",
      "                                                     'max-seq-length': '128',\n",
      "                                                     'model_dir': '\"s3://sagemaker-us-east-1-828802286385/tensorflow-training-2020-04-14-21-35-53-981/model\"',\n",
      "                                                     'run-sample-predictions': 'true',\n",
      "                                                     'run-test': 'true',\n",
      "                                                     'run-validation': 'true',\n",
      "                                                     'sagemaker_container_log_level': '20',\n",
      "                                                     'sagemaker_enable_cloudwatch_metrics': 'false',\n",
      "                                                     'sagemaker_estimator_class_name': '\"TensorFlow\"',\n",
      "                                                     'sagemaker_estimator_module': '\"sagemaker.tensorflow.estimator\"',\n",
      "                                                     'sagemaker_job_name': '\"tensorflow-training-2020-04-14-21-42-34-300\"',\n",
      "                                                     'sagemaker_program': '\"NEW_tf_bert_reviews.py\"',\n",
      "                                                     'sagemaker_region': '\"us-east-1\"',\n",
      "                                                     'sagemaker_submit_directory': '\"s3://sagemaker-us-east-1-828802286385/tensorflow-training-2020-04-14-21-42-34-300/source/sourcedir.tar.gz\"',\n",
      "                                                     'test-batch-size': '128',\n",
      "                                                     'test-steps': '100',\n",
      "                                                     'train-steps-per-epoch': '100',\n",
      "                                                     'use-amp': 'true',\n",
      "                                                     'use-xla': 'true',\n",
      "                                                     'validation-batch-size': '128',\n",
      "                                                     'validation-steps': '100'},\n",
      "                           'StoppingCondition': {'MaxRuntimeInSeconds': 7200}},\n",
      " 'TrainingJobStatusCounters': {'Completed': 0,\n",
      "                               'InProgress': 0,\n",
      "                               'NonRetryableError': 0,\n",
      "                               'RetryableError': 0,\n",
      "                               'Stopped': 0}}\n",
      "Not yet complete, but {} jobs have completed.\n",
      "No training jobs have reported results yet.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "tuning_job_name = tuner.latest_tuning_job.job_name\n",
    "\n",
    "job_description = sm.describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuning_job_name\n",
    ")\n",
    "\n",
    "status = job_description['HyperParameterTuningJobStatus']\n",
    "\n",
    "print('\\n')\n",
    "print(status)\n",
    "print('\\n')\n",
    "pprint(job_description)\n",
    "\n",
    "if status != 'Completed':\n",
    "    job_count = job_description['TrainingJobStatusCounters']['Completed']\n",
    "    print('Not yet complete, but {} jobs have completed.')\n",
    "    \n",
    "    if job_description.get('BestTrainingJob', None):\n",
    "        print(\"Best candidate:\")\n",
    "        pprint(job_description['BestTrainingJob']['TrainingJobName'])\n",
    "        pprint(job_description['BestTrainingJob']['FinalHyperParameterTuningJobObjectiveMetric'])\n",
    "    else:\n",
    "        print(\"No training jobs have reported results yet.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Tuning Jobs\n",
    "Note:  This will fail at first.  Please wait about 30-60 seconds and re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>epochs</th>\n",
       "      <th>freeze-bert-layer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>train-batch-size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>tensorflow-training-200414-2142-002-1a86aaea</td>\n",
       "      <td>InProgress</td>\n",
       "      <td>None</td>\n",
       "      <td>6.0</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>\"128\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>tensorflow-training-200414-2142-001-cc9acb6a</td>\n",
       "      <td>InProgress</td>\n",
       "      <td>None</td>\n",
       "      <td>3.0</td>\n",
       "      <td>\"True\"</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>\"128\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FinalObjectiveValue TrainingEndTime  \\\n",
       "0                None            None   \n",
       "1                None            None   \n",
       "\n",
       "                                TrainingJobName TrainingJobStatus  \\\n",
       "0  tensorflow-training-200414-2142-002-1a86aaea        InProgress   \n",
       "1  tensorflow-training-200414-2142-001-cc9acb6a        InProgress   \n",
       "\n",
       "  TrainingStartTime  epochs freeze-bert-layer  learning_rate train-batch-size  \n",
       "0              None     6.0           \"False\"       0.000011            \"128\"  \n",
       "1              None     3.0            \"True\"       0.000037            \"128\"  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "\n",
    "hp_results = HyperparameterTuningJobAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    hyperparameter_tuning_job_name=tuning_job_name\n",
    ")\n",
    "\n",
    "df_results = hp_results.dataframe()\n",
    "\n",
    "df_results.sort_values('FinalObjectiveValue', ascending=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the Best Candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>epochs</th>\n",
       "      <th>freeze-bert-layer</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>train-batch-size</th>\n",
       "      <th>train-steps-per-epoch</th>\n",
       "      <th>use-amp</th>\n",
       "      <th>use-xla</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>tensorflow-training-200414-1929-002-8516e6d0</td>\n",
       "      <td>InProgress</td>\n",
       "      <td>None</td>\n",
       "      <td>16.0</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>\"256\"</td>\n",
       "      <td>16.0</td>\n",
       "      <td>\"False\"</td>\n",
       "      <td>\"True\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FinalObjectiveValue TrainingEndTime  \\\n",
       "0                None            None   \n",
       "\n",
       "                                TrainingJobName TrainingJobStatus  \\\n",
       "0  tensorflow-training-200414-1929-002-8516e6d0        InProgress   \n",
       "\n",
       "  TrainingStartTime  epochs freeze-bert-layer  learning_rate train-batch-size  \\\n",
       "0              None    16.0           \"False\"       0.000047            \"256\"   \n",
       "\n",
       "   train-steps-per-epoch  use-amp use-xla  \n",
       "0                   16.0  \"False\"  \"True\"  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results.sort_values('FinalObjectiveValue', ascending=0).head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/hyper-tuning-jobs/tensorflow-training-200414-1929\">Hyper-Parameter Tuning Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/hyper-tuning-jobs/{}\">Hyper-Parameter Tuning Job</a></b>'.format(region, tuning_job_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = './models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model and output artifacts from AWS S3\n",
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz $models_dir/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "#!ls -al ./models\n",
    "\n",
    "tar = tarfile.open('{}/model.tar.gz'.format(models_dir))\n",
    "tar.extractall(path=models_dir)\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -al $models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow and Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install -q pip --upgrade\n",
    "!pip install -q wrapt --upgrade --ignore-installed\n",
    "!pip install -q tensorflow==2.1.0 --upgrade --ignore-installed\n",
    "!pip install -q transformers==2.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model_dir = '{}/transformer/pretrained/'.format(models_dir)\n",
    "\n",
    "!ls -al $transformer_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat $transformer_model_dir/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained(transformer_model_dir,\n",
    "                                                                     id2label={\n",
    "                                                                       0: 1,\n",
    "                                                                       1: 2,\n",
    "                                                                       2: 3,\n",
    "                                                                       3: 4,\n",
    "                                                                       4: 5\n",
    "                                                                     },\n",
    "                                                                     label2id={\n",
    "                                                                       1: 0,\n",
    "                                                                       2: 1,\n",
    "                                                                       3: 2,\n",
    "                                                                       4: 3,\n",
    "                                                                       5: 4\n",
    "                                                                     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "inference_device = -1 # CPU: -1, GPU: 0\n",
    "print('inference_device {}'.format(inference_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "inference_pipeline = TextClassificationPipeline(model=loaded_model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                                framework='tf',\n",
    "                                                device=inference_device) # -1 is CPU, >= 0 is GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"I loved it!  I will recommend this to everyone.\"\"\"\n",
    "print(review, inference_pipeline(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = \"\"\"Really bad.  I hope they don't make this anymore.\"\"\"\n",
    "print(review, inference_pipeline(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uncomment to Start Tensorboard\n",
    "Let's start Tensorboard and point to the tensorboard logs that we downloaded from S3 directly.\n",
    "\n",
    "Note:  If you pointed Tensorboard to S3 directly, you must prepend this command with `S3_REGION=[your-region]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dir = './outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive s3://$bucket/$training_job_name/debug-output/events/ $outputs_dir/events/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -al $outputs_dir/events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!S3_REGION=us-east-1 tensorboard --port 6006 --logdir s3://$bucket/$training_job_name/debug-output/events/  # <== MAKE SURE YOU INCLUDE THE TRAILING `/`\n",
    "!S3_REGION=us-east-1 tensorboard --port 6006 --logdir $outputs_dir/events/ # <== MAKE SURE YOU INCLUDE THE TRAILING `/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Tensorboard is running locally on your SageMaker Notebook instance, it is reading the training logs from Amazon S3.\n",
    "\n",
    "Navigate to https://workshop.notebook.[your-region].sagemaker.aws/proxy/6006/  <== MAKE SURE YOU INCLUDE THE TRAILING SLASH\n",
    "\n",
    "_Note:  Make sure you copy the trailing `/` in the link above.  If you see no data, you are likely not using the correct S3 bucket above._\n",
    "\n",
    "![Tensorboard](img/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Tensorboard\n",
    "Once you are done, hit `Kernel => Stop` to stop the running `Tensorboard` process in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
