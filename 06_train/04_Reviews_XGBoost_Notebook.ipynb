{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q boto3\n",
    "#!pip install -q scikit-learn==0.20.3\n",
    "#!pip install -q nltk==3.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the datasets from S3 to this notebook instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $S3_BUCKET/feature-store/amazon-reviews/balanced-tfidf-without-header/data.csv\n",
    "\n",
    "prefix_train = 'feature-store/amazon-reviews/csv/balanced-tfidf-without-header/train'\n",
    "prefix_validation = 'feature-store/amazon-reviews/csv/balanced-tfidf-without-header/validation'\n",
    "prefix_test = 'feature-store/amazon-reviews/csv/balanced-tfidf-without-header/test'\n",
    "\n",
    "balanced_tfidf_without_header_train_path = './{}/data.csv'.format(prefix_train)\n",
    "balanced_tfidf_without_header_validation_path = './{}/data.csv'.format(prefix_validation)\n",
    "balanced_tfidf_without_header_test_path = './{}/data.csv'.format(prefix_test)\n",
    "\n",
    "import os\n",
    "os.makedirs(prefix_train, exist_ok=True)\n",
    "os.makedirs(prefix_validation, exist_ok=True)\n",
    "os.makedirs(prefix_test, exist_ok=True)\n",
    "\n",
    "balanced_tfidf_without_header_train_s3_uri = 's3://{}/{}/data.csv'.format(bucket, prefix_train)\n",
    "balanced_tfidf_without_header_validation_s3_uri = 's3://{}/{}/data.csv'.format(bucket, prefix_validation)\n",
    "balanced_tfidf_without_header_test_s3_uri = 's3://{}/{}/data.csv'.format(bucket, prefix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $balanced_tfidf_without_header_train_s3_uri $balanced_tfidf_without_header_train_path\n",
    "!aws s3 cp $balanced_tfidf_without_header_validation_s3_uri $balanced_tfidf_without_header_validation_path\n",
    "!aws s3 cp $balanced_tfidf_without_header_test_s3_uri $balanced_tfidf_without_header_test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "_Note:  `header=None`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, sep, header):\n",
    "    data = pd.read_csv(path, sep=sep, header=header)\n",
    "\n",
    "    labels = data.iloc[:,0]\n",
    "    features = data.drop(data.columns[0], axis=1)\n",
    "    \n",
    "    if header==None:\n",
    "        # Adjust the column names after dropped the 0th column above\n",
    "        # New column names are 0 (inclusive) to len(features.columns) (exclusive)\n",
    "        new_column_names = list(range(0, len(features.columns)))\n",
    "        features.columns = new_column_names\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transformed features (is_positive_sentiment, f0, f1, ...)\n",
    "X_train, y_train = load_dataset(path=balanced_tfidf_without_header_train_path, sep=',', header=None)\n",
    "X_validation, y_validation = load_dataset(path=balanced_tfidf_without_header_validation_path, sep=',', header=None)\n",
    "X_test, y_test = load_dataset(path=balanced_tfidf_without_header_test_path, sep=',', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model with XGBoost\n",
    "This may take a few minutes.  Please be patient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q xgboost==0.90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "_This will take a few minutes.  Please be patient._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "objective  = 'binary:logistic'\n",
    "max_depth  = 5\n",
    "num_round  = 1\n",
    "\n",
    "model = XGBClassifier(objective=objective,\n",
    "                      num_round=num_round,\n",
    "                      max_depth=max_depth)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "# See https://xgboost.readthedocs.io/en/latest/tutorials/saving_model.html\n",
    "# Need to save with joblib or pickle.  `xgb.save_model()` does not save feature_names\n",
    "model_dir  = './models/notebook/'\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_dir, 'xgboost-model')\n",
    "pkl.dump(model, open(model_path, 'wb'))\n",
    "print('Wrote model to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:  Explain Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restore Model \n",
    "This simulates restoring a model within an application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    model_path = os.path.join(model_dir, 'xgboost-model')\n",
    "    model = pkl.load(open(model_path, 'rb'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir  = './models/notebook/'\n",
    "model_restored = model_fn(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the feature importance for this model\n",
    "TODO:  Display the values of the features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import xgboost\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,12))\n",
    "xgboost.plot_importance(model_restored, \n",
    "                        importance_type='gain', \n",
    "                        max_num_features=30, \n",
    "                        height=0.8, \n",
    "                        ax=ax, \n",
    "                        show_values = True)\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:  Perform hyperparamter tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "auc = model_restored.score(X_validation, y_validation)\n",
    "print('Validation AUC: ', auc)\n",
    "\n",
    "preds_validation = model_restored.predict(X_validation)\n",
    "print('Validation Accuracy: ', accuracy_score(y_validation, preds_validation))\n",
    "print('Validation Precision: ', precision_score(y_validation, preds_validation, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_validation, preds_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm_validation = confusion_matrix(y_validation, preds_validation)\n",
    "df_cm_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm_validation, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "# TODO:  Add labels to each quadrant (False, True / False, True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "auc = model_restored.score(X_test, y_test)\n",
    "print('Test AUC ', auc)\n",
    "\n",
    "preds_test = model_restored.predict(X_test)\n",
    "print('Test Accuracy: ', accuracy_score(y_test, preds_test))\n",
    "print('Test Precision: ', precision_score(y_test, preds_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm_test = confusion_matrix(y_test, preds_test)\n",
    "df_cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(df_cm_test, annot=True, annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "# TODO:  Add labels to each quadrant (False, True / False, True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict in Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `feature_transform_fn()` function (same used during `prepare` phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def feature_transform_fn(df_text, column_name, n_components):\n",
    "    text_processors = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                'tfidfvectorizer',\n",
    "                TfidfVectorizer(\n",
    "                    max_df=0.25,                                       \n",
    "                    min_df=.0025,\n",
    "                    analyzer='word',\n",
    "                    max_features=10000\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    column_transformer = ColumnTransformer(\n",
    "        transformers=[('text_processing', text_processors, df_text.columns.get_loc(column_name))]\n",
    "    )\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('column_transformer',\n",
    "             column_transformer), ('svd', TruncatedSVD(n_components=n_components)),\n",
    "            ('standardscaler', StandardScaler())\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $S3_BUCKET/feature-store/amazon-reviews/balanced-raw-with-header/data.csv\n",
    "\n",
    "prefix_raw = 'feature-store/amazon-reviews/csv/scrubbed-raw-with-header'\n",
    "\n",
    "scrubbed_raw_path = './{}/data.csv'.format(prefix_raw)\n",
    "\n",
    "import os\n",
    "os.makedirs(prefix_raw, exist_ok=True)\n",
    "\n",
    "scrubbed_raw_s3_uri = 's3://{}/{}/data.csv'.format(bucket, prefix_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $scrubbed_raw_s3_uri $scrubbed_raw_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y_raw = load_dataset(path=scrubbed_raw_path, sep=',', header=0)\n",
    "X_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_tfidf = feature_transform_fn(X_raw, 'review_body', 300).fit_transform(X_raw)\n",
    "df_tfidf = pd.DataFrame(np_tfidf)\n",
    "df_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model_restored.predict(df_tfidf)\n",
    "df_preds = pd.DataFrame(preds)\n",
    "df_preds.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "auc = model_restored.score(df_tfidf, y_raw)\n",
    "print('Test AUC ', auc)\n",
    "\n",
    "preds_raw = model_restored.predict(df_tfidf)\n",
    "print('Test Accuracy: ', accuracy_score(y_raw, preds_raw))\n",
    "print('Test Precision: ', precision_score(y_raw, preds_raw, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(scrubbed_raw_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:  Deploy the model\n",
    "1. Create a SageMaker endpoint using this model.\n",
    "\n",
    "2. Define the predict function to transform raw text into TF/IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO:  1) update this to do TF/IDF\n",
    "# #        2) use this in other versions of the model\n",
    "# # Derived from the following:\n",
    "# #   https://aim357.readthedocs.io/en/latest/GluePySparkMLFeatureEngineering/GluePySparkMLFeatureEngineering.html#deepar-deep-dive\n",
    "\n",
    "# class XGBoostPredictor(sagemaker.predictor.RealTimePredictor):\n",
    "\n",
    "#     def __init__(self, *args, **kwargs):\n",
    "#         super().__init__(*args, content_type=sagemaker.content_types.CONTENT_TYPE_CSV, **kwargs)\n",
    "\n",
    "#     def predict(self, df):\n",
    "#         \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "#         corresponding category listed in `cat`.\n",
    "\n",
    "#         df -- `pandas.Series` object, the data frame to predict\n",
    "\n",
    "#         Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "#         \"\"\"\n",
    "#         req = self.__encode_request(df)\n",
    "#         # TODO:  change this\n",
    "#         res = predict(req) # super(DeepARPredictor, self).predict(req)\n",
    "#         return self.__decode_response(res)\n",
    "\n",
    "#     def __encode_request(self, df):\n",
    "#         # TODO:  Add transform\n",
    "# #        df = feature_transform\n",
    "#         encoded_request = pd.DataFrame([0,1])\n",
    "#         return encoded_request\n",
    "\n",
    "#     def __decode_response(self, response):\n",
    "#         predictions = response\n",
    "#         return pd.DataFrame(data=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_endpoint_name = prefix + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "# xgb_predictor = xgb_estimator.deploy(\n",
    "#                      initial_instance_count=1, \n",
    "#                      instance_type='local',\n",
    "# #                     instance_type='ml.m4.xlarge',\n",
    "#                      predictor_cls=XGBoostPredictor,\n",
    "#                      endpoint_name=xgb_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
