{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the S3 Location of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r scikit_processing_job_s3_output_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Scikit Processing Job Name: sagemaker-scikit-learn-2020-03-30-03-34-18-188\n"
     ]
    }
   ],
   "source": [
    "print('Previous Scikit Processing Job Name: {}'.format(scikit_processing_job_s3_output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_train = '{}/output/bert-train-all'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_validation = '{}/output/bert-validation-all'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_test = '{}/output/bert-test-all'.format(scikit_processing_job_s3_output_prefix)\n",
    "\n",
    "path_train = './{}'.format(prefix_train)\n",
    "path_validation = './{}'.format(prefix_validation)\n",
    "path_test = './{}'.format(prefix_test)\n",
    "\n",
    "train_s3_uri = 's3://{}/{}'.format(bucket, prefix_train)\n",
    "validation_s3_uri = 's3://{}/{}'.format(bucket, prefix_validation)\n",
    "test_s3_uri = 's3://{}/{}'.format(bucket, prefix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-85-93b3c10d512f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-85-93b3c10d512f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri), distribution='ShardedByS3Key')\u001b[0m\n\u001b[0m                                                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri), distribution='ShardedByS3Key') \n",
    "s3_input_validation_data = sagemaker.s3_input(s3_data=validation_s3_uri), distribution='ShardedByS3Key')\n",
    "s3_input_test_data = sagemaker.s3_input(s3_data=test_s3_uri), distribution='ShardedByS3Key')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import time\r\n",
      "import random\r\n",
      "import pandas as pd\r\n",
      "from glob import glob\r\n",
      "import argparse\r\n",
      "import json\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "import os\r\n",
      "import tensorflow as tf\r\n",
      "#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.0.0'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'transformers'])\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sagemaker-tensorflow==2.0.0.1.1.0'])\r\n",
      "from transformers import DistilBertTokenizer\r\n",
      "from transformers import TFDistilBertForSequenceClassification\r\n",
      "from transformers import TextClassificationPipeline\r\n",
      "from transformers.configuration_distilbert import DistilBertConfig\r\n",
      "\r\n",
      "MAX_SEQ_LENGTH = 128\r\n",
      "BATCH_SIZE = 128 \r\n",
      "EVAL_BATCH_SIZE = BATCH_SIZE * 2\r\n",
      "EPOCHS = 10 \r\n",
      "STEPS_PER_EPOCH = 1000\r\n",
      "VALIDATION_STEPS = 1000\r\n",
      "CLASSES = [1, 2, 3, 4, 5]\r\n",
      "# XLA is an optimization compiler for tensorflow\r\n",
      "USE_XLA = False \r\n",
      "# Mixed precision can help to speed up training time\r\n",
      "USE_AMP = False\r\n",
      "\r\n",
      "\r\n",
      "def select_data_and_label_from_record(record):\r\n",
      "    x = {\r\n",
      "        'input_ids': record['input_ids'],\r\n",
      "        'input_mask': record['input_mask'],\r\n",
      "        'segment_ids': record['segment_ids']\r\n",
      "    }\r\n",
      "\r\n",
      "    y = record['label_ids']\r\n",
      "    print(y)\r\n",
      "\r\n",
      "    return (x, y)\r\n",
      "\r\n",
      "\r\n",
      "def file_based_input_dataset_builder(channel,\r\n",
      "                                     input_filenames,\r\n",
      "                                     pipe_mode,\r\n",
      "                                     is_training,\r\n",
      "                                     drop_remainder):\r\n",
      "\r\n",
      "    # For training, we want a lot of parallel reading and shuffling.\r\n",
      "    # For eval, we want no shuffling and parallel reading doesn't matter.\r\n",
      "\r\n",
      "    if pipe_mode:\r\n",
      "        print('***** Using pipe_mode with channel {}'.format(channel))\r\n",
      "        from sagemaker_tensorflow import PipeModeDataset\r\n",
      "        dataset = PipeModeDataset(channel=channel,\r\n",
      "                                  record_format='TFRecord')\r\n",
      "    else:\r\n",
      "        print('***** Using input_filenames {}'.format(input_filenames))\r\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\r\n",
      "\r\n",
      "    dataset = dataset.repeat(EPOCHS * STEPS_PER_EPOCH)\r\n",
      "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
      "\r\n",
      "    name_to_features = {\r\n",
      "      \"input_ids\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\r\n",
      "      \"input_mask\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\r\n",
      "      \"segment_ids\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\r\n",
      "      \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\r\n",
      "      \"is_real_example\": tf.io.FixedLenFeature([], tf.int64),\r\n",
      "    }\r\n",
      "\r\n",
      "    def _decode_record(record, name_to_features):\r\n",
      "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\r\n",
      "        example = tf.io.parse_single_example(record, name_to_features)\r\n",
      "\r\n",
      "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\r\n",
      "        # So cast all int64 to int32.\r\n",
      "        for name in list(example.keys()):\r\n",
      "            t = example[name]\r\n",
      "            if t.dtype == tf.int64:\r\n",
      "                t = tf.cast(t, tf.int32)\r\n",
      "            example[name] = t\r\n",
      "\r\n",
      "        return example\r\n",
      "        \r\n",
      "    dataset = dataset.apply(\r\n",
      "        tf.data.experimental.map_and_batch(\r\n",
      "          lambda record: _decode_record(record, name_to_features),\r\n",
      "          batch_size=BATCH_SIZE,\r\n",
      "          drop_remainder=drop_remainder,\r\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\r\n",
      "\r\n",
      "    dataset.cache()\r\n",
      "\r\n",
      "    if is_training:\r\n",
      "        dataset = dataset.shuffle(seed=42,\r\n",
      "                                  buffer_size=1000,\r\n",
      "                                  reshuffle_each_iteration=True)\r\n",
      "\r\n",
      "    return dataset\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "#    parser.add_argument('--model-type', type=str, default='bert')\r\n",
      "#    parser.add_argument('--model-name', type=str, default='bert-base-cased')\r\n",
      "    parser.add_argument('--train-data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_TRAIN'])\r\n",
      "    parser.add_argument('--validation-data', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CHANNEL_VALIDATION'])\r\n",
      "    parser.add_argument('--model-dir', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_MODEL_DIR'])\r\n",
      "    parser.add_argument('--hosts', \r\n",
      "                        type=list, \r\n",
      "                        default=json.loads(os.environ['SM_HOSTS']))\r\n",
      "    parser.add_argument('--current-host', \r\n",
      "                        type=str, \r\n",
      "                        default=os.environ['SM_CURRENT_HOST'])    \r\n",
      "    parser.add_argument('--num-gpus', \r\n",
      "                        type=int, \r\n",
      "                        default=os.environ['SM_NUM_GPUS'])\r\n",
      "\r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "#    model_type = args.model_type\r\n",
      "#    model_name = args.model_name\r\n",
      "    train_data = args.train_data\r\n",
      "    validation_data = args.validation_data\r\n",
      "    model_dir = args.model_dir\r\n",
      "    hosts = args.hosts\r\n",
      "    current_host = args.current_host\r\n",
      "    num_gpus = args.num_gpus\r\n",
      "\r\n",
      "    tokenizer = None\r\n",
      "    config = None\r\n",
      "    model = None\r\n",
      "\r\n",
      "    # This is required when launching many instances at once...  the urllib request seems to get denied periodically\r\n",
      "    successful_download = False\r\n",
      "    retries = 0\r\n",
      "    while (retries < 5 and not successful_download):\r\n",
      "        try:\r\n",
      "            tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "            config = DistilBertConfig.from_pretrained('distilbert-base-uncased',\r\n",
      "                                                      num_labels=len(CLASSES))\r\n",
      "            model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', \r\n",
      "                                                                          config=config)\r\n",
      "            successful_download = True\r\n",
      "            print('Sucessfully downloaded after {} retries.'.format(retries))\r\n",
      "        except:\r\n",
      "            retries = retries + 1\r\n",
      "            random_sleep = random.randint(1, 30)\r\n",
      "            print('Retry #{}.  Sleeping for {} seconds'.format(retries, random_sleep))\r\n",
      "            time.sleep(random_sleep)\r\n",
      "\r\n",
      "    if not tokenizer or not model or not config:\r\n",
      "        print('Not properly initialized...')\r\n",
      "    \r\n",
      "    pipe_mode_str = os.environ.get('SM_INPUT_DATA_CONFIG', '')\r\n",
      "    print('pipe_mode_str {}'.format(pipe_mode_str))\r\n",
      "    pipe_mode = (pipe_mode_str.find('Pipe') >= 0)\r\n",
      "    print('pipe_mode {}'.format(pipe_mode))\r\n",
      "\r\n",
      "    train_data_filenames = glob('{}/*.tfrecord'.format(train_data))\r\n",
      "    print('train_data_filenames {}'.format(train_data_filenames))\r\n",
      "    train_dataset = file_based_input_dataset_builder(\r\n",
      "        channel='train',\r\n",
      "        input_filenames=train_data_filenames,\r\n",
      "        pipe_mode=pipe_mode,\r\n",
      "        is_training=True,\r\n",
      "        drop_remainder=False).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "    validation_data_filenames = glob('{}/*.tfrecord'.format(validation_data))\r\n",
      "    print('validation_data_filenames {}'.format(validation_data_filenames))\r\n",
      "    validation_dataset = file_based_input_dataset_builder(\r\n",
      "        channel='validation',\r\n",
      "        input_filenames=validation_data_filenames,\r\n",
      "        pipe_mode=pipe_mode,\r\n",
      "        is_training=False,\r\n",
      "        drop_remainder=False).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "    tf.config.optimizer.set_jit(USE_XLA)\r\n",
      "    tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": USE_AMP})\r\n",
      "\r\n",
      "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\r\n",
      "    if USE_AMP:\r\n",
      "        # loss scaling is currently required when using mixed precision\r\n",
      "        optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, 'dynamic')\r\n",
      "\r\n",
      "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\r\n",
      "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\r\n",
      "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
      "#    model.layers[0].trainable=False\r\n",
      "    model.summary()\r\n",
      "\r\n",
      "    log_dir = './tensorboard/classification/'\r\n",
      "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\r\n",
      "\r\n",
      "    history = model.fit(train_dataset,\r\n",
      "                        shuffle=True,\r\n",
      "                        epochs=EPOCHS,\r\n",
      "                        steps_per_epoch=STEPS_PER_EPOCH,\r\n",
      "#                        validation_data=validation_dataset,\r\n",
      "#                        validation_steps=VALIDATION_STEPS,\r\n",
      "                        callbacks=[tensorboard_callback])\r\n",
      "\r\n",
      "    print('Trained model {}'.format(model))\r\n",
      "\r\n",
      "    # Save the Model\r\n",
      "    model.save_pretrained(model_dir)\r\n",
      "\r\n",
      "    loaded_model = TFDistilBertForSequenceClassification.from_pretrained(model_dir,\r\n",
      "                                                                   id2label={\r\n",
      "                                                                    0: 1,\r\n",
      "                                                                    1: 2,\r\n",
      "                                                                    2: 3,\r\n",
      "                                                                    3: 4,\r\n",
      "                                                                    4: 5\r\n",
      "                                                                   },\r\n",
      "                                                                   label2id={\r\n",
      "                                                                    1: 0,\r\n",
      "                                                                    2: 1,\r\n",
      "                                                                    3: 2,\r\n",
      "                                                                    4: 3,\r\n",
      "                                                                    5: 4\r\n",
      "                                                                   })\r\n",
      "\r\n",
      "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\r\n",
      "\r\n",
      "    inference_pipeline = TextClassificationPipeline(model=loaded_model, \r\n",
      "                                                    tokenizer=tokenizer,\r\n",
      "                                                    framework='tf',\r\n",
      "                                                    device=-1) # -1 is CPU, >= 0 is GPU\r\n",
      "\r\n",
      "    print(\"\"\"I loved it!  I will recommend this to everyone.\"\"\", inference_pipeline(\"\"\"I loved it!  I will recommend this to everyone.\"\"\"))\r\n",
      "    print(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\", inference_pipeline(\"\"\"Really bad.  I hope they don't make this anymore.\"\"\"))\r\n",
      "    print(\"\"\"It's OK.\"\"\", inference_pipeline(\"\"\"It's OK.\"\"\"))\r\n"
     ]
    }
   ],
   "source": [
    "!cat src_bert_tf2/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "model_output_path = 's3://{}/models/tf2-bert'.format(bucket)\n",
    "\n",
    "bert_estimator = TensorFlow(entry_point='tf_bert_reviews.py',\n",
    "                            source_dir='src_bert_tf2',\n",
    "                            role=role,\n",
    "                            train_instance_count=2, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "                            train_instance_type='ml.p3.16xlarge',\n",
    "                            py_version='py3',\n",
    "                            framework_version='2.0.0',\n",
    "                            output_path=model_output_path,\n",
    "#                            hyperparameters={'model_type':'bert',\n",
    "#                                             'model_name': 'bert-base-cased'},\n",
    "                            distributions={'parameter_server': {'enabled': True}},\n",
    "                            enable_cloudwatch_metrics=True,\n",
    "                            input_mode='Pipe'\n",
    "                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_estimator.fit(inputs={'train': s3_input_train_data, \n",
    "                           'validation': s3_input_validation_data,}, \n",
    "                   wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_job_name:  tensorflow-training-2020-04-04-07-42-44-091\n"
     ]
    }
   ],
   "source": [
    "training_job_name = bert_estimator.latest_training_job.name\n",
    "print('training_job_name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/jobs/tensorflow-training-2020-04-04-07-42-44-091\">Training Job</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(region, training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TrainingJobs;prefix=tensorflow-training-2020-04-04-07-42-44-091;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/models/tf2-bert/tensorflow-training-2020-04-04-07-42-44-091/?region=us-east-1&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "training_job_s3_output_prefix = 'models/tf2-bert/{}'.format(training_job_name) # 'models/tf-bert/script-mode/training-runs/{}'.format(training_job_name)\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket, training_job_s3_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model artifact from AWS S3\n",
    "\n",
    "!aws s3 cp $model_output_path/$training_job_name/output/model.tar.gz ./models\n",
    "\n",
    "#!aws s3 cp s3://sagemaker-us-east-1-835319576252/models/tf-bert/script-mode/training-runs/tensorflow-training-2020-03-24-04-41-39-405/output/model.tar.gz ./models/tf2-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "tar = tarfile.open('./models/model.tar.gz')\n",
    "tar.extractall(path='./models')\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -al ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Must upgrade wrapt before installing TF\n",
    "!pip install -q pip --upgrade\n",
    "!pip install -q wrapt --upgrade --ignore-installed\n",
    "!pip install -q tensorflow==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
