{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q smdebug==0.7.2\n",
    "!pip install -q sagemaker-experiments==0.1.11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the S3 Location of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r scikit_processing_job_s3_output_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Previous Scikit Processing Job Name: {}'.format(scikit_processing_job_s3_output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_train = '{}/output/bert-train-all'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_validation = '{}/output/bert-validation-all'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_test = '{}/output/bert-test-all'.format(scikit_processing_job_s3_output_prefix)\n",
    "\n",
    "path_train = './{}'.format(prefix_train)\n",
    "path_validation = './{}'.format(prefix_validation)\n",
    "path_test = './{}'.format(prefix_test)\n",
    "\n",
    "train_s3_uri = 's3://{}/{}'.format(bucket, prefix_train)\n",
    "validation_s3_uri = 's3://{}/{}'.format(bucket, prefix_validation)\n",
    "test_s3_uri = 's3://{}/{}'.format(bucket, prefix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri, distribution='ShardedByS3Key') \n",
    "s3_input_validation_data = sagemaker.s3_input(s3_data=validation_s3_uri, distribution='ShardedByS3Key')\n",
    "s3_input_test_data = sagemaker.s3_input(s3_data=test_s3_uri, distribution='ShardedByS3Key')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!cat src_bert_tf2/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "from sagemaker.debugger import Rule, DebuggerHookConfig, TensorBoardOutputConfig, CollectionConfig, rule_configs\n",
    "\n",
    "model_output_path = 's3://{}/models/tf2-bert'.format(bucket)\n",
    "\n",
    "rules=[\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.loss_not_decreasing(),\n",
    "            rule_parameters={\n",
    "                'collection_names': 'losses,metrics',\n",
    "                'use_losses_collection': 'true',\n",
    "                'num_steps': '5',\n",
    "                'diff_percent': '5'\n",
    "            },\n",
    "            collections_to_save=[\n",
    "                CollectionConfig(name='losses',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '5',\n",
    "                                 }),\n",
    "                CollectionConfig(name='metrics',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '5',\n",
    "                                 })\n",
    "            ]\n",
    "        ),\n",
    "        Rule.sagemaker(\n",
    "            rule_configs.overtraining(),\n",
    "            rule_parameters={\n",
    "                'collection_names': 'losses,metrics',\n",
    "                'patience_train': '10',\n",
    "                'patience_validation': '20',\n",
    "                'delta': '0.1'\n",
    "            },\n",
    "            collections_to_save=[\n",
    "                CollectionConfig(name='losses',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '5',\n",
    "                                 }),\n",
    "                CollectionConfig(name='metrics',\n",
    "                                 parameters={\n",
    "                                     'save_interval': '5',\n",
    "                                 })\n",
    "            ]\n",
    "        )\n",
    "    ]\n",
    "\n",
    "# Hyper-parameters\n",
    "epochs=2\n",
    "train_batch_size=128\n",
    "train_instance_count=1\n",
    "use_parameter_server=False\n",
    "input_mode='File' # 'File' or 'Pipe'\n",
    "\n",
    "unique_name = '{}-{}'.format(input_mode, int(time.time()))\n",
    "\n",
    "# Track Experiments\n",
    "from smexperiments.experiment import Experiment\n",
    "\n",
    "experiment=Experiment.create(\n",
    "    experiment_name='train-reviews-bert-{}'.format(unique_name),\n",
    "    description='Train Reviews BERT', \n",
    "    sagemaker_boto_client=sm)\n",
    "\n",
    "from smexperiments.tracker import Tracker\n",
    "\n",
    "tracker_display_name='Train-Reviews-BERT-Tracker-{}'.format(unique_name)\n",
    "with Tracker.create(display_name=tracker_display_name, sagemaker_boto_client=sm) as tracker:\n",
    "    tracker.log_parameters({\n",
    "        'epochs': epochs,\n",
    "    })\n",
    "    # we can log the s3 uri to the dataset we just uploaded\n",
    "    tracker.log_input(name='reviews_dataset_train', media_type='s3/uri', value=train_s3_uri)\n",
    "    tracker.log_input(name='reviews_dataset_validation', media_type='s3/uri', value=validation_s3_uri)\n",
    "    tracker.log_input(name='reviews_dataset_test', media_type='s3/uri', value=test_s3_uri)\n",
    "    \n",
    "from smexperiments.trial import Trial\n",
    "\n",
    "trial_name='train-reviews-bert-training-job-{}'.format(unique_name)\n",
    "trial = Trial.create(trial_name=trial_name, experiment_name=experiment.experiment_name, sagemaker_boto_client=sm)\n",
    "trial.add_trial_component(tracker.trial_component)\n",
    "\n",
    "trial_component_display_name='Train-Reviews-BERT-Trial-{}'.format(unique_name)\n",
    "    \n",
    "experiment_config={'ExperimentName': experiment.experiment_name,\n",
    "                   'TrialName': trial.trial_name,\n",
    "                   'TrialComponentDisplayName': trial_component_display_name}\n",
    "\n",
    "estimator = TensorFlow(entry_point='tf_bert_reviews.py',\n",
    "                            source_dir='src',\n",
    "                            role=role,\n",
    "                            train_instance_count=train_instance_count, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "                            train_instance_type='ml.p3.2xlarge',\n",
    "                            train_volume_size=1800,\n",
    "                            py_version='py3',\n",
    "                            framework_version='2.0.0',\n",
    "                            output_path=model_output_path,\n",
    "                            hyperparameters={'use-xla': False,\n",
    "                                             'use-amp': False,\n",
    "                                             'train-batch-size': train_batch_size,\n",
    "                                             'validation-batch-size': 128,\n",
    "                                             'test-batch-size': 128,\n",
    "                                             'epochs': epochs,\n",
    "                                             'train-steps-per-epoch': 100,\n",
    "                                             'validation-steps': 100,\n",
    "                                             'test-steps': 100,\n",
    "                                             'max-seq-length': 128,\n",
    "                                             'freeze-bert-layer': False,\n",
    "                                             'enable-sagemaker-debugger': True},\n",
    "                            distributions={'parameter_server': {'enabled': use_parameter_server}},\n",
    "                            input_mode=input_mode,\n",
    "                            enable_cloudwatch_metrics=True,\n",
    "                            metric_definitions=[\n",
    "                                 {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "                                 {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "                                 {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "                                 {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "                            ],\n",
    "                            rules=rules\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "# this is where we create a Trial object that allows access to saved tensors\n",
    "trial = create_trial(estimator.latest_job_debugger_artifacts_path())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_estimator.fit(inputs={'train': s3_input_train_data, \n",
    "                           'validation': s3_input_validation_data,\n",
    "                           'test': s3_input_test_data},\n",
    "                   experiment_config=experiment_config,                   \n",
    "                   wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = bert_estimator.latest_training_job.name\n",
    "print('training_job_name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/jobs/{}\">Training Job</a> After About 5 Minutes</b>'.format(region, training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TrainingJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a> After About 5 Minutes</b>'.format(region, training_job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "training_job_s3_output_prefix = 'models/tf2-bert/{}'.format(training_job_name) # 'models/tf-bert/script-mode/training-runs/{}'.format(training_job_name)\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://s3.console.aws.amazon.com/s3/buckets/{}/{}/?region={}&tab=overview\">S3 Output Data</a> After The Training Job Has Completed</b>'.format(bucket, training_job_s3_output_prefix, region)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Might need to convert ' => \"\n",
    "# search_expression = {\n",
    "#     'Filters':[\n",
    "#         {\n",
    "#             'Name': 'DisplayName',\n",
    "#             'Operator': 'Equals',\n",
    "#             'Value': 'Training'\n",
    "#         }\n",
    "#     ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.analytics import ExperimentAnalytics\n",
    "\n",
    "trial_component_analytics = ExperimentAnalytics(\n",
    "    sagemaker_session=sess, \n",
    "    experiment_name=experiment.experiment_name,\n",
    "#    search_expression=search_expression,\n",
    "    sort_by='metrics.validation:accuracy.max',\n",
    "    sort_order='Descending',\n",
    "    metric_names=['validation:accuracy'],\n",
    "    parameter_names=['epochs', 'train_batch_size']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytics_table = trial_component_analytics.dataframe()\n",
    "analytics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineage_table = ExperimentAnalytics(\n",
    "    sagemaker_session=sess,\n",
    "    experiment_name=experiment.experiment_name,\n",
    "#     search_expression={\n",
    "#         'Filters':[{\n",
    "#             'Name': 'Parents.TrialName',\n",
    "#             'Operator': 'Equals',\n",
    "#             'Value': ??\n",
    "#         }]\n",
    "#     },\n",
    "    sort_by=\"CreationTime\",\n",
    "    sort_order=\"Ascending\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and Load the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model artifact from AWS S3\n",
    "\n",
    "!aws s3 cp $model_output_path/$training_job_name/output/model.tar.gz ./models\n",
    "\n",
    "#!aws s3 cp s3://sagemaker-us-east-1-835319576252/models/tf-bert/script-mode/training-runs/tensorflow-training-2020-03-24-04-41-39-405/output/model.tar.gz ./models/tf2-bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "tar = tarfile.open('./models/model.tar.gz')\n",
    "tar.extractall(path='./models')\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ls -al ./models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Must upgrade wrapt before installing TF\n",
    "!pip install -q pip --upgrade\n",
    "!pip install -q wrapt --upgrade --ignore-installed\n",
    "!pip install -q tensorflow==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
