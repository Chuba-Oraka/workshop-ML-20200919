{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the S3 Location of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r scikit_processing_job_s3_output_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Previous Scikit Processing Job Name: {}'.format(scikit_processing_job_s3_output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_train = '{}/output/raw-labeled-split-balanced-header-train'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_validation = '{}/output/raw-labeled-split-balanced-header-validation'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_test = '{}/output/raw-labeled-split-balanced-header-test'.format(scikit_processing_job_s3_output_prefix)\n",
    "\n",
    "path_train = './{}'.format(prefix_train)\n",
    "path_validation = './{}'.format(prefix_validation)\n",
    "path_test = './{}'.format(prefix_test)\n",
    "\n",
    "train_s3_uri = 's3://{}/{}'.format(bucket, prefix_train)\n",
    "validation_s3_uri = 's3://{}/{}'.format(bucket, prefix_validation)\n",
    "test_s3_uri = 's3://{}/{}'.format(bucket, prefix_test)\n",
    "\n",
    "s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri, content_type='text/csv')\n",
    "s3_input_validation_data = sagemaker.s3_input(s3_data=validation_s3_uri, content_type='text/csv')\n",
    "s3_input_test_data = sagemaker.s3_input(s3_data=test_s3_uri, content_type='text/csv')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat src_bert/bert_reviews.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "model_output_path = 's3://{}/models/bert/script-mode/training-runs'.format(bucket)\n",
    "\n",
    "bert_estimator = PyTorch(entry_point='bert_reviews.py',\n",
    "                         source_dir='src_bert',\n",
    "                         role=role,\n",
    "                         train_instance_count=1, # 1 is actually faster due to communication overhead with >1\n",
    "                         train_instance_type='ml.p3.8xlarge',\n",
    "                         py_version='py3',\n",
    "                         framework_version='1.4.0',\n",
    "                         output_path=model_output_path,\n",
    "                         hyperparameters={'model_type':'bert',\n",
    "                                          'model_name': 'bert-base-cased',\n",
    "                                          'backend': 'gloo'},\n",
    "                         enable_cloudwatch_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_estimator.fit(inputs={'train': s3_input_train_data, \n",
    "                           'validation': s3_input_validation_data,}, \n",
    "                   wait=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = bert_estimator.latest_training_job.name\n",
    "print('training_job_name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-21 18:05:40 Starting - Preparing the instances for training......\n",
      "2020-03-21 18:07:00 Downloading - Downloading input data...\n",
      "2020-03-21 18:07:21 Training - Downloading the training image......\n",
      "2020-03-21 18:08:47 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-21 18:08:48,265 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-03-21 18:08:48,309 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-03-21 18:08:51,338 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-03-21 18:08:58,987 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-21 18:08:58,987 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-21 18:08:58,988 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-21 18:08:58,988 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpta1ie6m4/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=4366 sha256=d7499fec9def0b42bea960aefabfbc1faa629fd5609bb6997404337a846161ed\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8k3fhtqy/wheels/d2/14/9f/3ab0024907c7c4c5bad2f62db502efe35bad9dd4d5c40f9231\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34m2020-03-21 18:09:08,693 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_name\": \"bert-base-cased\",\n",
      "        \"model_type\": \"bert\",\n",
      "        \"backend\": \"gloo\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-03-21-18-03-37-762\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-835319576252/pytorch-training-2020-03-21-18-03-37-762/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"bert_reviews\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 32,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"bert_reviews.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"model_name\":\"bert-base-cased\",\"model_type\":\"bert\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=bert_reviews.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=bert_reviews\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=32\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-835319576252/pytorch-training-2020-03-21-18-03-37-762/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"model_name\":\"bert-base-cased\",\"model_type\":\"bert\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-03-21-18-03-37-762\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-835319576252/pytorch-training-2020-03-21-18-03-37-762/source/sourcedir.tar.gz\",\"module_name\":\"bert_reviews\",\"network_interface_name\":\"eth0\",\"num_cpus\":32,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"bert_reviews.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--model_name\",\"bert-base-cased\",\"--model_type\",\"bert\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=bert-base-cased\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_TYPE=bert\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python bert_reviews.py --backend gloo --model_name bert-base-cased --model_type bert\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34mCollecting simpletransformers\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/939 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 939/939 [00:00<00:00, 965kB/s]\n",
      "  Downloading simpletransformers-0.22.1-py3-none-any.whl (144 kB)\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/263M [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.70M/263M [00:00<00:05, 47.0MB/s]#015Downloading:   4%|▎         | 9.58M/263M [00:00<00:05, 47.5MB/s]#015Downloading:   6%|▌         | 14.5M/263M [00:00<00:05, 48.0MB/s]#015Downloading:   7%|▋         | 19.5M/263M [00:00<00:05, 48.5MB/s]#015Downloading:   9%|▉         | 24.5M/263M [00:00<00:04, 49.1MB/s]#015Downloading:  11%|█         | 29.5M/263M [00:00<00:04, 49.3MB/s]#015Downloading:  13%|█▎        | 34.7M/263M [00:00<00:04, 50.0MB/s]#015Downloading:  15%|█▌        | 39.9M/263M [00:00<00:04, 50.6MB/s]#015Downloading:  17%|█▋        | 44.9M/263M [00:00<00:04, 50.4MB/s]#015Downloading:  19%|█▉        | 50.0M/263M [00:01<00:04, 50.6MB/s]#015Downloading:  21%|██        | 55.0M/263M [00:01<00:04, 50.6MB/s]#015Downloading:  23%|██▎       | 60.1M/263M [00:01<00:04, 50.5MB/s]#015Downloading:  25%|██▍       | 65.1M/263M [00:01<00:03, 50.4MB/s]#015Downloading:  27%|██▋       | 70.1M/263M [00:01<00:03, 50.4MB/s]#015Downloading:  29%|██▊       | 75.3M/263M [00:01<00:03, 50.8MB/s]#015Downloading:  31%|███       | 80.5M/263M [00:01<00:03, 51.1MB/s]#015Downloading:  33%|███▎      | 85.8M/263M [00:01<00:03, 51.8MB/s]#015Downloading:  35%|███▍      | 91.1M/263M [00:01<00:03, 52.1MB/s]#015Downloading:  37%|███▋      | 96.5M/263M [00:01<00:03, 52.6MB/s]#015Downloading:  39%|███▊      | 102M/263M [00:02<00:03, 52.9MB/s] #015Downloading:  41%|████      | 107M/263M [00:02<00:02, 53.1MB/s]#015Downloading:  43%|████▎     | 113M/263M [00:02<00:02, 53.6MB/s]#015Downloading:  45%|████▍     | 118M/263M [00:02<00:02, 53.7MB/s]#015Downloading:  47%|████▋     | 123M/263M [00:02<00:02, 53.8MB/s]#015Downloading:  49%|████▉     | 129M/263M [00:02<00:02, 53.9MB/s]#015Downloading:  51%|█████     | 134M/263M [00:02<00:02, 53.8MB/s]#015Downloading:  53%|█████▎    | 140M/263M [00:02<00:02, 53.8MB/s]#015Downloading:  55%|█████▌    | 145M/263M [00:02<00:02, 53.8MB/s]#015Downloading:  57%|█████▋    | 150M/263M [00:02<00:02, 53.7MB/s]#015Downloading:  59%|█████▉    | 156M/263M [00:03<00:01, 53.8MB/s]#015Downloading:  61%|██████    | 161M/263M [00:03<00:01, 53.8MB/s]#015Downloading:  63%|██████▎   | 167M/263M [00:03<00:01, 54.0MB/s]#015Downloading:  65%|██████▌   | 172M/263M [00:03<00:01, 54.1MB/s]#015Downloading:  67%|██████▋   | 178M/263M [00:03<00:01, 53.3MB/s]#015Downloading:  70%|██████▉   | 183M/263M [00:03<00:01, 54.4MB/s]#015Downloading:  72%|███████▏  | 189M/263M [00:03<00:01, 55.1MB/s]#015Downloading:  74%|███████▍  | 195M/263M [00:03<00:01, 55.8MB/s]#015Downloading:  76%|███████▌  | 200M/263M [00:03<00:01, 56.3MB/s]#015Downloading:  78%|███████▊  | 206M/263M [00:03<00:01, 56.6MB/s]#015Downloading:  80%|████████  | 212M/263M [00:04<00:00, 56.9MB/s]#015Downloading:  83%|████████▎ | 218M/263M [00:04<00:00, 57.2MB/s]#015Downloading:  85%|████████▍ | 223M/263M [00:04<00:00, 57.4MB/s]#015Downloading:  87%|████████▋ | 229M/263M [00:04<00:00, 57.5MB/s]#015Downloading:  89%|████████▉ | 235M/263M [00:04<00:00, 57.5MB/s]#015Downloading:  91%|█████████▏| 241M/263M [00:04<00:00, 57.6MB/s]#015Downloading:  94%|█████████▎| 247M/263M [00:04<00:00, 57.3MB/s]#015Downloading:  96%|█████████▌| 252M/263M [00:04<00:00, 57.4MB/s]#015Downloading:  98%|█████████▊| 258M/263M [00:04<00:00, 57.5MB/s]#015Downloading: 100%|██████████| 263M/263M [00:04<00:00, 53.7MB/s]\u001b[0m\n",
      "\u001b[34mCollecting regex\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 213k/213k [00:00<00:00, 33.7MB/s]\n",
      "  Downloading regex-2020.2.20-cp36-cp36m-manylinux2010_x86_64.whl (690 kB)\u001b[0m\n",
      "\u001b[34m#015Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (0.21.2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   0%|          | 0/225 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (4.42.1)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   0%|          | 1/225 [00:01<05:19,  1.43s/it]#033[AERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34mCollecting seqeval\n",
      "\n",
      "  Downloading seqeval-0.0.12.tar.gz (21 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   1%|▏         | 3/225 [00:01<03:45,  1.02s/it]#033[A\u001b[0m\n",
      "\u001b[34mCollecting tensorboardx\u001b[0m\n",
      "\u001b[34m#015Current iteration:   2%|▏         | 5/225 [00:01<02:40,  1.37it/s]#033[A\n",
      "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   3%|▎         | 7/225 [00:01<01:54,  1.90it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (2.22.0)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   4%|▍         | 9/225 [00:01<01:23,  2.60it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (1.2.2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   5%|▍         | 11/225 [00:01<01:01,  3.49it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (0.25.0)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   6%|▌         | 13/225 [00:02<00:45,  4.61it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (1.16.4)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   7%|▋         | 15/225 [00:02<00:35,  5.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting transformers\u001b[0m\n",
      "\u001b[34m#015Current iteration:   8%|▊         | 17/225 [00:02<00:27,  7.44it/s]#033[A\n",
      "  Downloading transformers-2.5.1-py3-none-any.whl (499 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   8%|▊         | 19/225 [00:02<00:22,  9.04it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->simpletransformers) (0.14.1)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   9%|▉         | 21/225 [00:02<00:19, 10.64it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting Keras>=2.2.4\u001b[0m\n",
      "\u001b[34m#015Current iteration:  10%|█         | 23/225 [00:02<00:16, 12.14it/s]#033[A\n",
      "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  11%|█         | 25/225 [00:02<00:14, 13.47it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardx->simpletransformers) (3.11.3)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  12%|█▏        | 27/225 [00:02<00:13, 14.59it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from tensorboardx->simpletransformers) (1.14.0)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  13%|█▎        | 29/225 [00:02<00:12, 15.51it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (1.25.8)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  14%|█▍        | 31/225 [00:03<00:11, 16.22it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (2019.11.28)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  15%|█▍        | 33/225 [00:03<00:11, 16.74it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (2.8)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  16%|█▌        | 35/225 [00:03<00:11, 17.13it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (3.0.4)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  16%|█▋        | 37/225 [00:03<00:10, 17.43it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->simpletransformers) (2019.3)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  17%|█▋        | 39/225 [00:03<00:10, 17.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->simpletransformers) (2.8.1)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  18%|█▊        | 41/225 [00:03<00:10, 17.79it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\u001b[0m\n",
      "\u001b[34m#015Current iteration:  19%|█▉        | 43/225 [00:03<00:10, 17.87it/s]#033[A\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  20%|██        | 45/225 [00:03<00:10, 17.94it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers->simpletransformers) (1.12.23)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  21%|██        | 47/225 [00:03<00:09, 18.00it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.5.2\u001b[0m\n",
      "\u001b[34m#015Current iteration:  22%|██▏       | 49/225 [00:04<00:09, 18.05it/s]#033[A/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\n",
      "\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  23%|██▎       | 51/225 [00:04<00:09, 17.93it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34m#015Current iteration:  24%|██▎       | 53/225 [00:04<00:09, 17.98it/s]#033[A\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  24%|██▍       | 55/225 [00:04<00:09, 18.00it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting keras-preprocessing>=1.0.5\u001b[0m\n",
      "\u001b[34m#015Current iteration:  25%|██▌       | 57/225 [00:04<00:09, 17.93it/s]#033[A\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  26%|██▌       | 59/225 [00:04<00:09, 17.99it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.9.0)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  27%|██▋       | 61/225 [00:04<00:09, 18.03it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (5.3)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-03-21 18:09:49 Uploading - Uploading generated training model\u001b[34m#015Current iteration:  28%|██▊       | 63/225 [00:04<00:08, 18.06it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting keras-applications>=1.0.6\u001b[0m\n",
      "\u001b[34m#015Current iteration:  29%|██▉       | 65/225 [00:04<00:08, 18.09it/s]#033[A\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  30%|██▉       | 67/225 [00:05<00:08, 18.11it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (46.0.0.post20200309)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  31%|███       | 69/225 [00:05<00:08, 18.12it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->simpletransformers) (7.1.1)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  32%|███▏      | 71/225 [00:05<00:08, 18.12it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.16.0,>=1.15.23 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (1.15.23)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  32%|███▏      | 73/225 [00:05<00:08, 18.12it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.9.5)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  33%|███▎      | 75/225 [00:05<00:08, 18.13it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.3.3)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  34%|███▍      | 77/225 [00:05<00:08, 18.12it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers->simpletransformers) (0.15.2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  35%|███▌      | 79/225 [00:05<00:08, 18.12it/s]#033[A\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: seqeval, sacremoses\u001b[0m\n",
      "\u001b[34m#015Current iteration:  36%|███▌      | 81/225 [00:05<00:07, 18.11it/s]#033[A\n",
      "  Building wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34m#015Current iteration:  37%|███▋      | 83/225 [00:05<00:07, 18.12it/s]#033[A\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34m#015Current iteration:  38%|███▊      | 85/225 [00:06<00:07, 18.13it/s]#033[A\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7423 sha256=f7340a8bbfcd8b09f12e24b369e721900d5471108784bc05e86ad21d2e96da02\u001b[0m\n",
      "\u001b[34m#015Current iteration:  39%|███▊      | 87/225 [00:06<00:07, 18.15it/s]#033[A\n",
      "  Stored in directory: /root/.cache/pip/wheels/1f/1b/a6/a808a7e4d1f7584e42f5e279664cd48bf24ed8392218ce6be4\u001b[0m\n",
      "\u001b[34m#015Current iteration:  40%|███▉      | 89/225 [00:06<00:07, 18.14it/s]#033[A\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m#015Current iteration:  40%|████      | 91/225 [00:06<00:07, 18.14it/s]#033[A\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34m#015Current iteration:  41%|████▏     | 93/225 [00:06<00:07, 18.13it/s]#033[A\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=30deccc3daff6f917b5bdc7008c85b913b234c40eeff87aa380cbe876da50757\u001b[0m\n",
      "\u001b[34m#015Current iteration:  42%|████▏     | 95/225 [00:06<00:07, 18.13it/s]#033[A\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\u001b[0m\n",
      "\u001b[34m#015Current iteration:  43%|████▎     | 97/225 [00:06<00:07, 18.08it/s]#033[A\u001b[0m\n",
      "\u001b[34mSuccessfully built seqeval sacremoses\u001b[0m\n",
      "\u001b[34m#015Current iteration:  44%|████▍     | 99/225 [00:06<00:06, 18.08it/s]#033[A\u001b[0m\n",
      "\u001b[34mInstalling collected packages: regex, keras-preprocessing, keras-applications, Keras, seqeval, tensorboardx, sacremoses, tokenizers, sentencepiece, filelock, transformers, simpletransformers\u001b[0m\n",
      "\u001b[34m#015Current iteration:  45%|████▍     | 101/225 [00:06<00:06, 18.06it/s]#033[A\u001b[0m\n",
      "\u001b[34mSuccessfully installed Keras-2.3.1 filelock-3.0.12 keras-applications-1.0.8 keras-preprocessing-1.1.0 regex-2020.2.20 sacremoses-0.0.38 sentencepiece-0.1.85 seqeval-0.0.12 simpletransformers-0.22.1 tensorboardx-2.0 tokenizers-0.5.2 transformers-2.5.1\u001b[0m\n",
      "\u001b[34m#015Current iteration:  46%|████▌     | 103/225 [00:07<00:06, 18.09it/s]#033[A\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34m#015Current iteration:  47%|████▋     | 105/225 [00:07<00:06, 18.10it/s]#033[A\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 4\u001b[0m\n",
      "\u001b[34m#015Current iteration:  48%|████▊     | 107/225 [00:07<00:06, 18.20it/s]#033[A\u001b[0m\n",
      "\u001b[34m(1800, 2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  48%|████▊     | 109/225 [00:07<00:06, 18.17it/s]#033[A\u001b[0m\n",
      "\u001b[34m(100, 2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  49%|████▉     | 111/225 [00:07<00:06, 18.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m(100, 2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  50%|█████     | 113/225 [00:07<00:06, 18.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2020-03-21 18:09:30.585 algo-1:90 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  51%|█████     | 115/225 [00:07<00:06, 18.00it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2020-03-21 18:09:30.585 algo-1:90 INFO hook.py:170] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  52%|█████▏    | 117/225 [00:07<00:05, 18.01it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2020-03-21 18:09:30.585 algo-1:90 INFO hook.py:215] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m#015Current iteration:  53%|█████▎    | 119/225 [00:07<00:05, 18.03it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2020-03-21 18:09:30.599 algo-1:90 INFO hook.py:351] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m#015Current iteration:  54%|█████▍    | 121/225 [00:08<00:05, 18.06it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Running loss: 0.649339#015Running loss: 0.698853#015Running loss: 0.721269#015Running loss: 0.624438#015Running loss: 0.644265#015Running loss: 0.603610#015Running loss: 0.583821#015Running loss: 0.673179#015Running loss: 0.716085#015Running loss: 0.662134#015Running loss: 0.745710#015Running loss: 0.607674#015Running loss: 0.642692#015Running loss: 0.685851#015Running loss: 0.679096#015Running loss: 0.647204#015Running loss: 0.660334#015Running loss: 0.712236#015Running loss: 0.719369#015Running loss: 0.605650#015Running loss: 0.632653#015Running loss: 0.760027#015Running loss: 0.717341#015Running loss: 0.739876#015Running loss: 0.633543#015Running loss: 0.613753#015Running loss: 0.648987#015Running loss: 0.635690#015Running loss: 0.630402#015Running loss: 0.623203#015Running loss: 0.700383#015Running loss: 0.660631#015Running loss: 0.498024#015Running loss: 0.524044#015Running loss: 0.630072#015Running loss: 0.547514#015Running loss: 0.529294#015Running loss: 0.472697#015Running loss: 0.647897#015Running loss: 0.952855#015Running loss: 0.458784#015Running loss: 0.573460#015Running loss: 0.546872#015Running loss: 0.430021#015Running loss: 0.451906#015Running loss: 0.458573#015Running loss: 0.490728#015Running loss: 0.708694#015Running loss: 0.509642#015Running loss: 0.483532#015Running loss: 0.342251#015Running loss: 0.486747#015Running loss: 0.502486#015Running loss: 0.439834#015Running loss: 0.546574#015Running loss: 0.537216#015Running loss: 0.431876#015Running loss: 0.594272#015Running loss: 0.166597#015Running loss: 0.610893#015Running loss: 0.169741#015Running loss: 0.348364#015Running loss: 0.286871#015Running loss: 0.399744#015Running loss: 0.765940#015Running loss: 0.454830#015Running loss: 0.330959#015Running loss: 0.465255#015Running loss: 0.541196#015Running loss: 0.476094#015Running loss: 0.190530#015Running loss: 0.182779#015Running loss: 0.425856#015Running loss: 0.561107#015Running loss: 0.605277#015Running loss: 0.889940#015Running loss: 0.159488#015Running loss: 0.156071#015Running loss: 0.417044#015Running loss: 0.157979#015Running loss: 0.234664#015Running loss: 0.345096#015Running loss: 0.285091#015Running loss: 0.398251#015Running loss: 0.287940#015Running loss: 0.603571#015Running loss: 0.155061#015Running loss: 0.320615#015Running loss: 0.573725#015Running loss: 0.279052#015Running loss: 0.176341#015Running loss: 0.241204#015Running loss: 0.389733#015Running loss: 0.097461#015Running loss: 0.190002#015Running loss: 0.230030#015Running loss: 0.239597#015Running loss: 0.102998#015Running loss: 0.260954#015Running loss: 0.165525#015Running loss: 0.602218#015Running loss: 0.230572#015Running loss: 0.223980#015Running loss: 0.644024#015Running loss: 0.342391#015Running loss: 0.042298#015Running loss: 0.446354#015Running loss: 0.216421#015Running loss: 0.457461#015Running loss: 0.698980#015Running loss: 0.661037#015Running loss: 0.327714#015Running loss: 0.601107#015Running loss: 0.534625#015Running loss: 0.875587#015Running loss: 0.744713#015Running loss: 0.118118#015Running loss: 0.811931#015Running loss: 0.358824#015Running loss: 0.518647#015Running loss: 0.382091#015Running loss: 0.399808#015Running loss: 0.506791#015Running loss: 0.261268#015Running loss: 0.646129#015Running loss: 0.376101#015Running loss: 0.406029#015Running loss: 0.328343#015Running loss: 0.325487#015Running loss: 0.572834#015Running loss: 0.364093#015Running loss: 0.119996#015Running loss: 0.081827#015Running loss: 0.428669#015Running loss: 0.965276#015Running loss: 0.072566#015Running loss: 0.237588#015Running loss: 0.142933#015Running loss: 0.319812#015Running loss: 0.694885#015Running loss: 0.067725#015Running loss: 0.838675#015Running loss: 0.048847#015Running loss: 0.207728#015Running loss: 0.189496#015Running loss: 0.516886#015Running loss: 0.469269#015Running loss: 0.307133#015Running loss: 0.341221#015Running loss: 0.099437#015Running loss: 0.336073#015Running loss: 0.680456#015Running loss: 0.058185#015Running loss: 0.097427#015Running loss: 0.185147#015Running loss: 0.035949#015Running loss: 0.296690#015Running loss: 0.600285#015Running loss: 0.758355#015Running loss: 0.404033#015Running loss: 0.319507#015Running loss: 0.785302#015Running loss: 0.033584#015Running loss: 0.078631#015Running loss: 0.494906#015Running loss: 0.619599#015Running loss: 0.519130#015Running loss: 0.134714#015Running loss: 0.299971#015Running loss: 0.068035#015Running loss: 0.070803#015Running loss: 0.407450#015Running loss: 0.459641#015Running loss: 0.711564#015Running loss: 0.049583#015Running loss: 0.787569#015Running loss: 0.413244#015Running loss: 0.095119#015Running loss: 0.471535#015Running loss: 0.145773#015Running loss: 0.043289#015Running loss: 0.437361#015Running loss: 0.214302#015Running loss: 0.954840#015Running loss: 0.053089#015Running loss: 0.113513#015Running loss: 0.046644#015Running loss: 0.324652#015Running loss: 0.490220#015Running loss: 0.503500#015Running loss: 0.037249#015Running loss: 0.202348#015Running loss: 0.420266#015Running loss: 0.554563#015Running loss: 0.299479#015Running loss: 0.033740#015Running loss: 0.426396#015Running loss: 0.362393#015Running loss: 0.326168#015Running loss: 0.778451#015Running loss: 0.297912#015Running loss: 0.096957#015Running loss: 0.377130#015Running loss: 0.052913#015Running loss: 0.672008#015Running loss: 0.856096#015Running loss: 0.108316#015Running loss: 0.731701#015Running loss: 0.387147#015Running loss: 1.013080#015Running loss: 0.605102#015Running loss: 0.257340#015Running loss: 0.238009#015Running loss: 0.395501#015Running loss: 0.243063#015Running loss: 0.319517#015Running loss: 0.636289#015Running loss: 0.446088#015Running loss: 0.418319#015Running loss: 0.175390#015Running loss: 0.176813#015Running loss: 0.625355#015Running loss: 0.430793#015Running loss: 0.546082#015Running loss: 0.060019{'mcc': 0.7879120845063968, 'tp': 60, 'tn': 30, 'fp': 2, 'fn': 8, 'acc': 0.9, 'eval_loss': 0.2390195308969571}\u001b[0m\n",
      "\u001b[34m#015Current iteration:  55%|█████▍    | 123/225 [00:08<00:05, 18.10it/s]#033[A\u001b[0m\n",
      "\u001b[34mNumber of wrong predictions: 10\u001b[0m\n",
      "\u001b[34m#015Current iteration:  56%|█████▌    | 125/225 [00:08<00:05, 18.12it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  56%|█████▋    | 127/225 [00:08<00:05, 18.13it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  57%|█████▋    | 129/225 [00:08<00:05, 18.14it/s]#033[A\u001b[0m\n",
      "\u001b[34mIt's MS, what can you say? Some of the features included were ones long overdue, others new and very smart, but with MS they have to find places to take two steps back. Schools make you buy this for their convenience when older versions would work just fine. Just another college, money raking kick back scheme to help fleece students because they don't cover this with all of the tuition and innumerable fees.<br /><br />In most regards, I like my older version of office I had already bought and paid for. This version may be better for people with different wants or needs in their use, but so far I have not seen enough improvements to have otherwise updated\u001b[0m\n",
      "\u001b[34m#015Current iteration:  58%|█████▊    | 131/225 [00:08<00:05, 18.11it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  59%|█████▉    | 133/225 [00:08<00:05, 18.13it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  60%|██████    | 135/225 [00:08<00:04, 18.10it/s]#033[A\u001b[0m\n",
      "\u001b[34mprogram is easy to use and guides you step by step. I bought this after I started using the online version and encountered a hiccup - online system does not have a way to pay (for filing) for users with an address outside the US. While it accepts the credit card number the address format is for US only making it impossible if your addres is in a different format. Luckily this product includes online filing.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  61%|██████    | 137/225 [00:08<00:04, 18.10it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  62%|██████▏   | 139/225 [00:09<00:04, 18.11it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  63%|██████▎   | 141/225 [00:09<00:04, 18.12it/s]#033[A\u001b[0m\n",
      "\u001b[34mI have been using this iBank 5 for only about a month now, but I'm giving it 5 stars for one reason more than any other ...CUSTOMER SERVICE!  I had been using Quicken since 1991.  Any long time Quicken users know that using that program is like being in an abusive relationship.  You are forced to upgrade every few years.  There hasn't been any real functional improvement in  a decade or so.  Instead, each upgrade would bring problems, as the program would bloat and bloat.  The records you have carefully maintained would suddenly develop years worth of errors.  A few years ago, I switched to Mac, and since Intuit has some sort of vendetta against Mac, and never developed an equivalent Mac version, I had to buy Parallels ($), buy Window's 7 ($) and then put Quicken on the virtual machine just to run the d**n program.  This means every time Mac upgrades their OS, you have to worry about Parallels not working or Windows not working ..... what a PIA. Finally when Quicken, through a known glitch, prevented me from connecting a few accounts online, and I was going to have to delete the accounts, and start over, I'd had enough.  Of course there was never an easy way to get any support through this. But I had so many years invested in that program I was extremely reluctant to leave.  Nevertheless,  It was time to take the money out from under the mattress and escape to the safe house of iBank.<br /><br />Since I've only been using it for a month, and I have so much data and so many accounts to review I won't say this is a final verdict.  But in general the import from Quicken went extremely extremely well.  Even though I have many different accounts:  bank, savings, loan, investments, 401K, option, brokerage accounts etc.  they all seem to have come over well.<br /><br />I've had some issues with their &#34;Direct Access&#34; duplication entries.  I also don't like that Direct Access doesn't allow bill payments to originate in iBank.  But &#34;Direct Download&#34;  seems towork just fine.  I'd recommend using only that for getting your accounts online.<br /><br />But the BEST part, is that as I've had problems, I was able to quickly get someone in a chat window who in each case, actually knew what they were talking about, and quickly helped me work out solutions.  HOW GREAT IS THAT!  I know it will take a long time to get everything just so, but knowing I really have help is wonderful, and rare enough these days.<br /><br />I agree that reporting is not nearly as good as Quicken, and I like my reports.  But I think the reports are reasonably good.  If that's the worst of it, I'll be happy.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  64%|██████▎   | 143/225 [00:09<00:04, 18.22it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  64%|██████▍   | 145/225 [00:09<00:04, 18.20it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  65%|██████▌   | 147/225 [00:09<00:04, 18.17it/s]#033[A\u001b[0m\n",
      "\u001b[34mFor years, I was having a continual problem with my PC where it would just reboot at random.  This would happen anytime from once a month to four times a day.  VERY frustrating!  Over time, I had replaced the power supply, the motherboard, the RAM, the drives, the cables, and more.  Short of totally wiping and rebuilding my system, I tried everything.  Then, out of desparation, I downloaded PC Matic.  No more random rebooting!  My IT friends (who had discouraged me from buying something I had seen on TV -- but they didn't have another solution) were shocked and amazed.  Thank you PC Matic for creating a terrific product that has helped extend the life of my old PC and saved me countless hours of frustration and time recovering lost work!\u001b[0m\n",
      "\u001b[34m#015Current iteration:  66%|██████▌   | 149/225 [00:09<00:04, 18.16it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  67%|██████▋   | 151/225 [00:09<00:04, 18.12it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  68%|██████▊   | 153/225 [00:09<00:03, 18.11it/s]#033[A\u001b[0m\n",
      "\u001b[34mI don't do a lot of video recording these days, but when I do, I want it to be quick to setup and simple to use, but still be able to provide good quality. My PC desperately needs some upgrades; so right now I can't record AA games with Fraps because I'm already struggling with performance and Shadowplay is even worse. Tested this out with Witcher 3 and no problems, woo!<br /><br />Recording video goes up to 2560x1980 resolution (which I haven't seen before), records at 60FPS (or whatever lower number you choose) and in .avi or .wmv. You can also turn off capturing the cursor, which is super nice. Microphone can be recorded in a second audio track, making it easier to clean up and apply later - not that anyone particularly wants to hear my charming Welsh accent.<br /><br />It also lets you save screenshots in any of the 4 formats (bmp, jpg, png, gif) which I like and I streamed for several hours to a friend direct to Twitch, which was way easier for me, as a very small-scale streamer than setting up xSplit, although I think overall there were less options/customisability - you can still use the overlay to add webcam direct through D3DGear.<br /><br />I've been using Fraps for around 5 years now, and whilst it is easy to use, the one major drawback (for me) is the absolutely massive file size. D3DGear records and compresses, thus making files a fraction of the size which makes storage much easier for me. If I had to describe it, I'd say it's an improved fraps meets a simplfied XSplit.<br /><br />There's a free 15 day trial on their website, so you don't have to take my word for it - if you're interested in recording or streaming you should definitely check it out.<br /><br />Note: I didn't purchase this through Amazon, but own and have tested it extensively through Steam.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  69%|██████▉   | 155/225 [00:09<00:03, 18.12it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  70%|██████▉   | 157/225 [00:10<00:03, 18.19it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  71%|███████   | 159/225 [00:10<00:03, 18.18it/s]#033[A\u001b[0m\n",
      "\u001b[34mYou cannot create statements for tenants with this software. Quickbooks is definitely better and easier to use unless you have just one or two tenants and have the time to manually enter every transaction each month.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  72%|███████▏  | 161/225 [00:10<00:03, 18.17it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  72%|███████▏  | 163/225 [00:10<00:03, 18.23it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  73%|███████▎  | 165/225 [00:10<00:03, 18.22it/s]#033[A\u001b[0m\n",
      "\u001b[34mHey, it's free.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  74%|███████▍  | 167/225 [00:10<00:03, 18.20it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  75%|███████▌  | 169/225 [00:10<00:03, 18.19it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  76%|███████▌  | 171/225 [00:10<00:02, 18.18it/s]#033[A\u001b[0m\n",
      "\u001b[34mWorks ok\u001b[0m\n",
      "\u001b[34m#015Current iteration:  77%|███████▋  | 173/225 [00:10<00:02, 18.17it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  78%|███████▊  | 175/225 [00:11<00:02, 18.15it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  79%|███████▊  | 177/225 [00:11<00:02, 18.15it/s]#033[A\u001b[0m\n",
      "\u001b[34mMBAM is one of the very, very, very few pieces of software I have ever bought. I had to make a exception for it. In my humble opinion, it is one of the very best security softwares out there. I really trust it a lot. It is used where I work to look for malwares. I must admit that when I saw this product on Amazon with a lifetime license, I got a little suspicious, leery. &#34;What is the catch?&#34;, I asked myself. Its developer is not issuing lifetime licenses any longer, it stopped in the first quarter of last year. If you buy it on its website, you only get a one year subscription for 25 bucks. But I took the plunge and spent the 39 bucks on it. The &#34;My Account&#34; tab on the right upper side when clicked shows duration &#34;Lifetime&#34;. I also ran mbam-check(available on Malwarebytes homepage) and it also shows no date for &#34;Expiration Date.&#34; So, I think this is the real Mccoy. Besides, I do not really think Amazon would risk its reputation by badly misrepresenting something. But in order to be 100% sure, I will have to wait until next year to see if any message pops up asking for more money to renew my license. That would be, for me, a huge disappointment, with Amazon...\u001b[0m\n",
      "\u001b[34m#015Current iteration:  80%|███████▉  | 179/225 [00:11<00:02, 18.16it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  80%|████████  | 181/225 [00:11<00:02, 18.16it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  81%|████████▏ | 183/225 [00:11<00:02, 18.15it/s]#033[A\u001b[0m\n",
      "\u001b[34mHave been using this for years and sad to see it may be no worth paying for next year.  Oh well, the market will provide something better hopefully.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  82%|████████▏ | 185/225 [00:11<00:02, 18.15it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  83%|████████▎ | 187/225 [00:11<00:02, 18.16it/s]#033[A\n",
      "\u001b[0m\n",
      "\u001b[34m#015Current iteration:  84%|████████▍ | 189/225 [00:11<00:01, 18.17it/s]#033[A\u001b[0m\n",
      "\u001b[34mPredictions: [1]\u001b[0m\n",
      "\u001b[34m#015Current iteration:  85%|████████▍ | 191/225 [00:11<00:01, 18.24it/s]#033[A\u001b[0m\n",
      "\u001b[34mRaw outputs: [[-1.8581387  1.9727952]]\u001b[0m\n",
      "\u001b[34m#015Current iteration:  86%|████████▌ | 193/225 [00:12<00:01, 18.22it/s]#033[A\u001b[0m\n",
      "\u001b[34mPredictions: [0]\u001b[0m\n",
      "\u001b[34m#015Current iteration:  87%|████████▋ | 195/225 [00:12<00:01, 18.20it/s]#033[A\u001b[0m\n",
      "\u001b[34mRaw outputs: [[ 1.4245691 -1.6129103]]\u001b[0m\n",
      "\u001b[34m#015Current iteration:  88%|████████▊ | 197/225 [00:12<00:01, 18.27it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2020-03-21 18:09:46.824 algo-1:90 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  88%|████████▊ | 199/225 [00:12<00:01, 18.24it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  89%|████████▉ | 201/225 [00:12<00:01, 18.04it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  90%|█████████ | 203/225 [00:12<00:01, 18.07it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  91%|█████████ | 205/225 [00:12<00:01, 18.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  92%|█████████▏| 207/225 [00:12<00:00, 18.09it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  93%|█████████▎| 209/225 [00:12<00:00, 18.08it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  94%|█████████▍| 211/225 [00:13<00:00, 18.10it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  95%|█████████▍| 213/225 [00:13<00:00, 18.11it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  96%|█████████▌| 215/225 [00:13<00:00, 18.13it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  96%|█████████▋| 217/225 [00:13<00:00, 18.14it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  97%|█████████▋| 219/225 [00:13<00:00, 18.15it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  98%|█████████▊| 221/225 [00:13<00:00, 18.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration:  99%|█████████▉| 223/225 [00:13<00:00, 18.16it/s]#033[A\u001b[0m\n",
      "\u001b[34m#015Current iteration: 100%|██████████| 225/225 [00:13<00:00, 18.13it/s]#033[A#015Current iteration: 100%|██████████| 225/225 [00:13<00:00, 16.31it/s]\u001b[0m\n",
      "\u001b[34mINFO:transformers.configuration_utils:Configuration saved in /opt/ml/model/checkpoint-225-epoch-1/config.json\u001b[0m\n",
      "\u001b[34mINFO:transformers.modeling_utils:Model weights saved in /opt/ml/model/checkpoint-225-epoch-1/pytorch_model.bin\u001b[0m\n",
      "\u001b[34m#015Epoch: 100%|██████████| 1/1 [00:14<00:00, 14.08s/it]#015Epoch: 100%|██████████| 1/1 [00:14<00:00, 14.08s/it]\u001b[0m\n",
      "\u001b[34mINFO:transformers.configuration_utils:Configuration saved in /opt/ml/model/config.json\u001b[0m\n",
      "\u001b[34mINFO:transformers.modeling_utils:Model weights saved in /opt/ml/model/pytorch_model.bin\u001b[0m\n",
      "\u001b[34mINFO:simpletransformers.classification.classification_model: Training of distilbert model complete. Saved to /opt/ml/model.\u001b[0m\n",
      "\u001b[34mINFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/100 [00:00<?, ?it/s]#015  1%|          | 1/100 [00:00<00:15,  6.23it/s]#015100%|██████████| 100/100 [00:00<00:00, 621.60it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/13 [00:00<?, ?it/s]#015 69%|██████▉   | 9/13 [00:00<00:00, 89.62it/s]#015100%|██████████| 13/13 [00:00<00:00, 90.43it/s]\u001b[0m\n",
      "\u001b[34mINFO:simpletransformers.classification.classification_model:{'mcc': 0.7879120845063968, 'tp': 60, 'tn': 30, 'fp': 2, 'fn': 8, 'acc': 0.9, 'eval_loss': 0.2390195308969571}\u001b[0m\n",
      "\u001b[34mINFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 48.26it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 67.64it/s]\u001b[0m\n",
      "\u001b[34mINFO:simpletransformers.classification.classification_model: Converting to features started. Cache is not used.\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 48.71it/s]\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0/1 [00:00<?, ?it/s]#015100%|██████████| 1/1 [00:00<00:00, 70.60it/s]\u001b[0m\n",
      "\u001b[34m2020-03-21 18:09:47,498 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-03-21 18:11:02 Completed - Training job completed\n",
      "Training seconds: 242\n",
      "Billable seconds: 242\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "bert_estimator = PyTorch.attach(training_job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model artifact from AWS S3\n",
    "!aws s3 cp $model_output_path/$training_job_name/output/model.tar.gz ./models/bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "tar = tarfile.open('./models/bert/model.tar.gz')\n",
    "tar.extractall(path='./models/bert-model')\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "args = {\n",
    "   'fp16': False,\n",
    "   'max_seq_length': 128\n",
    "}\n",
    "\n",
    "bert_model = ClassificationModel(model_type='distilbert', # bert, distilbert, etc, etc.\n",
    "                                 model_name='./models/bert-model',\n",
    "                                 args=args,\n",
    "                                 use_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, raw_outputs = bert_model.predict([\"\"\"I really enjoyed this item.  I highly recommend it.\"\"\"])\n",
    "\n",
    "print('Predictions: {}'.format(predictions))\n",
    "print('Raw outputs: {}'.format(raw_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, raw_outputs = bert_model.predict([\"\"\"This item is awful and terrible.\"\"\"])\n",
    "\n",
    "print('Predictions: {}'.format(predictions))\n",
    "print('Raw outputs: {}'.format(raw_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
