{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the S3 Location of the Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r scikit_processing_job_s3_output_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Previous Scikit Processing Job Name: {}'.format(scikit_processing_job_s3_output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-21-00-51-49-387/output/raw-labeled-split-balanced-header-train', 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'text/csv'}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-21-00-51-49-387/output/raw-labeled-split-balanced-header-validation', 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'text/csv'}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-03-21-00-51-49-387/output/raw-labeled-split-balanced-header-test', 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'text/csv'}\n"
     ]
    }
   ],
   "source": [
    "prefix_train = '{}/output/raw-labeled-split-balanced-header-train'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_validation = '{}/output/raw-labeled-split-balanced-header-validation'.format(scikit_processing_job_s3_output_prefix)\n",
    "prefix_test = '{}/output/raw-labeled-split-balanced-header-test'.format(scikit_processing_job_s3_output_prefix)\n",
    "\n",
    "path_train = './{}'.format(prefix_train)\n",
    "path_validation = './{}'.format(prefix_validation)\n",
    "path_test = './{}'.format(prefix_test)\n",
    "\n",
    "train_s3_uri = 's3://{}/{}'.format(bucket, prefix_train)\n",
    "validation_s3_uri = 's3://{}/{}'.format(bucket, prefix_validation)\n",
    "test_s3_uri = 's3://{}/{}'.format(bucket, prefix_test)\n",
    "\n",
    "s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri, content_type='text/csv')\n",
    "s3_input_validation_data = sagemaker.s3_input(s3_data=validation_s3_uri, content_type='text/csv')\n",
    "s3_input_test_data = sagemaker.s3_input(s3_data=test_s3_uri, content_type='text/csv')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import os\r\n",
      "import argparse\r\n",
      "import csv\r\n",
      "import pickle as pkl\r\n",
      "import pandas as pd\r\n",
      "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\r\n",
      "import sklearn\r\n",
      "from sklearn import metrics\r\n",
      "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
      "import re\r\n",
      "import glob\r\n",
      "import json\r\n",
      "import subprocess\r\n",
      "import sys\r\n",
      "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'simpletransformers'])\r\n",
      "import torch\r\n",
      "import torch.distributed as dist\r\n",
      "import torch.utils.data\r\n",
      "import torch.utils.data.distributed\r\n",
      "\r\n",
      "import simpletransformers\r\n",
      "from simpletransformers.classification import ClassificationModel\r\n",
      "\r\n",
      "# Note:  header=None\r\n",
      "def load_dataset(path, sep, header):\r\n",
      "    data = pd.concat([pd.read_csv(f, sep=sep, header=header) for f in glob.glob('{}/*.csv'.format(path))], ignore_index = True)\r\n",
      "\r\n",
      "    labels = data.iloc[:,0]\r\n",
      "    features = data.drop(data.columns[0], axis=1)\r\n",
      "    \r\n",
      "    if header==None:\r\n",
      "        # Adjust the column names after dropped the 0th column above\r\n",
      "        # New column names are 0 (inclusive) to len(features.columns) (exclusive)\r\n",
      "        new_column_names = list(range(0, len(features.columns)))\r\n",
      "        features.columns = new_column_names\r\n",
      "\r\n",
      "    return features, labels\r\n",
      "\r\n",
      "\r\n",
      "if __name__ == '__main__':\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "    parser.add_argument('--model-type', type=str, default='bert')\r\n",
      "    parser.add_argument('--model-name', type=str, default='bert-base-cased')\r\n",
      "    parser.add_argument('--use-cuda', type=bool, default=False)\r\n",
      "    parser.add_argument('--backend', type=str, default='gloo')\r\n",
      "    parser.add_argument('--train-data', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\r\n",
      "    parser.add_argument('--validation-data', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])\r\n",
      "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\r\n",
      "    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))\r\n",
      "    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])\r\n",
      "    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])\r\n",
      "\r\n",
      "    args, _ = parser.parse_known_args()   \r\n",
      "    model_type = args.model_type\r\n",
      "    model_name = args.model_name\r\n",
      "#    use_cuda = args.use_cuda\r\n",
      "    backend = args.backend\r\n",
      "    train_data = args.train_data\r\n",
      "    validation_data = args.validation_data\r\n",
      "    model_dir = args.model_dir\r\n",
      "    hosts = args.hosts\r\n",
      "    current_host = args.current_host\r\n",
      "    num_gpus = args.num_gpus\r\n",
      "\r\n",
      "################\r\n",
      "# From https://github.com/aws/sagemaker-python-sdk/issues/1110\r\n",
      "    is_distributed = len(args.hosts) > 1 and args.backend is not None\r\n",
      "    print('Distributed training - {}'.format(is_distributed))\r\n",
      "    use_cuda = args.num_gpus > 0\r\n",
      "    print('Number of gpus available - {}'.format(args.num_gpus))\r\n",
      "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\r\n",
      "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\r\n",
      "\r\n",
      "    if is_distributed:\r\n",
      "        # Initialize the distributed environment.\r\n",
      "        world_size = len(args.hosts)\r\n",
      "        os.environ['WORLD_SIZE'] = str(world_size)\r\n",
      "        host_rank = args.hosts.index(args.current_host)\r\n",
      "        os.environ['RANK'] = str(host_rank)\r\n",
      "        dist.init_process_group(backend=args.backend, rank=host_rank, world_size=world_size)\r\n",
      "        print('Initialized the distributed environment: \\'{}\\' backend on {} nodes. '.format(\r\n",
      "            args.backend, dist.get_world_size()) + 'Current host rank is {}. Number of gpus: {}'.format(\r\n",
      "            dist.get_rank(), args.num_gpus))\r\n",
      "###############\r\n",
      "\r\n",
      "#    X_train, y_train = load_dataset(train_data, ',', header=None)\r\n",
      "#    X_validation, y_validation = load_dataset(validation_data, ',', header=None)\r\n",
      "\r\n",
      "    # TODO:  Change this to use SM_CHANNEL_TRAIN, etc\r\n",
      "\r\n",
      "    df = pd.read_csv('./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz', \r\n",
      "                 delimiter='\\t', \r\n",
      "                 quoting=csv.QUOTE_NONE,\r\n",
      "                 compression='gzip')\r\n",
      "\r\n",
      "    # Enrich the data\r\n",
      "    df['is_positive_sentiment'] = (df['star_rating'] >= 4).astype(int)\r\n",
      "\r\n",
      "    df_bert = df[['review_body', 'is_positive_sentiment']]\r\n",
      "    df_bert.columns = ['text', 'labels']\r\n",
      "    df_bert.head(5)\r\n",
      "\r\n",
      "    df_bert = df_bert[:2000]\r\n",
      "    df_bert.shape\r\n",
      "\r\n",
      "    from sklearn.model_selection import train_test_split\r\n",
      "\r\n",
      "    df_bert_train, df_bert_holdout = train_test_split(df_bert, test_size=0.10)\r\n",
      "    df_bert_validation, df_bert_test = train_test_split(df_bert_holdout, test_size=0.50)\r\n",
      "\r\n",
      "    print(df_bert_train.shape)\r\n",
      "    print(df_bert_validation.shape)\r\n",
      "    print(df_bert_test.shape)\r\n",
      "\r\n",
      "    # TODO:  change output_dir to SM_model_dir or output_path\r\n",
      "    bert_args = {\r\n",
      "       'output_dir': model_dir, \r\n",
      "       'cache_dir': 'cache/',\r\n",
      "       'fp16': False,\r\n",
      "       'max_seq_length': 128,\r\n",
      "       'train_batch_size': 8,\r\n",
      "       'eval_batch_size': 8,\r\n",
      "       'gradient_accumulation_steps': 1,\r\n",
      "       'num_train_epochs': 1,\r\n",
      "       'weight_decay': 0,\r\n",
      "       'learning_rate': 3e-5,\r\n",
      "       'adam_epsilon': 1e-8,\r\n",
      "       'warmup_ratio': 0.06,\r\n",
      "       'warmup_steps': 0,\r\n",
      "       'max_grad_norm': 1.0,\r\n",
      "       'logging_steps': 50,\r\n",
      "       'evaluate_during_training': False,\r\n",
      "       'save_steps': 2000,\r\n",
      "       'eval_all_checkpoints': True,\r\n",
      "       'use_tensorboard': True,\r\n",
      "       'tensorboard_dir': 'tensorboard',\r\n",
      "       'overwrite_output_dir': True,\r\n",
      "       'reprocess_input_data': False,\r\n",
      "    }\r\n",
      "\r\n",
      "    bert_model = ClassificationModel(model_type='distilbert', # bert, distilbert, etc, etc.\r\n",
      "                                     model_name='distilbert-base-cased',\r\n",
      "                                     args=bert_args,\r\n",
      "                                     use_cuda=False)\r\n",
      "\r\n",
      "    bert_model.train_model(train_df=df_bert_train,\r\n",
      "                       eval_df=df_bert_validation,\r\n",
      "                       show_running_loss=True)\r\n",
      "\r\n",
      "    # TODO:  use the model_dir that is passed in through args\r\n",
      "    #        (currently SM_MODEL_DIR)\r\n",
      "#    os.makedirs(model_dir, exist_ok=True)\r\n",
      "#    model_path = os.path.join(model_dir, 'bert-model')\r\n",
      "\r\n",
      "#    pkl.dump(bert_model, open(model_path, 'wb'))\r\n",
      "#    print('Wrote model to {}'.format(model_path))\r\n",
      "   \r\n",
      "    result, model_outputs, wrong_predictions = bert_model.eval_model(eval_df=df_bert_test, acc=sklearn.metrics.accuracy_score)\r\n",
      "\r\n",
      "    print(result)\r\n",
      "\r\n",
      "    # Show bad predictions\r\n",
      "    print('Number of wrong predictions: {}'.format(len(wrong_predictions)))\r\n",
      "    print('\\n')\r\n",
      "\r\n",
      "    for prediction in wrong_predictions:\r\n",
      "        print(prediction.text_a)\r\n",
      "        print('\\n')\r\n",
      "\r\n",
      "    predictions, raw_outputs = bert_model.predict([\"\"\"I really enjoyed this item.  I highly recommend it.\"\"\"])\r\n",
      "\r\n",
      "    print('Predictions: {}'.format(predictions))\r\n",
      "    print('Raw outputs: {}'.format(raw_outputs))\r\n",
      "\r\n",
      "    predictions, raw_outputs = bert_model.predict([\"\"\"This item is awful and terrible.\"\"\"])\r\n",
      "\r\n",
      "    print('Predictions: {}'.format(predictions))\r\n",
      "    print('Raw outputs: {}'.format(raw_outputs))\r\n"
     ]
    }
   ],
   "source": [
    "!cat src_bert/bert_reviews.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "model_output_path = 's3://{}/models/bert/script-mode/training-runs'.format(bucket)\n",
    "\n",
    "bert_estimator = PyTorch(entry_point='bert_reviews.py',\n",
    "                         source_dir='src_bert',\n",
    "                         role=role,\n",
    "                         train_instance_count=2, \n",
    "                         train_instance_type='ml.c5.9xlarge',\n",
    "                         py_version='py3',\n",
    "                         framework_version='1.4.0',\n",
    "                         output_path=model_output_path,\n",
    "                         hyperparameters={'model_type':'bert',\n",
    "                                          'model_name': 'bert-base-cased',\n",
    "                                          'backend': 'gloo'},\n",
    "                         enable_cloudwatch_metrics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_estimator.fit(inputs={'train': s3_input_train_data, \n",
    "                           'validation': s3_input_validation_data,}, \n",
    "                   wait=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_job_name:  pytorch-training-2020-03-21-05-09-07-267\n"
     ]
    }
   ],
   "source": [
    "training_job_name = bert_estimator.latest_training_job.name\n",
    "print('training_job_name:  {}'.format(training_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-21 05:09:34 Starting - Starting the training job...\n",
      "2020-03-21 05:09:36 Starting - Launching requested ML instances.........\n",
      "2020-03-21 05:11:08 Starting - Preparing the instances for training...\n",
      "2020-03-21 05:11:49 Downloading - Downloading input data\n",
      "2020-03-21 05:11:49 Training - Downloading the training image...\n",
      "2020-03-21 05:12:23 Training - Training image download completed. Training in progress.\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:23,516 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:23,518 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:23,527 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:24,341 sagemaker-containers INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:24,344 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:24,353 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:27,473 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:29,764 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:34,710 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:34,711 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:34,711 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:34,711 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:34,973 sagemaker-containers INFO     Module default_user_module_name does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:34,973 sagemaker-containers INFO     Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:34,973 sagemaker-containers INFO     Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:34,973 sagemaker-containers INFO     Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /tmp/tmpddjonyv8/module_dir\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[34mProcessing /tmp/tmpq1g60ue4/module_dir\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: default-user-module-name\n",
      "  Building wheel for default-user-module-name (setup.py): started\u001b[0m\n",
      "\u001b[35m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=4365 sha256=5ec1dda6698df989fc9931a8dc135f1e3e362342e6366a7224de4ea649b4a895\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-lvxc_8rr/wheels/0c/06/0e/d1da0eea442aa9f98773a6fe325fcbeb65f4c600d59b00f591\u001b[0m\n",
      "\u001b[35mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[35mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[35mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[34m  Building wheel for default-user-module-name (setup.py): finished with status 'done'\n",
      "  Created wheel for default-user-module-name: filename=default_user_module_name-1.0.0-py2.py3-none-any.whl size=4364 sha256=c7272e6a7e53f5ba2287b267cba4ba1173d5c15fd55ac735a0fcc53d86c5ec37\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-r6fzoayx/wheels/1d/74/40/ec0b08ced59da72d74ec9f271c46b1e7dccb636cf639bd513c\u001b[0m\n",
      "\u001b[34mSuccessfully built default-user-module-name\u001b[0m\n",
      "\u001b[34mInstalling collected packages: default-user-module-name\u001b[0m\n",
      "\u001b[34mSuccessfully installed default-user-module-name-1.0.0\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:41,602 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:41,615 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:41,626 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2020-03-21 05:12:41,636 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_name\": \"bert-base-cased\",\n",
      "        \"model_type\": \"bert\",\n",
      "        \"backend\": \"gloo\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"pytorch-training-2020-03-21-05-09-07-267\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-835319576252/pytorch-training-2020-03-21-05-09-07-267/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"bert_reviews\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 36,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"bert_reviews.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"backend\":\"gloo\",\"model_name\":\"bert-base-cased\",\"model_type\":\"bert\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=bert_reviews.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=bert_reviews\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=36\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-835319576252/pytorch-training-2020-03-21-05-09-07-267/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"model_name\":\"bert-base-cased\",\"model_type\":\"bert\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"pytorch-training-2020-03-21-05-09-07-267\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-835319576252/pytorch-training-2020-03-21-05-09-07-267/source/sourcedir.tar.gz\",\"module_name\":\"bert_reviews\",\"network_interface_name\":\"eth0\",\"num_cpus\":36,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"bert_reviews.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--model_name\",\"bert-base-cased\",\"--model_type\",\"bert\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[35mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_NAME=bert-base-cased\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_TYPE=bert\u001b[0m\n",
      "\u001b[35mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python bert_reviews.py --backend gloo --model_name bert-base-cased --model_type bert\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:41,502 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:41,515 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:41,526 sagemaker-containers INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2020-03-21 05:12:41,536 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"validation\": \"/opt/ml/input/data/validation\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"model_name\": \"bert-base-cased\",\n",
      "        \"model_type\": \"bert\",\n",
      "        \"backend\": \"gloo\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"validation\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"ContentType\": \"text/csv\",\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2020-03-21-05-09-07-267\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-835319576252/pytorch-training-2020-03-21-05-09-07-267/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"bert_reviews\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 36,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"bert_reviews.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"model_name\":\"bert-base-cased\",\"model_type\":\"bert\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=bert_reviews.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\",\"validation\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=bert_reviews\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=36\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-835319576252/pytorch-training-2020-03-21-05-09-07-267/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\",\"validation\":\"/opt/ml/input/data/validation\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"backend\":\"gloo\",\"model_name\":\"bert-base-cased\",\"model_type\":\"bert\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"validation\":{\"ContentType\":\"text/csv\",\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2020-03-21-05-09-07-267\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-835319576252/pytorch-training-2020-03-21-05-09-07-267/source/sourcedir.tar.gz\",\"module_name\":\"bert_reviews\",\"network_interface_name\":\"eth0\",\"num_cpus\":36,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"bert_reviews.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--model_name\",\"bert-base-cased\",\"--model_type\",\"bert\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VALIDATION=/opt/ml/input/data/validation\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=bert-base-cased\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_TYPE=bert\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python bert_reviews.py --backend gloo --model_name bert-base-cased --model_type bert\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mCollecting simpletransformers\u001b[0m\n",
      "\u001b[34mCollecting simpletransformers\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m#015Downloading:   0%|          | 0.00/939 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 939/939 [00:00<00:00, 975kB/s]\n",
      "  Downloading simpletransformers-0.22.1-py3-none-any.whl (144 kB)\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/939 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 939/939 [00:00<00:00, 1.18MB/s]\n",
      "  Downloading simpletransformers-0.22.1-py3-none-any.whl (144 kB)\u001b[0m\n",
      "\u001b[35m#015Downloading:   0%|          | 0.00/263M [00:00<?, ?B/s]#015Downloading:   3%|▎         | 7.84M/263M [00:00<00:03, 78.4MB/s]#015Downloading:   6%|▌         | 16.0M/263M [00:00<00:03, 79.4MB/s]#015Downloading:   9%|▉         | 24.3M/263M [00:00<00:02, 80.5MB/s]#015Downloading:  12%|█▏        | 32.6M/263M [00:00<00:02, 81.1MB/s]#015Downloading:  16%|█▌        | 40.9M/263M [00:00<00:02, 81.8MB/s]#015Downloading:  19%|█▉        | 49.4M/263M [00:00<00:02, 82.6MB/s]#015Downloading:  22%|██▏       | 57.9M/263M [00:00<00:02, 83.2MB/s]#015Downloading:  25%|██▌       | 66.4M/263M [00:00<00:02, 83.7MB/s]#015Downloading:  28%|██▊       | 74.3M/263M [00:00<00:02, 80.2MB/s]#015Downloading:  31%|███       | 82.1M/263M [00:01<00:02, 79.6MB/s]#015Downloading:  34%|███▍      | 89.9M/263M [00:01<00:02, 77.1MB/s]#015Downloading:  37%|███▋      | 97.6M/263M [00:01<00:02, 77.3MB/s]#015Downloading:  40%|███▉      | 105M/263M [00:01<00:02, 66.8MB/s] #015Downloading:  43%|████▎     | 113M/263M [00:01<00:02, 69.3MB/s]#015Downloading:  46%|████▌     | 120M/263M [00:01<00:02, 70.8MB/s]#015Downloading:  49%|████▊     | 128M/263M [00:01<00:01, 72.3MB/s]#015Downloading:  52%|█████▏    | 136M/263M [00:01<00:01, 73.4MB/s]#015Downloading:  54%|█████▍    | 143M/263M [00:01<00:01, 73.9MB/s]#015Downloading:  57%|█████▋    | 151M/263M [00:01<00:01, 75.3MB/s]#015Downloading:  60%|██████    | 159M/263M [00:02<00:01, 75.0MB/s]#015Downloading:  63%|██████▎   | 166M/263M [00:02<00:01, 75.0MB/s]#015Downloading:  66%|██████▌   | 174M/263M [00:02<00:01, 75.0MB/s]#015Downloading:  69%|██████▉   | 181M/263M [00:02<00:01, 75.1MB/s]#015Downloading:  72%|███████▏  | 189M/263M [00:02<00:01, 74.1MB/s]#015Downloading:  74%|███████▍  | 196M/263M [00:02<00:01, 55.2MB/s]#015Downloading:  78%|███████▊  | 204M/263M [00:02<00:00, 61.2MB/s]#015Downloading:  80%|████████  | 211M/263M [00:02<00:01, 51.2MB/s]#015Downloading:  83%|████████▎ | 219M/263M [00:03<00:00, 56.6MB/s]#015Downloading:  86%|████████▌ | 226M/263M [00:03<00:00, 60.9MB/s]#015Downloading:  89%|████████▊ | 234M/263M [00:03<00:00, 64.6MB/s]#015Downloading:  92%|█████████▏| 241M/263M [00:03<00:00, 67.4MB/s]#015Downloading:  94%|█████████▍| 248M/263M [00:03<00:00, 69.3MB/s]#015Downloading:  97%|█████████▋| 256M/263M [00:03<00:00, 71.1MB/s]#015Downloading: 100%|██████████| 263M/263M [00:03<00:00, 72.0MB/s]\u001b[0m\n",
      "\u001b[35mCollecting regex\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/263M [00:00<?, ?B/s]#015Downloading:   3%|▎         | 7.87M/263M [00:00<00:03, 78.7MB/s]#015Downloading:   6%|▌         | 16.1M/263M [00:00<00:03, 79.6MB/s]#015Downloading:   9%|▉         | 23.8M/263M [00:00<00:03, 78.9MB/s]#015Downloading:  12%|█▏        | 31.9M/263M [00:00<00:02, 79.6MB/s]#015Downloading:  14%|█▍        | 38.0M/263M [00:00<00:03, 61.9MB/s]#015Downloading:  17%|█▋        | 45.2M/263M [00:00<00:03, 64.7MB/s]#015Downloading:  20%|█▉        | 52.5M/263M [00:00<00:03, 66.9MB/s]#015Downloading:  23%|██▎       | 60.2M/263M [00:00<00:02, 69.7MB/s]#015Downloading:  26%|██▌       | 67.7M/263M [00:00<00:02, 71.2MB/s]#015Downloading:  29%|██▊       | 75.1M/263M [00:01<00:02, 72.0MB/s]#015Downloading:  31%|███▏      | 82.7M/263M [00:01<00:02, 73.0MB/s]#015Downloading:  34%|███▍      | 90.0M/263M [00:01<00:02, 73.2MB/s]#015Downloading:  37%|███▋      | 97.6M/263M [00:01<00:02, 74.1MB/s]#015Downloading:  40%|███▉      | 105M/263M [00:01<00:02, 74.4MB/s] #015Downloading:  43%|████▎     | 113M/263M [00:01<00:02, 74.5MB/s]#015Downloading:  46%|████▌     | 120M/263M [00:01<00:01, 74.9MB/s]#015Downloading:  49%|████▊     | 128M/263M [00:01<00:01, 75.1MB/s]#015Downloading:  51%|█████▏    | 135M/263M [00:01<00:01, 74.9MB/s]#015Downloading:  54%|█████▍    | 143M/263M [00:01<00:01, 75.2MB/s]#015Downloading:  57%|█████▋    | 150M/263M [00:02<00:01, 74.7MB/s]#015Downloading:  60%|█████▉    | 158M/263M [00:02<00:01, 74.9MB/s]#015Downloading:  63%|██████▎   | 165M/263M [00:02<00:01, 74.6MB/s]#015Downloading:  66%|██████▌   | 173M/263M [00:02<00:01, 74.2MB/s]#015Downloading:  68%|██████▊   | 180M/263M [00:02<00:01, 74.1MB/s]#015Downloading:  72%|███████▏  | 189M/263M [00:02<00:01, 71.2MB/s]#015Downloading:  74%|███████▍  | 196M/263M [00:02<00:00, 70.1MB/s]#015Downloading:  78%|███████▊  | 204M/263M [00:02<00:00, 73.3MB/s]#015Downloading:  80%|████████  | 212M/263M [00:02<00:00, 63.9MB/s]#015Downloading:  83%|████████▎ | 218M/263M [00:03<00:00, 59.0MB/s]#015Downloading:  86%|████████▌ | 225M/263M [00:03<00:00, 62.2MB/s]#015Downloading:  88%|████████▊ | 233M/263M [00:03<00:00, 65.0MB/s]#015Downloading:  91%|█████████ | 240M/263M [00:03<00:00, 67.4MB/s]#015Downloading:  94%|█████████▍| 248M/263M [00:03<00:00, 69.6MB/s]#015Downloading:  97%|█████████▋| 255M/263M [00:03<00:00, 70.6MB/s]#015Downloading: 100%|██████████| 263M/263M [00:03<00:00, 71.6MB/s]\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (2.22.0)\u001b[0m\n",
      "\u001b[35m#015Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 213k/213k [00:00<00:00, 37.9MB/s]\n",
      "  Downloading regex-2020.2.20-cp36-cp36m-manylinux2010_x86_64.whl (690 kB)\u001b[0m\n",
      "\u001b[35m#015Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[35mCollecting transformers\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]#015Downloading: 100%|██████████| 213k/213k [00:00<00:00, 40.5MB/s]\u001b[0m\n",
      "\u001b[34mCollecting tensorboardx\u001b[0m\n",
      "\u001b[34m#015Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   0%|          | 0/225 [00:00<?, ?it/s]#033[A\n",
      "  Downloading transformers-2.5.1-py3-none-any.whl (499 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   0%|          | 1/225 [00:00<02:43,  1.37it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (1.16.4)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   0%|          | 0/225 [00:00<?, ?it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (0.21.2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   0%|          | 1/225 [00:00<03:13,  1.16it/s]#033[AERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34mCollecting regex\n",
      "\n",
      "  Downloading regex-2020.2.20-cp36-cp36m-manylinux2010_x86_64.whl (690 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   1%|          | 2/225 [00:01<02:31,  1.47it/s]#033[A\u001b[0m\n",
      "\u001b[35mCollecting tensorboardx\u001b[0m\n",
      "\u001b[35m#015Current iteration:   1%|▏         | 3/225 [00:01<02:22,  1.56it/s]#033[A\n",
      "  Downloading tensorboardX-2.0-py2.py3-none-any.whl (195 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   1%|          | 2/225 [00:01<02:52,  1.29it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (4.42.1)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   2%|▏         | 4/225 [00:02<02:15,  1.63it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (2.22.0)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   1%|▏         | 3/225 [00:01<02:37,  1.41it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting transformers\u001b[0m\n",
      "\u001b[34m#015Current iteration:   2%|▏         | 4/225 [00:02<02:24,  1.53it/s]#033[A\n",
      "  Downloading transformers-2.5.1-py3-none-any.whl (499 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   2%|▏         | 5/225 [00:02<02:08,  1.71it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (1.2.2)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   3%|▎         | 6/225 [00:03<02:06,  1.73it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (0.21.2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   2%|▏         | 5/225 [00:03<02:13,  1.65it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (1.16.4)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   3%|▎         | 6/225 [00:03<02:08,  1.70it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting seqeval\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m#015Current iteration:   3%|▎         | 7/225 [00:04<02:03,  1.77it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (0.25.0)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   4%|▎         | 8/225 [00:04<02:00,  1.80it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (4.42.1)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   3%|▎         | 7/225 [00:04<02:05,  1.74it/s]#033[A\n",
      "  Downloading seqeval-0.0.12.tar.gz (21 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   4%|▎         | 8/225 [00:04<02:02,  1.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (1.2.2)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   4%|▍         | 9/225 [00:05<01:58,  1.82it/s]#033[A\u001b[0m\n",
      "\u001b[35mCollecting seqeval\u001b[0m\n",
      "\u001b[35m#015Current iteration:   4%|▍         | 10/225 [00:05<01:56,  1.84it/s]#033[A\n",
      "  Downloading seqeval-0.0.12.tar.gz (21 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   4%|▍         | 9/225 [00:05<01:59,  1.81it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from simpletransformers) (0.25.0)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   4%|▍         | 10/225 [00:05<01:58,  1.81it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (2019.11.28)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   5%|▍         | 11/225 [00:06<01:55,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[35mCollecting sentencepiece\u001b[0m\n",
      "\u001b[35m#015Current iteration:   5%|▌         | 12/225 [00:06<01:54,  1.86it/s]#033[A\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   5%|▍         | 11/225 [00:06<01:58,  1.81it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (1.25.8)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   5%|▌         | 12/225 [00:06<01:59,  1.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (3.0.4)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   6%|▌         | 13/225 [00:07<01:54,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[35mCollecting filelock\u001b[0m\n",
      "\u001b[35m#015Current iteration:   6%|▌         | 14/225 [00:07<01:51,  1.89it/s]#033[A\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   6%|▌         | 13/225 [00:07<01:58,  1.78it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (2.8)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   6%|▌         | 14/225 [00:07<01:56,  1.81it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from tensorboardx->simpletransformers) (1.14.0)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   7%|▋         | 15/225 [00:08<01:51,  1.89it/s]#033[A\u001b[0m\n",
      "\u001b[35mCollecting tokenizers==0.5.2\u001b[0m\n",
      "\u001b[35m#015Current iteration:   7%|▋         | 16/225 [00:08<01:50,  1.89it/s]#033[A\n",
      "  Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   7%|▋         | 15/225 [00:08<01:53,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardx->simpletransformers) (3.11.3)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   7%|▋         | 16/225 [00:08<01:50,  1.89it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->simpletransformers) (0.14.1)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   8%|▊         | 17/225 [00:09<01:46,  1.95it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers->simpletransformers) (1.12.23)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   8%|▊         | 18/225 [00:09<01:48,  1.91it/s]#033[A\u001b[0m\n",
      "\u001b[35mCollecting sacremoses\u001b[0m\n",
      "\u001b[34m#015Current iteration:   8%|▊         | 17/225 [00:09<01:50,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting tokenizers==0.5.2\u001b[0m\n",
      "\u001b[35m#015Current iteration:   8%|▊         | 19/225 [00:10<01:49,  1.89it/s]#033[A\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:   9%|▉         | 20/225 [00:10<01:49,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardx->simpletransformers) (3.11.3)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   8%|▊         | 18/225 [00:10<01:49,  1.89it/s]#033[A\n",
      "  Downloading tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   8%|▊         | 19/225 [00:10<01:50,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece\u001b[0m\n",
      "\u001b[35m#015Current iteration:   9%|▉         | 21/225 [00:11<01:50,  1.85it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from tensorboardx->simpletransformers) (1.14.0)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   9%|▉         | 20/225 [00:11<01:52,  1.83it/s]#033[A\n",
      "  Downloading sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0 MB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:   9%|▉         | 21/225 [00:11<01:51,  1.83it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting sacremoses\u001b[0m\n",
      "\u001b[35m#015Current iteration:  10%|▉         | 22/225 [00:11<01:47,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (3.0.4)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  10%|█         | 23/225 [00:12<01:48,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (2019.11.28)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  10%|▉         | 22/225 [00:12<01:52,  1.80it/s]#033[A\n",
      "  Downloading sacremoses-0.0.38.tar.gz (860 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  10%|█         | 23/225 [00:12<01:50,  1.83it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from transformers->simpletransformers) (1.12.23)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  11%|█         | 24/225 [00:13<01:49,  1.84it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (2.8)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  11%|█         | 25/225 [00:13<01:45,  1.89it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->simpletransformers) (1.25.8)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  11%|█         | 24/225 [00:13<01:49,  1.84it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting filelock\u001b[0m\n",
      "\u001b[34m#015Current iteration:  11%|█         | 25/225 [00:13<01:49,  1.83it/s]#033[A\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  12%|█▏        | 26/225 [00:14<01:46,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn->simpletransformers) (0.14.1)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  12%|█▏        | 27/225 [00:14<01:46,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->simpletransformers) (2.8.1)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  12%|█▏        | 26/225 [00:14<01:48,  1.83it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting Keras>=2.2.4\u001b[0m\n",
      "\u001b[34m#015Current iteration:  12%|█▏        | 27/225 [00:14<01:47,  1.83it/s]#033[A\n",
      "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  12%|█▏        | 28/225 [00:15<01:45,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->simpletransformers) (2019.3)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  13%|█▎        | 29/225 [00:15<01:44,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[35mCollecting Keras>=2.2.4\u001b[0m\n",
      "\u001b[34m#015Current iteration:  12%|█▏        | 28/225 [00:15<01:45,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.6.1 in /opt/conda/lib/python3.6/site-packages (from pandas->simpletransformers) (2.8.1)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  13%|█▎        | 30/225 [00:16<01:44,  1.86it/s]#033[A\n",
      "  Downloading Keras-2.3.1-py2.py3-none-any.whl (377 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  14%|█▍        | 31/225 [00:16<01:44,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.9.5)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  13%|█▎        | 29/225 [00:16<01:44,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->simpletransformers) (2019.3)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  13%|█▎        | 30/225 [00:16<01:43,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (46.0.0.post20200309)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  14%|█▍        | 32/225 [00:17<01:43,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: botocore<1.16.0,>=1.15.23 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (1.15.23)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  15%|█▍        | 33/225 [00:17<01:41,  1.89it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.3.3)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  14%|█▍        | 31/225 [00:17<01:42,  1.89it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->simpletransformers) (7.1.1)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  14%|█▍        | 32/225 [00:17<01:43,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.16.0,>=1.15.23 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (1.15.23)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  15%|█▌        | 34/225 [00:18<01:42,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers->simpletransformers) (7.1.1)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  15%|█▍        | 33/225 [00:18<01:42,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.3.3)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  15%|█▌        | 34/225 [00:18<01:42,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->transformers->simpletransformers) (0.9.5)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  16%|█▌        | 35/225 [00:18<01:43,  1.84it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.8.0->tensorboardx->simpletransformers) (46.0.0.post20200309)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  16%|█▌        | 36/225 [00:19<01:42,  1.85it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (5.3)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  16%|█▌        | 35/225 [00:19<01:43,  1.84it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.9.0)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  16%|█▌        | 36/225 [00:19<01:40,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (5.3)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  16%|█▋        | 37/225 [00:20<01:40,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[35mCollecting keras-preprocessing>=1.0.5\u001b[0m\n",
      "\u001b[35m#015Current iteration:  17%|█▋        | 38/225 [00:20<01:39,  1.88it/s]#033[A\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  16%|█▋        | 37/225 [00:20<01:40,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting keras-preprocessing>=1.0.5\u001b[0m\n",
      "\u001b[34m#015Current iteration:  17%|█▋        | 38/225 [00:20<01:39,  1.88it/s]#033[A\n",
      "  Downloading Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  17%|█▋        | 39/225 [00:21<01:38,  1.89it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras>=2.2.4->seqeval->simpletransformers) (2.9.0)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  18%|█▊        | 40/225 [00:21<01:38,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[35mCollecting keras-applications>=1.0.6\u001b[0m\n",
      "\u001b[34m#015Current iteration:  17%|█▋        | 39/225 [00:21<01:39,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[34mCollecting keras-applications>=1.0.6\u001b[0m\n",
      "\u001b[34m#015Current iteration:  18%|█▊        | 40/225 [00:21<01:41,  1.83it/s]#033[A\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  18%|█▊        | 41/225 [00:22<01:38,  1.86it/s]#033[A\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  19%|█▊        | 42/225 [00:22<01:36,  1.90it/s]#033[A\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers->simpletransformers) (0.15.2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  18%|█▊        | 41/225 [00:22<01:41,  1.82it/s]#033[A\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.23->boto3->transformers->simpletransformers) (0.15.2)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  19%|█▉        | 43/225 [00:23<01:36,  1.89it/s]#033[A\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: seqeval, sacremoses\u001b[0m\n",
      "\u001b[35m#015Current iteration:  20%|█▉        | 44/225 [00:23<01:36,  1.87it/s]#033[A\n",
      "  Building wheel for seqeval (setup.py): started\u001b[0m\n",
      "\u001b[34m#015Current iteration:  19%|█▊        | 42/225 [00:23<01:41,  1.80it/s]#033[A\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: seqeval, sacremoses\u001b[0m\n",
      "\u001b[34m#015Current iteration:  19%|█▉        | 43/225 [00:23<01:40,  1.81it/s]#033[A\n",
      "  Building wheel for seqeval (setup.py): started\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[35m#015Current iteration:  20%|██        | 45/225 [00:24<01:37,  1.85it/s]#033[A\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35m#015Current iteration:  20%|██        | 46/225 [00:24<01:36,  1.85it/s]#033[A\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7423 sha256=c8d4293b72ce186c8402278cbcdbb1849c561cf69661382ff951db3033c13c28\u001b[0m\n",
      "\u001b[34m#015Current iteration:  20%|█▉        | 44/225 [00:24<01:40,  1.81it/s]#033[A\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34m#015Current iteration:  20%|██        | 45/225 [00:24<01:39,  1.81it/s]#033[A\n",
      "  Created wheel for seqeval: filename=seqeval-0.0.12-py3-none-any.whl size=7423 sha256=c8d4293b72ce186c8402278cbcdbb1849c561cf69661382ff951db3033c13c28\u001b[0m\n",
      "\u001b[35m#015Current iteration:  21%|██        | 47/225 [00:25<01:36,  1.84it/s]#033[A\n",
      "  Stored in directory: /root/.cache/pip/wheels/1f/1b/a6/a808a7e4d1f7584e42f5e279664cd48bf24ed8392218ce6be4\u001b[0m\n",
      "\u001b[35m#015Current iteration:  21%|██▏       | 48/225 [00:25<01:35,  1.85it/s]#033[A\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[34m#015Current iteration:  20%|██        | 46/225 [00:25<01:39,  1.80it/s]#033[A\n",
      "  Stored in directory: /root/.cache/pip/wheels/1f/1b/a6/a808a7e4d1f7584e42f5e279664cd48bf24ed8392218ce6be4\u001b[0m\n",
      "\u001b[34m#015Current iteration:  21%|██        | 47/225 [00:25<01:38,  1.80it/s]#033[A\n",
      "  Building wheel for sacremoses (setup.py): started\u001b[0m\n",
      "\u001b[35m#015Current iteration:  22%|██▏       | 49/225 [00:26<01:35,  1.84it/s]#033[A/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=8b45708a66b9aa43dea3d486eb0d2966b097aaf1bae0e41e3823059c31113e30\n",
      "\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\u001b[0m\n",
      "\u001b[34m#015Current iteration:  21%|██▏       | 48/225 [00:26<01:37,  1.81it/s]#033[A\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34m#015Current iteration:  22%|██▏       | 49/225 [00:26<01:38,  1.78it/s]#033[A/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:224: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-py3-none-any.whl size=884629 sha256=8b45708a66b9aa43dea3d486eb0d2966b097aaf1bae0e41e3823059c31113e30\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "  Stored in directory: /root/.cache/pip/wheels/03/e9/be/8b52f6e7e8c333b56f9440575b4c5eb4d96d27b5d22df5a71e\n",
      "\u001b[0m\n",
      "\u001b[34mSuccessfully built seqeval sacremoses\u001b[0m\n",
      "\u001b[35m#015Current iteration:  22%|██▏       | 50/225 [00:26<01:35,  1.84it/s]#033[A\u001b[0m\n",
      "\u001b[35mSuccessfully built seqeval sacremoses\u001b[0m\n",
      "\u001b[35m#015Current iteration:  23%|██▎       | 51/225 [00:27<01:33,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[35mInstalling collected packages: regex, sentencepiece, filelock, tokenizers, sacremoses, transformers, tensorboardx, keras-preprocessing, keras-applications, Keras, seqeval, simpletransformers\u001b[0m\n",
      "\u001b[34m#015Current iteration:  22%|██▏       | 50/225 [00:27<01:37,  1.80it/s]#033[A\u001b[0m\n",
      "\u001b[34mInstalling collected packages: tensorboardx, regex, tokenizers, sentencepiece, sacremoses, filelock, transformers, keras-preprocessing, keras-applications, Keras, seqeval, simpletransformers\u001b[0m\n",
      "\u001b[35m#015Current iteration:  23%|██▎       | 52/225 [00:28<01:32,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[35mSuccessfully installed Keras-2.3.1 filelock-3.0.12 keras-applications-1.0.8 keras-preprocessing-1.1.0 regex-2020.2.20 sacremoses-0.0.38 sentencepiece-0.1.85 seqeval-0.0.12 simpletransformers-0.22.1 tensorboardx-2.0 tokenizers-0.5.2 transformers-2.5.1\u001b[0m\n",
      "\u001b[35m#015Current iteration:  24%|██▎       | 53/225 [00:28<01:33,  1.85it/s]#033[A\u001b[0m\n",
      "\u001b[35mDistributed training - True\u001b[0m\n",
      "\u001b[34m#015Current iteration:  23%|██▎       | 51/225 [00:28<01:38,  1.77it/s]#033[A\u001b[0m\n",
      "\u001b[34mSuccessfully installed Keras-2.3.1 filelock-3.0.12 keras-applications-1.0.8 keras-preprocessing-1.1.0 regex-2020.2.20 sacremoses-0.0.38 sentencepiece-0.1.85 seqeval-0.0.12 simpletransformers-0.22.1 tensorboardx-2.0 tokenizers-0.5.2 transformers-2.5.1\u001b[0m\n",
      "\u001b[34m#015Current iteration:  23%|██▎       | 52/225 [00:28<01:35,  1.80it/s]#033[A\u001b[0m\n",
      "\u001b[34mDistributed training - True\u001b[0m\n",
      "\u001b[35m#015Current iteration:  24%|██▍       | 54/225 [00:29<01:33,  1.83it/s]#033[A\u001b[0m\n",
      "\u001b[35mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[35m#015Current iteration:  24%|██▍       | 55/225 [00:29<01:33,  1.82it/s]#033[A\u001b[0m\n",
      "\u001b[35mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 1. Number of gpus: 0\u001b[0m\n",
      "\u001b[34m#015Current iteration:  24%|██▎       | 53/225 [00:29<01:34,  1.81it/s]#033[A\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34m#015Current iteration:  24%|██▍       | 54/225 [00:29<01:33,  1.83it/s]#033[A\u001b[0m\n",
      "\u001b[34mInitialized the distributed environment: 'gloo' backend on 2 nodes. Current host rank is 0. Number of gpus: 0\u001b[0m\n",
      "\u001b[35m#015Current iteration:  25%|██▍       | 56/225 [00:30<01:33,  1.81it/s]#033[A\u001b[0m\n",
      "\u001b[35m(1800, 2)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  25%|██▌       | 57/225 [00:30<01:32,  1.82it/s]#033[A\u001b[0m\n",
      "\u001b[35m(100, 2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  24%|██▍       | 55/225 [00:30<01:32,  1.84it/s]#033[A\u001b[0m\n",
      "\u001b[34m(1800, 2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  25%|██▍       | 56/225 [00:30<01:32,  1.83it/s]#033[A\u001b[0m\n",
      "\u001b[34m(100, 2)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  26%|██▌       | 58/225 [00:31<01:31,  1.82it/s]#033[A\u001b[0m\n",
      "\u001b[35m(100, 2)\u001b[0m\n",
      "\u001b[35m#015Current iteration:  26%|██▌       | 59/225 [00:31<01:29,  1.85it/s]#033[A\u001b[0m\n",
      "\u001b[35m[2020-03-21 05:12:58.501 algo-2:95 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  25%|██▌       | 57/225 [00:31<01:29,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m(100, 2)\u001b[0m\n",
      "\u001b[34m#015Current iteration:  26%|██▌       | 58/225 [00:31<01:29,  1.87it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2020-03-21 05:12:58.553 algo-1:95 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[35m#015Current iteration:  27%|██▋       | 60/225 [00:32<01:27,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[35m[2020-03-21 05:12:58.501 algo-2:95 INFO hook.py:170] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[35m#015Current iteration:  27%|██▋       | 61/225 [00:32<01:27,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[35m[2020-03-21 05:12:58.502 algo-2:95 INFO hook.py:215] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m#015Current iteration:  26%|██▌       | 59/225 [00:32<01:28,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2020-03-21 05:12:58.553 algo-1:95 INFO hook.py:170] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m#015Current iteration:  27%|██▋       | 60/225 [00:32<01:28,  1.86it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2020-03-21 05:12:58.553 algo-1:95 INFO hook.py:215] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[35m#015Current iteration:  28%|██▊       | 62/225 [00:33<01:26,  1.89it/s]#033[A\u001b[0m\n",
      "\u001b[35m[2020-03-21 05:12:58.503 algo-2:95 INFO hook.py:351] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m#015Current iteration:  27%|██▋       | 61/225 [00:33<01:27,  1.88it/s]#033[A\u001b[0m\n",
      "\u001b[34m[2020-03-21 05:12:58.554 algo-1:95 INFO hook.py:351] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m#015Current iteration:  28%|██▊       | 62/225 [00:33<01:25,  1.91it/s]#033[A\u001b[0m\n",
      "\u001b[35m#015Current iteration:  28%|██▊       | 63/225 [00:33<01:25,  1.89it/s]#033[A\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "bert_estimator = PyTorch.attach(training_job_name=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the model artifact from AWS S3\n",
    "!aws s3 cp $model_output_path/$training_job_name/output/model.tar.gz ./models/bert/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import pickle as pkl\n",
    "\n",
    "tar = tarfile.open('./models/bert/model.tar.gz')\n",
    "tar.extractall(path='./models/bert-model')\n",
    "tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "args = {\n",
    "   'fp16': False,\n",
    "   'max_seq_length': 128\n",
    "}\n",
    "\n",
    "bert_model = ClassificationModel(model_type='distilbert', # bert, distilbert, etc, etc.\n",
    "                                 model_name='./models/bert-model',\n",
    "                                 args=args,\n",
    "                                 use_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, raw_outputs = bert_model.predict([\"\"\"I really enjoyed this item.  I highly recommend it.\"\"\"])\n",
    "\n",
    "print('Predictions: {}'.format(predictions))\n",
    "print('Raw outputs: {}'.format(raw_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, raw_outputs = bert_model.predict([\"\"\"This item is awful and terrible.\"\"\"])\n",
    "\n",
    "print('Predictions: {}'.format(predictions))\n",
    "print('Raw outputs: {}'.format(raw_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
