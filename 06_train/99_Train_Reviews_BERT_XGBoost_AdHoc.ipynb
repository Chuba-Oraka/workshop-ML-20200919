{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "izA3-6kffbdT"
   },
   "source": [
    "# BERT + XGBoost!\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-sentence-classification.png\" />\n",
    "\n",
    "In this notebook, we will use pre-trained deep learning model to process some text. We will then use the output of that model to classify the text. The text is a list of sentences from film reviews. And we will calssify each sentence as either speaking \"positively\" about its subject of \"negatively\".\n",
    "\n",
    "## Introduction\n",
    "Our goal is to create a model that takes a sentence (just like the ones in our dataset) and produces either 1 (indicating the sentence carries a positive sentiment) or a 0 (indicating the sentence carries a negative sentiment). We can think of it as looking like this:\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/sentiment-classifier-1.png\" />\n",
    "\n",
    "Under the hood, the model is actually made up of two model.\n",
    "\n",
    "* DistilBERT processes the sentence and passes along some information it extracted from it on to the next model. DistilBERT is a smaller version of BERT developed and open sourced by the team at HuggingFace. It’s a lighter and faster version of BERT that roughly matches its performance.\n",
    "* The next model, a basic Logistic Regression model from scikit learn will take in the result of DistilBERT’s processing, and classify the sentence as either positive or negative (1 or 0, respectively).\n",
    "\n",
    "The data we pass between the two models is a vector of size 768. We can think of this of vector as an embedding for the sentence that we can use for classification.\n",
    "\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/distilbert-bert-sentiment-classifier.png\" />\n",
    "\n",
    "## Dataset\n",
    "The dataset we will use in this example is [SST2](https://nlp.stanford.edu/sentiment/index.html), which contains sentences from movie reviews, each labeled as either positive (has the value 1) or negative (has the value 0):\n",
    "\n",
    "\n",
    "<table class=\"features-table\">\n",
    "  <tr>\n",
    "    <th class=\"mdc-text-light-green-600\">\n",
    "    sentence\n",
    "    </th>\n",
    "    <th class=\"mdc-text-purple-600\">\n",
    "    label\n",
    "    </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      a stirring , funny and finally transporting re imagining of beauty and the beast and 1930s horror films\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      apparently reassembled from the cutting room floor of any given daytime soap\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      0\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      they presume their audience won't sit still for a sociology lesson\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      0\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      this is a visually stunning rumination on love , memory , history and the war between art and commerce\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td class=\"mdc-bg-light-green-50\" style=\"text-align:left\">\n",
    "      jonathan parker 's bartleby should have been the be all end all of the modern office anomie films\n",
    "    </td>\n",
    "    <td class=\"mdc-bg-purple-50\">\n",
    "      1\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "## Installing the transformers library\n",
    "Let's start by installing the huggingface transformers library so we can load our deep learning NLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 632
    },
    "colab_type": "code",
    "id": "To9ENLU90WGl",
    "outputId": "4b46c997-c16c-4141-eaf2-e7aa7da6d3a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.5.1)\n",
      "Requirement already satisfied: tokenizers==0.5.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (0.5.2)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (1.12.26)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (1.14.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (2020.2.20)\n",
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (0.1.85)\n",
      "Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (0.0.38)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (4.43.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from transformers) (3.0.4)\n",
      "Requirement already satisfied: requests in /home/ec2-user/.local/lib/python3.6/site-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.26 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers) (1.15.26)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->transformers) (0.9.5)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers) (6.7)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sacremoses->transformers) (1.14.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers) (2.6)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (2.8.1)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.26->boto3->transformers) (0.15.2)\n",
      "\u001b[31mtensorflow 2.0.0 requires opt-einsum>=2.3.2, which is not installed.\u001b[0m\n",
      "\u001b[31mtensorflow 2.0.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 2.0.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.14.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.18.20 has requirement botocore==1.15.20, but you'll have botocore 1.15.26 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==1.0.2) (1.14.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==1.0.2) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas==1.0.2) (2018.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas==1.0.2) (1.14.0)\n",
      "\u001b[31mtensorflow 2.0.0 requires opt-einsum>=2.3.2, which is not installed.\u001b[0m\n",
      "\u001b[31mtensorflow 2.0.0 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mtensorflow 2.0.0 has requirement numpy<2.0,>=1.16.0, but you'll have numpy 1.14.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mawscli 1.18.20 has requirement botocore==1.15.20, but you'll have botocore 1.15.26 which is incompatible.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas==1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 63
    },
    "colab_type": "code",
    "id": "fvFvBLJV0Dkv",
    "outputId": "140119e5-4cee-4604-c0d2-be279c18b125"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zQ-42fh0hjsF"
   },
   "source": [
    "## Importing the dataset\n",
    "We'll use pandas to read the dataset and load it into a dataframe.  For performance reasons, we'll only use a subset of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-835319576252/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz to data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp 's3://{bucket}/amazon-reviews-pds/tsv/amazon_reviews_us_Digital_Software_v1_00.tsv.gz' ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102084, 15)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "df = pd.read_csv('./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz', \n",
    "                 delimiter='\\t', \n",
    "                 quoting=csv.QUOTE_NONE,\n",
    "                 compression='gzip')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>17747349</td>\n",
       "      <td>R2EI7QLPK4LF7U</td>\n",
       "      <td>B00U7LCE6A</td>\n",
       "      <td>106182406</td>\n",
       "      <td>CCleaner Free [Download]</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>So far so good</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10956619</td>\n",
       "      <td>R1W5OMFK1Q3I3O</td>\n",
       "      <td>B00HRJMOM4</td>\n",
       "      <td>162269768</td>\n",
       "      <td>ResumeMaker Professional Deluxe 18</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Three Stars</td>\n",
       "      <td>Needs a little more work.....</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>13132245</td>\n",
       "      <td>RPZWSYWRP92GI</td>\n",
       "      <td>B00P31G9PQ</td>\n",
       "      <td>831433899</td>\n",
       "      <td>Amazon Drive Desktop [PC]</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>One Star</td>\n",
       "      <td>Please cancel.</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>35717248</td>\n",
       "      <td>R2WQWM04XHD9US</td>\n",
       "      <td>B00FGDEPDY</td>\n",
       "      <td>991059534</td>\n",
       "      <td>Norton Internet Security 1 User 3 Licenses</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Works as Expected!</td>\n",
       "      <td>Works as Expected!</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>17710652</td>\n",
       "      <td>R1WSPK2RA2PDEF</td>\n",
       "      <td>B00FZ0FK0U</td>\n",
       "      <td>574904556</td>\n",
       "      <td>SecureAnywhere Intermet Security Complete 5 De...</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Great antivirus. Worthless customer support</td>\n",
       "      <td>I've had Webroot for a few years. It expired a...</td>\n",
       "      <td>2015-08-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     17747349  R2EI7QLPK4LF7U  B00U7LCE6A       106182406   \n",
       "1          US     10956619  R1W5OMFK1Q3I3O  B00HRJMOM4       162269768   \n",
       "2          US     13132245   RPZWSYWRP92GI  B00P31G9PQ       831433899   \n",
       "3          US     35717248  R2WQWM04XHD9US  B00FGDEPDY       991059534   \n",
       "4          US     17710652  R1WSPK2RA2PDEF  B00FZ0FK0U       574904556   \n",
       "\n",
       "                                       product_title  product_category  \\\n",
       "0                           CCleaner Free [Download]  Digital_Software   \n",
       "1                 ResumeMaker Professional Deluxe 18  Digital_Software   \n",
       "2                          Amazon Drive Desktop [PC]  Digital_Software   \n",
       "3         Norton Internet Security 1 User 3 Licenses  Digital_Software   \n",
       "4  SecureAnywhere Intermet Security Complete 5 De...  Digital_Software   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            4              0            0    N                 Y   \n",
       "1            3              0            0    N                 Y   \n",
       "2            1              1            2    N                 Y   \n",
       "3            5              0            0    N                 Y   \n",
       "4            4              1            2    N                 Y   \n",
       "\n",
       "                               review_headline  \\\n",
       "0                                   Four Stars   \n",
       "1                                  Three Stars   \n",
       "2                                     One Star   \n",
       "3                           Works as Expected!   \n",
       "4  Great antivirus. Worthless customer support   \n",
       "\n",
       "                                         review_body review_date  \n",
       "0                                     So far so good  2015-08-31  \n",
       "1                      Needs a little more work.....  2015-08-31  \n",
       "2                                     Please cancel.  2015-08-31  \n",
       "3                                 Works as Expected!  2015-08-31  \n",
       "4  I've had Webroot for a few years. It expired a...  2015-08-31  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>marketplace</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_parent</th>\n",
       "      <th>product_title</th>\n",
       "      <th>product_category</th>\n",
       "      <th>star_rating</th>\n",
       "      <th>helpful_votes</th>\n",
       "      <th>total_votes</th>\n",
       "      <th>vine</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>review_headline</th>\n",
       "      <th>review_body</th>\n",
       "      <th>review_date</th>\n",
       "      <th>is_positive_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US</td>\n",
       "      <td>17747349</td>\n",
       "      <td>R2EI7QLPK4LF7U</td>\n",
       "      <td>B00U7LCE6A</td>\n",
       "      <td>106182406</td>\n",
       "      <td>CCleaner Free [Download]</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Four Stars</td>\n",
       "      <td>So far so good</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US</td>\n",
       "      <td>10956619</td>\n",
       "      <td>R1W5OMFK1Q3I3O</td>\n",
       "      <td>B00HRJMOM4</td>\n",
       "      <td>162269768</td>\n",
       "      <td>ResumeMaker Professional Deluxe 18</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Three Stars</td>\n",
       "      <td>Needs a little more work.....</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US</td>\n",
       "      <td>13132245</td>\n",
       "      <td>RPZWSYWRP92GI</td>\n",
       "      <td>B00P31G9PQ</td>\n",
       "      <td>831433899</td>\n",
       "      <td>Amazon Drive Desktop [PC]</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>One Star</td>\n",
       "      <td>Please cancel.</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US</td>\n",
       "      <td>35717248</td>\n",
       "      <td>R2WQWM04XHD9US</td>\n",
       "      <td>B00FGDEPDY</td>\n",
       "      <td>991059534</td>\n",
       "      <td>Norton Internet Security 1 User 3 Licenses</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Works as Expected!</td>\n",
       "      <td>Works as Expected!</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US</td>\n",
       "      <td>17710652</td>\n",
       "      <td>R1WSPK2RA2PDEF</td>\n",
       "      <td>B00FZ0FK0U</td>\n",
       "      <td>574904556</td>\n",
       "      <td>SecureAnywhere Intermet Security Complete 5 De...</td>\n",
       "      <td>Digital_Software</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Great antivirus. Worthless customer support</td>\n",
       "      <td>I've had Webroot for a few years. It expired a...</td>\n",
       "      <td>2015-08-31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  marketplace  customer_id       review_id  product_id  product_parent  \\\n",
       "0          US     17747349  R2EI7QLPK4LF7U  B00U7LCE6A       106182406   \n",
       "1          US     10956619  R1W5OMFK1Q3I3O  B00HRJMOM4       162269768   \n",
       "2          US     13132245   RPZWSYWRP92GI  B00P31G9PQ       831433899   \n",
       "3          US     35717248  R2WQWM04XHD9US  B00FGDEPDY       991059534   \n",
       "4          US     17710652  R1WSPK2RA2PDEF  B00FZ0FK0U       574904556   \n",
       "\n",
       "                                       product_title  product_category  \\\n",
       "0                           CCleaner Free [Download]  Digital_Software   \n",
       "1                 ResumeMaker Professional Deluxe 18  Digital_Software   \n",
       "2                          Amazon Drive Desktop [PC]  Digital_Software   \n",
       "3         Norton Internet Security 1 User 3 Licenses  Digital_Software   \n",
       "4  SecureAnywhere Intermet Security Complete 5 De...  Digital_Software   \n",
       "\n",
       "   star_rating  helpful_votes  total_votes vine verified_purchase  \\\n",
       "0            4              0            0    N                 Y   \n",
       "1            3              0            0    N                 Y   \n",
       "2            1              1            2    N                 Y   \n",
       "3            5              0            0    N                 Y   \n",
       "4            4              1            2    N                 Y   \n",
       "\n",
       "                               review_headline  \\\n",
       "0                                   Four Stars   \n",
       "1                                  Three Stars   \n",
       "2                                     One Star   \n",
       "3                           Works as Expected!   \n",
       "4  Great antivirus. Worthless customer support   \n",
       "\n",
       "                                         review_body review_date  \\\n",
       "0                                     So far so good  2015-08-31   \n",
       "1                      Needs a little more work.....  2015-08-31   \n",
       "2                                     Please cancel.  2015-08-31   \n",
       "3                                 Works as Expected!  2015-08-31   \n",
       "4  I've had Webroot for a few years. It expired a...  2015-08-31   \n",
       "\n",
       "   is_positive_sentiment  \n",
       "0                      1  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      1  \n",
       "4                      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_positive_sentiment'] = (df['star_rating'] >= 4).astype(int)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gTM3hOHW4hUY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1 = df[['review_body', 'is_positive_sentiment']][:100]\n",
    "batch_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_body</th>\n",
       "      <th>is_positive_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So far so good</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Needs a little more work.....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Please cancel.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Works as Expected!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've had Webroot for a few years. It expired a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_body  is_positive_sentiment\n",
       "0                                     So far so good                      1\n",
       "1                      Needs a little more work.....                      0\n",
       "2                                     Please cancel.                      0\n",
       "3                                 Works as Expected!                      1\n",
       "4  I've had Webroot for a few years. It expired a...                      1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_1.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRc2L89hh1Tf"
   },
   "source": [
    "We can ask pandas how many sentences are labeled as \"positive\" (value 1) and how many are labeled \"negative\" (having the value 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jGvcfcCP5xpZ",
    "outputId": "4c4a8afc-1035-4b21-ba9a-c4bb6cfc6347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority (positive) count: 59\n",
      "Minority (negative) count: 41\n",
      "Ratio of Majority to Minority: 1.4390243902439024\n"
     ]
    }
   ],
   "source": [
    "is_positive_sentiment_count = len(batch_1.query('is_positive_sentiment == 1'))\n",
    "is_negative_sentiment_count = len(batch_1.query('is_positive_sentiment == 0'))\n",
    "\n",
    "print('Majority (positive) count: {}'.format(is_positive_sentiment_count))\n",
    "print('Minority (negative) count: {}'.format(is_negative_sentiment_count))\n",
    "print('Ratio of Majority to Minority: {}'.format(is_positive_sentiment_count / is_negative_sentiment_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7_MO08_KiAOb"
   },
   "source": [
    "## Loading the Pre-trained BERT model\n",
    "Let's now load a pre-trained BERT model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "q1InADgf5xm2",
    "outputId": "dbc52856-4d52-42f8-8a74-a89944280a02"
   },
   "outputs": [],
   "source": [
    "# For DistilBERT (lightweight for a notebook like this):\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "# For Bert (requires a lot more memory):\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lZDBMn3wiSX6"
   },
   "source": [
    "Right now, the variable `model` holds a pretrained BERT or distilBERT model (a version of BERT that is smaller, but much faster and requiring a lot less memory.)\n",
    "\n",
    "## Preparing the Dataset\n",
    "Before we can hand our sentences to BERT, we need to so some minimal processing to put them in the format it requires.\n",
    "\n",
    "### Tokenization\n",
    "Our first step is to tokenize the sentences -- break them up into word and subwords in the format BERT is comfortable with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dg82ndBA5xlN"
   },
   "outputs": [],
   "source": [
    "tokenized = batch_1['review_body'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHwjUwYgi-uL"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tokenization-2-token-ids.png\" />\n",
    "\n",
    "### Padding\n",
    "After tokenization, `tokenized` is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "URn-DWJt5xhP"
   },
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mdjg306wjjmL"
   },
   "source": [
    "Our dataset is now in the `padded` variable, we can view its dimensions below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jdi7uXo95xeq",
    "outputId": "be786022-e84f-4e28-8531-0143af2347bc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 391)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(padded).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sDZBsYSDjzDV"
   },
   "source": [
    "### Masking\n",
    "If we directly send `padded` to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4K_iGRNa_Ozc",
    "outputId": "d03b0a9b-1f6e-4e32-831e-b04f5389e57c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 391)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "attention_mask.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jK-CQB9-kN99"
   },
   "source": [
    "## And Now, Deep Learning!\n",
    "Now that we have our model and inputs ready, let's run our model!\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-tutorial-sentence-embedding.png\" />\n",
    "\n",
    "The `model()` function runs our sentences through BERT. The results of the processing will be returned into `last_hidden_states`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39UVjAV56PJz"
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoCep_WVuB3v"
   },
   "source": [
    "Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called `[CLS]` (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.\n",
    "\n",
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-output-tensor-selection.png\" />\n",
    "\n",
    "We'll save those in the `features` variable, as they'll serve as the features to our logitics regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9t60At16PVs"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.6172475e-01, -1.5165129e-01,  6.8947881e-02, ...,\n",
       "        -3.7436504e-02,  2.2462969e-01,  1.3082646e-01],\n",
       "       [ 3.6312684e-02, -1.6463454e-01, -6.8841994e-02, ...,\n",
       "        -9.8265307e-03,  3.3971208e-01,  2.2334215e-01],\n",
       "       [-2.6727650e-02, -1.0930454e-01,  9.0976000e-02, ...,\n",
       "        -7.5316913e-02,  1.1025971e-01,  4.7545636e-01],\n",
       "       ...,\n",
       "       [-7.2206832e-02, -7.5432576e-02,  1.2837155e-01, ...,\n",
       "        -2.5279188e-01,  4.6122875e-02,  1.8310854e-01],\n",
       "       [-1.7792191e-02, -4.3500781e-02,  2.0042274e-02, ...,\n",
       "        -3.0581543e-01,  2.6719806e-01,  6.1388198e-02],\n",
       "       [ 3.0226892e-04,  9.8362081e-02,  1.7851058e-02, ...,\n",
       "        -7.0075884e-02,  3.2080364e-01,  4.1366479e-01]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_VZVU66Gurr-"
   },
   "source": [
    "The labels indicating which sentence is positive and negative now go into the `labels` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JD3fX2yh6PTx"
   },
   "outputs": [],
   "source": [
    "labels = batch_1['is_positive_sentiment']\n",
    "\n",
    "labels.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iaoEvM2evRx1"
   },
   "source": [
    "## Split the Embeddings into Train and Test Sets\n",
    "Let's now split the embeddings into training and test datasets.\n",
    "\n",
    "_Note that we were able to generate the BERT embeddings *before* we split - unlike TF/IDF requires us to split before we generate the TF/IDF embeddings or else test data will leak into train data which creates overly-optimistic performance metrics._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ddAqbkoU6PP9"
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, stratify=batch_1['is_positive_sentiment'])\n",
    "\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9bhSJpcv1Bl"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-distilbert-train-test-split-sentence-embedding.png\" />\n",
    "\n",
    "# [Bonus] Grid Search for Parameters\n",
    "We can dive into Logistic regression directly with the Scikit Learn default parameters, but sometimes it's worth searching for the best value of the C parameter, which determines regularization strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cyEwr7yYD3Ci"
   },
   "outputs": [],
   "source": [
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scores: ', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KCT9u8vAwnID"
   },
   "source": [
    "We now train the LogisticRegression model. If you've chosen to do the gridsearch, you can plug the value of C into the model declaration (e.g. `LogisticRegression(C=5.2)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "gG-EVWx4CzBc",
    "outputId": "9252ceff-a7d0-4359-fef9-2f72be89c7d6"
   },
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3rUMKuVgwzkY"
   },
   "source": [
    "<img src=\"https://jalammar.github.io/images/distilBERT/bert-training-logistic-regression.png\" />\n",
    "\n",
    "## Evaluate the Model\n",
    "So how well does our model do in classifying sentences? One way is to check the accuracy against the testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iCoyxRJ7ECTA",
    "outputId": "cfd86dea-5d16-476c-ab9b-47cbee3a014f"
   },
   "outputs": [],
   "source": [
    "lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Lg4LOpoxSOR"
   },
   "source": [
    "## Reference Metrics\n",
    "For reference, the [highest accuracy score](http://nlpprogress.com/english/sentiment_analysis.html) for this dataset is currently **96.8**. DistilBERT can be trained to improve its score on this task – a process called **fine-tuning** which updates BERT’s weights to make it achieve a better performance in this sentence classification task (which we can call the downstream task). The fine-tuned DistilBERT turns out to achieve an accuracy score of **90.7**. The full size BERT model achieves **94.9**.\n",
    "\n",
    "\n",
    "\n",
    "And that’s it! That’s a good first contact with BERT. The next step would be to head over to the documentation and try your hand at [fine-tuning](https://huggingface.co/transformers/examples.html#glue). You can also go back and switch between distilBERT and BERT - and see how that works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "objective  = 'binary:logistic'\n",
    "max_depth  = 10\n",
    "num_round  = 10\n",
    "scale_pos_weight=1.43 # this is the ratio majority (positive sentiment count) to minority (not positive sentiment count)\n",
    "\n",
    "xgb_estimator = XGBClassifier(objective=objective,\n",
    "                              num_round=num_round,\n",
    "                              max_depth=max_depth,\n",
    "                              scale_pos_weight=scale_pos_weight)\n",
    "\n",
    "xgb_estimator.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test = xgb_estimator.predict(test_features)\n",
    "\n",
    "preds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, classification_report, confusion_matrix\n",
    "\n",
    "print('Test Accuracy: ', accuracy_score(test_labels, preds_test))\n",
    "print('Test Precision: ', precision_score(test_labels, preds_test, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(test_labels, preds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_cm_test = confusion_matrix(test_labels, preds_test)\n",
    "df_cm_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def plot_conf_mat(cm, classes, title, cmap = plt.cm.Greens):\n",
    "    print(cm)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "        horizontalalignment=\"center\",\n",
    "        color=\"black\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label')\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "plot_conf_mat(df_cm_test, classes=['Not Positive Sentiment', 'Positive Sentiment'], \n",
    "                          title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "auc = round(metrics.roc_auc_score(test_labels, preds_test), 4)\n",
    "print('AUC is ' + repr(auc))\n",
    "\n",
    "fpr, tpr, _ = metrics.roc_curve(test_labels, preds_test)\n",
    "\n",
    "plt.title('ROC Curve')\n",
    "plt.plot(fpr, tpr, 'b',\n",
    "label='AUC = %0.2f'% auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.1])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "A Visual Notebook to Using BERT for the First Time.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
