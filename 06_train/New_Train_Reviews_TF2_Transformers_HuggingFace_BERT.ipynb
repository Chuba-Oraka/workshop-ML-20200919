{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip\n",
    "!pip install -q tensorflow==2.2.0-rc2\n",
    "!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.io import TFRecordWriter\n",
    "from tensorflow.io import FixedLenFeature\n",
    "from tensorflow.data import TFRecordDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH=128\n",
    "BATCH_SIZE=32\n",
    "EVAL_BATCH_SIZE=BATCH_SIZE*2\n",
    "EPOCHS=1\n",
    "STEPS_PER_EPOCH=30\n",
    "VALIDATION_STEPS=30\n",
    "CLASSES = [1, 2, 3, 4, 5]\n",
    "                    \n",
    "# XLA is an optimization compiler for tensorflow\n",
    "USE_XLA = True\n",
    "\n",
    "# Mixed precision can help to speed up training time\n",
    "USE_AMP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TFRecords encode and store data\n",
    "# train_dataset = TFRecordDataset(\"./data-tfrecord/train/part-algo-1-amazon_reviews_us_Software_v1_00.tfrecord\")\n",
    "# validation_dataset = TFRecordDataset(\"./data-tfrecord/validation/part-algo-1-amazon_reviews_us_Software_v1_00.tfrecord\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # The tensors you pull into the model MUST have the same name \n",
    "# # as what was encoded in the TFRecord\n",
    "\n",
    "# # FixedLenFeature means that you know the number of tensors associated\n",
    "# # with each label and example.\n",
    "\n",
    "# # For example, there will only be 1 review per example, and as \n",
    "# # a result, sentence is a FixedLenFeature.\n",
    "\n",
    "# feature_spec = {\n",
    "#       \"input_ids\": FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "#       \"input_mask\": FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "#       \"segment_ids\": FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "#       \"label_ids\": FixedLenFeature([], tf.int64),\n",
    "#       \"is_real_example\": FixedLenFeature([], tf.int64),\n",
    "# }\n",
    "\n",
    "# def parse_example(example_proto):\n",
    "#   # Parse the input tf.Example proto using the dictionary above.\n",
    "#   return tf.io.parse_single_example(example_proto, feature_spec)\n",
    "\n",
    "# train_parse_dataset = train_dataset.map(parse_example)\n",
    "# validation_parse_dataset = validation_dataset.map(parse_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_and_label_from_record(record):\n",
    "    x = {\n",
    "        'input_ids': record['input_ids'],\n",
    "        'input_mask': record['input_mask'],\n",
    "        'segment_ids': record['segment_ids']\n",
    "    }\n",
    "    y = record['label_ids']\n",
    "\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_features = {\n",
    "  \"input_ids\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "  \"input_mask\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "  \"segment_ids\": tf.io.FixedLenFeature([MAX_SEQ_LENGTH], tf.int64),\n",
    "  \"label_ids\": tf.io.FixedLenFeature([], tf.int64),\n",
    "  \"is_real_example\": tf.io.FixedLenFeature([], tf.int64),\n",
    "}\n",
    "\n",
    "def decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.io.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "        t = example[name]\n",
    "        if t.dtype == tf.int64:\n",
    "            t = tf.cast(t, tf.int32)\n",
    "        example[name] = t\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_dataset_builder(input_file,\n",
    "                                     is_training,\n",
    "                                     drop_remainder):\n",
    "\n",
    "\n",
    "  # For training, we want a lot of parallel reading and shuffling.\n",
    "  # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "  dataset = tf.data.TFRecordDataset(input_file)\n",
    "  if is_training:\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(buffer_size=10)\n",
    "\n",
    "  dataset = dataset.apply(\n",
    "      tf.data.experimental.map_and_batch(\n",
    "          lambda record: decode_record(record, name_to_features),\n",
    "          batch_size=BATCH_SIZE,\n",
    "          drop_remainder=drop_remainder))\n",
    "\n",
    "  return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "train_data = './data-tfrecord/train'\n",
    "train_data_filenames = glob('{}/*.tfrecord'.format(train_data))\n",
    "\n",
    "print(train_data_filenames)\n",
    "\n",
    "train_dataset = file_based_input_dataset_builder(\n",
    "    train_data_filenames,\n",
    "    is_training=True,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "validation_data = './data-tfrecord/validation'\n",
    "validation_data_filenames = glob('{}/*.tfrecord'.format(validation_data))\n",
    "\n",
    "print(validation_data_filenames)\n",
    "\n",
    "validation_dataset = file_based_input_dataset_builder(\n",
    "    validation_data_filenames,\n",
    "    is_training=False,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "test_data = './data-tfrecord/test'\n",
    "test_data_filenames = glob('{}/*.tfrecord'.format(test_data))\n",
    "\n",
    "print(test_data_filenames)\n",
    "\n",
    "test_dataset = file_based_input_dataset_builder(\n",
    "    test_data_filenames,\n",
    "    is_training=False,\n",
    "    drop_remainder=False).map(select_data_and_label_from_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import *\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.optimizer.set_jit(USE_XLA)\n",
    "tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": USE_AMP})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.configuration_bert import BertConfig\n",
    "config = BertConfig(num_labels=len(CLASSES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', \n",
    "                                                         config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_parse_dataset = train_parse_dataset.shuffle(train_parse_dataset).batch(BATCH_SIZE).repeat(-1)\n",
    "#validation_dataset = validation_dataset.batch(EVAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "\n",
    "if USE_AMP:\n",
    "    # loss scaling is currently required when using mixed precision\n",
    "    optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(opt, 'dynamic')\n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_data=validation_dataset,\n",
    "                    validation_steps=VALIDATION_STEPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./custom_pretrained\n",
    "model.save_pretrained('./custom_pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al ./custom_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ./custom_pretrained/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained('./custom_pretrained/',\n",
    "                                                              id2label={\n",
    "                                                               0: 1,\n",
    "                                                               1: 2,\n",
    "                                                               2: 3,\n",
    "                                                               3: 4,\n",
    "                                                               4: 5\n",
    "                                                              },\n",
    "                                                              label2id={\n",
    "                                                               1: 0,\n",
    "                                                               2: 1,\n",
    "                                                               3: 2,\n",
    "                                                               4: 3,\n",
    "                                                               5: 4\n",
    "                                                              })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TextClassificationPipeline\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inference_pipeline = TextClassificationPipeline(model=loaded_model, \n",
    "                                                tokenizer=tokenizer,\n",
    "                                                framework='tf',\n",
    "                                                device=-1) # -1 is CPU, >= 0 is GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline('This is great!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline('This is wonderful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline('This is OK.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_pipeline('This sucks!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
