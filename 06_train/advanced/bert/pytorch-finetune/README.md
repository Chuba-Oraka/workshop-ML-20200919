# Maximizing NLP model performance with automatic model tuning in Amazon SageMaker

This package shows how to fine-tune NLP models using Hugging Face's PyTorch-Transformers in Amazon SageMaker and apply the built-in automatic model-tuning capability for two NLP datasets: the Microsoft Research Paraphrase Corpus (MRPC) and the Stanford Question Answering Dataset (SQuAD) 1.1.


## How to run the code

The code execution details can be found in the Jupyter notebook `HPO_PyTorch_Transformers.ipynb`

Created in September 2019.

