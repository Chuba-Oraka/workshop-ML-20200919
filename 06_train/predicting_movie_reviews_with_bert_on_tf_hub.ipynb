{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCpvgG0vwXAZ"
   },
   "source": [
    "# Predicting Movie Review Sentiment with BERT on TF Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xiYrZKaHwV81"
   },
   "source": [
    "If you’ve been following Natural Language Processing over the past year, you’ve probably heard of BERT: Bidirectional Encoder Representations from Transformers. It’s a neural network architecture designed by Google researchers that’s totally transformed what’s state-of-the-art for NLP tasks, like text classification, translation, summarization, and question answering.\n",
    "\n",
    "Now that BERT's been added to [TF Hub](https://www.tensorflow.org/hub) as a loadable module, it's easy(ish) to add into existing Tensorflow text pipelines. In an existing pipeline, BERT can replace text embedding layers like ELMO and GloVE. Alternatively, [finetuning](http://wiki.fast.ai/index.php/Fine_tuning) BERT can provide both an accuracy boost and faster training time in many cases.\n",
    "\n",
    "Here, we'll train a model to predict whether a review is positive or negative using BERT in Tensorflow with tf hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version   \r\n",
      "---------------------------------- ----------\r\n",
      "absl-py                            0.9.0     \r\n",
      "alabaster                          0.7.10    \r\n",
      "anaconda-client                    1.6.14    \r\n",
      "anaconda-project                   0.8.2     \r\n",
      "asn1crypto                         0.24.0    \r\n",
      "astor                              0.8.1     \r\n",
      "astroid                            1.6.3     \r\n",
      "astropy                            3.0.2     \r\n",
      "attrs                              18.1.0    \r\n",
      "Automat                            0.3.0     \r\n",
      "autovizwidget                      0.15.0    \r\n",
      "awscli                             1.18.20   \r\n",
      "Babel                              2.5.3     \r\n",
      "backcall                           0.1.0     \r\n",
      "backports.shutil-get-terminal-size 1.0.0     \r\n",
      "bcrypt                             3.1.7     \r\n",
      "beautifulsoup4                     4.6.0     \r\n",
      "bert-tensorflow                    1.0.1     \r\n",
      "bitarray                           0.8.1     \r\n",
      "bkcharts                           0.2       \r\n",
      "blaze                              0.11.3    \r\n",
      "bleach                             2.1.3     \r\n",
      "bokeh                              1.4.0     \r\n",
      "boto                               2.48.0    \r\n",
      "boto3                              1.12.20   \r\n",
      "botocore                           1.15.20   \r\n",
      "Bottleneck                         1.2.1     \r\n",
      "cached-property                    1.5.1     \r\n",
      "certifi                            2019.11.28\r\n",
      "cffi                               1.11.5    \r\n",
      "characteristic                     14.3.0    \r\n",
      "chardet                            3.0.4     \r\n",
      "click                              6.7       \r\n",
      "cloudpickle                        0.5.3     \r\n",
      "clyent                             1.2.2     \r\n",
      "colorama                           0.3.9     \r\n",
      "contextlib2                        0.5.5     \r\n",
      "cryptography                       2.8       \r\n",
      "cycler                             0.10.0    \r\n",
      "Cython                             0.28.4    \r\n",
      "cytoolz                            0.9.0.1   \r\n",
      "dask                               1.2.2     \r\n",
      "datashape                          0.5.4     \r\n",
      "decorator                          4.3.0     \r\n",
      "defusedxml                         0.6.0     \r\n",
      "distributed                        1.28.1    \r\n",
      "docker                             4.2.0     \r\n",
      "docker-compose                     1.25.4    \r\n",
      "dockerpty                          0.4.1     \r\n",
      "docopt                             0.6.2     \r\n",
      "docutils                           0.14      \r\n",
      "entrypoints                        0.2.3     \r\n",
      "enum34                             1.1.9     \r\n",
      "environment-kernels                1.1.1     \r\n",
      "et-xmlfile                         1.0.1     \r\n",
      "fastcache                          1.0.2     \r\n",
      "filelock                           3.0.4     \r\n",
      "Flask                              1.0.2     \r\n",
      "Flask-Cors                         3.0.4     \r\n",
      "gast                               0.2.2     \r\n",
      "gevent                             1.3.0     \r\n",
      "glob2                              0.6       \r\n",
      "gmpy2                              2.0.8     \r\n",
      "google-pasta                       0.1.8     \r\n",
      "greenlet                           0.4.13    \r\n",
      "grpcio                             1.10.1    \r\n",
      "h5py                               2.8.0     \r\n",
      "hdijupyterutils                    0.15.0    \r\n",
      "heapdict                           1.0.0     \r\n",
      "horovod                            0.19.0    \r\n",
      "html5lib                           1.0.1     \r\n",
      "idna                               2.6       \r\n",
      "imageio                            2.3.0     \r\n",
      "imagesize                          1.0.0     \r\n",
      "importlib-metadata                 1.5.0     \r\n",
      "ipykernel                          4.8.2     \r\n",
      "ipyparallel                        6.2.2     \r\n",
      "ipython                            6.4.0     \r\n",
      "ipython-genutils                   0.2.0     \r\n",
      "ipywidgets                         7.4.0     \r\n",
      "isort                              4.3.4     \r\n",
      "itsdangerous                       0.24      \r\n",
      "jdcal                              1.4       \r\n",
      "jedi                               0.12.0    \r\n",
      "Jinja2                             2.10      \r\n",
      "jmespath                           0.9.4     \r\n",
      "joblib                             0.14.1    \r\n",
      "jsonschema                         2.6.0     \r\n",
      "jupyter                            1.0.0     \r\n",
      "jupyter-client                     5.2.3     \r\n",
      "jupyter-console                    5.2.0     \r\n",
      "jupyter-core                       4.4.0     \r\n",
      "jupyterlab                         0.32.1    \r\n",
      "jupyterlab-launcher                0.10.5    \r\n",
      "Keras                              2.2.4     \r\n",
      "Keras-Applications                 1.0.8     \r\n",
      "Keras-Preprocessing                1.1.0     \r\n",
      "kiwisolver                         1.0.1     \r\n",
      "lazy-object-proxy                  1.3.1     \r\n",
      "llvmlite                           0.23.1    \r\n",
      "locket                             0.2.0     \r\n",
      "lxml                               4.2.1     \r\n",
      "Markdown                           3.2.1     \r\n",
      "MarkupSafe                         1.0       \r\n",
      "matplotlib                         3.0.3     \r\n",
      "mccabe                             0.6.1     \r\n",
      "mistune                            0.8.3     \r\n",
      "mkl-fft                            1.0.15    \r\n",
      "mkl-random                         1.1.0     \r\n",
      "mkl-service                        2.3.0     \r\n",
      "mock                               4.0.1     \r\n",
      "more-itertools                     4.1.0     \r\n",
      "mpmath                             1.0.0     \r\n",
      "msgpack                            0.6.0     \r\n",
      "msgpack-python                     0.5.6     \r\n",
      "multipledispatch                   0.5.0     \r\n",
      "nb-conda                           2.2.1     \r\n",
      "nb-conda-kernels                   2.2.2     \r\n",
      "nbconvert                          5.4.1     \r\n",
      "nbformat                           4.4.0     \r\n",
      "networkx                           2.1       \r\n",
      "nltk                               3.3       \r\n",
      "nose                               1.3.7     \r\n",
      "notebook                           5.5.0     \r\n",
      "numba                              0.38.0    \r\n",
      "numexpr                            2.7.1     \r\n",
      "numpy                              1.16.4    \r\n",
      "numpydoc                           0.8.0     \r\n",
      "odo                                0.5.1     \r\n",
      "olefile                            0.45.1    \r\n",
      "opencv-python                      3.4.2.17  \r\n",
      "openpyxl                           2.5.3     \r\n",
      "opt-einsum                         3.1.0     \r\n",
      "packaging                          20.1      \r\n",
      "pandas                             0.24.2    \r\n",
      "pandocfilters                      1.4.2     \r\n",
      "paramiko                           2.7.1     \r\n",
      "parso                              0.2.0     \r\n",
      "partd                              0.3.8     \r\n",
      "path.py                            11.0.1    \r\n",
      "pathlib2                           2.3.2     \r\n",
      "patsy                              0.5.0     \r\n",
      "pep8                               1.7.1     \r\n",
      "pexpect                            4.5.0     \r\n",
      "pickleshare                        0.7.4     \r\n",
      "Pillow                             5.4.1     \r\n",
      "pip                                19.3.1    \r\n",
      "pkginfo                            1.4.2     \r\n",
      "plotly                             4.5.2     \r\n",
      "pluggy                             0.6.0     \r\n",
      "ply                                3.11      \r\n",
      "prompt-toolkit                     1.0.15    \r\n",
      "protobuf                           3.8.0     \r\n",
      "protobuf3-to-dict                  0.1.5     \r\n",
      "psutil                             5.4.5     \r\n",
      "psycopg2                           2.7.5     \r\n",
      "ptyprocess                         0.5.2     \r\n",
      "py                                 1.5.3     \r\n",
      "py4j                               0.10.7    \r\n",
      "pyasn1                             0.4.8     \r\n",
      "pycodestyle                        2.4.0     \r\n",
      "pycosat                            0.6.3     \r\n",
      "pycparser                          2.18      \r\n",
      "pycrypto                           2.6.1     \r\n",
      "pycurl                             7.43.0.2  \r\n",
      "pyflakes                           1.6.0     \r\n",
      "pygal                              2.4.0     \r\n",
      "Pygments                           2.2.0     \r\n",
      "pykerberos                         1.2.1     \r\n",
      "pylint                             1.8.4     \r\n",
      "PyNaCl                             1.3.0     \r\n",
      "pyodbc                             4.0.23    \r\n",
      "pyOpenSSL                          18.0.0    \r\n",
      "pyparsing                          2.2.0     \r\n",
      "PySocks                            1.6.8     \r\n",
      "pyspark                            2.3.2     \r\n",
      "pytest                             3.5.1     \r\n",
      "pytest-arraydiff                   0.2       \r\n",
      "pytest-astropy                     0.3.0     \r\n",
      "pytest-doctestplus                 0.1.3     \r\n",
      "pytest-openfiles                   0.3.0     \r\n",
      "pytest-remotedata                  0.2.1     \r\n",
      "python-dateutil                    2.7.3     \r\n",
      "pytz                               2018.4    \r\n",
      "PyWavelets                         0.5.2     \r\n",
      "PyYAML                             5.3       \r\n",
      "pyzmq                              17.0.0    \r\n",
      "QtAwesome                          0.4.4     \r\n",
      "qtconsole                          4.3.1     \r\n",
      "QtPy                               1.4.1     \r\n",
      "requests                           2.20.0    \r\n",
      "requests-kerberos                  0.12.0    \r\n",
      "retrying                           1.3.3     \r\n",
      "rope                               0.10.7    \r\n",
      "rsa                                3.4.2     \r\n",
      "ruamel-yaml                        0.15.35   \r\n",
      "s3fs                               0.1.5     \r\n",
      "s3transfer                         0.3.3     \r\n",
      "sagemaker                          1.51.3    \r\n",
      "sagemaker-pyspark                  1.2.8     \r\n",
      "scikit-image                       0.13.1    \r\n",
      "scikit-learn                       0.22.1    \r\n",
      "scipy                              1.4.1     \r\n",
      "seaborn                            0.8.1     \r\n",
      "Send2Trash                         1.5.0     \r\n",
      "setuptools                         45.2.0    \r\n",
      "simplegeneric                      0.8.1     \r\n",
      "singledispatch                     3.4.0.3   \r\n",
      "six                                1.11.0    \r\n",
      "smdebug-rulesconfig                0.1.2     \r\n",
      "snowballstemmer                    1.2.1     \r\n",
      "sortedcollections                  0.6.1     \r\n",
      "sortedcontainers                   1.5.10    \r\n",
      "sparkmagic                         0.12.5    \r\n",
      "Sphinx                             1.7.4     \r\n",
      "sphinxcontrib-websupport           1.0.1     \r\n",
      "spyder                             3.2.8     \r\n",
      "SQLAlchemy                         1.2.11    \r\n",
      "statsmodels                        0.9.0     \r\n",
      "sympy                              1.1.1     \r\n",
      "tables                             3.4.3     \r\n",
      "TBB                                0.1       \r\n",
      "tblib                              1.3.2     \r\n",
      "tensorboard                        1.15.0    \r\n",
      "tensorflow                         1.15.2    \r\n",
      "tensorflow-estimator               1.15.1    \r\n",
      "tensorflow-hub                     0.7.0     \r\n",
      "tensorflow-serving-api             1.15.0    \r\n",
      "termcolor                          1.1.0     \r\n",
      "terminado                          0.8.1     \r\n",
      "testpath                           0.3.1     \r\n",
      "texttable                          1.6.2     \r\n",
      "toolz                              0.9.0     \r\n",
      "tornado                            5.0.2     \r\n",
      "traitlets                          4.3.2     \r\n",
      "typing                             3.6.4     \r\n",
      "unicodecsv                         0.14.1    \r\n",
      "urllib3                            1.23      \r\n",
      "wcwidth                            0.1.7     \r\n",
      "webencodings                       0.5.1     \r\n",
      "websocket-client                   0.57.0    \r\n",
      "Werkzeug                           0.14.1    \r\n",
      "wheel                              0.31.1    \r\n",
      "widgetsnbextension                 3.4.2     \r\n",
      "wrapt                              1.11.2    \r\n",
      "xlrd                               1.1.0     \r\n",
      "XlsxWriter                         1.0.4     \r\n",
      "xlwt                               1.3.0     \r\n",
      "zict                               0.1.3     \r\n",
      "zipp                               3.0.0     \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-hub in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (0.7.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-hub) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-hub) (1.16.4)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from tensorflow-hub) (3.8.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow-hub) (45.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hsZvic2YxnTz",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cp5wfXDx5SPH"
   },
   "source": [
    "In addition to the standard libraries we imported above, we'll need to install BERT's python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "jviywGyWyKsA",
    "outputId": "166f3005-d219-404f-b201-2a0b75480360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bert-tensorflow in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (1.0.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (from bert-tensorflow) (1.11.0)\n",
      "\u001b[33mWARNING: You are using pip version 19.3.1; however, version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhbGEfwgdEtw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1467: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVB3eOcjxxm1"
   },
   "source": [
    "Below, we'll set an output directory location to store our model output and checkpoints. This can be a local directory, in which case you'd set OUTPUT_DIR to the name of the directory you'd like to create. If you're running this code in Google's hosted Colab, the directory won't persist after the Colab session ends.\n",
    "\n",
    "Alternatively, if you're a GCP user, you can store output in a GCP bucket. To do that, set a directory name in OUTPUT_DIR and the name of the GCP bucket in the BUCKET field.\n",
    "\n",
    "Set DO_DELETE to rewrite the OUTPUT_DIR if it exists. Otherwise, Tensorflow will load existing model checkpoints from that directory (if they exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "US_EAnICvP7f",
    "outputId": "7780a032-31d4-4794-e6aa-664a5d2ae7dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: ./output *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = './output'#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = True #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False #@param {type:\"boolean\"}\n",
    "BUCKET = 'BUCKET_NAME' #@param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "  OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "  from google.colab import auth\n",
    "  auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "  try:\n",
    "    tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "  except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "    pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pmFYvkylMwXn"
   },
   "source": [
    "#Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MC_w8SRqN0fr"
   },
   "source": [
    "First, let's download the dataset, hosted by Stanford. The code below, which downloads, extracts, and imports the IMDB Large Movie Review Dataset, is borrowed from [this Tensorflow tutorial](https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fom_ff20gyy6"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2abfwdn-g135"
   },
   "outputs": [],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XA8WHJgzhIZf"
   },
   "source": [
    "To keep training fast, we'll take a sample of 5000 train and test examples, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lw_F488eixTV"
   },
   "outputs": [],
   "source": [
    "train = train.sample(5000)\n",
    "test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6801</th>\n",
       "      <td>Unlike endemol USA's two other current game sh...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>This film is a stunning piece that will convin...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.5/10. This movie has absolutely nothing good...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11145</th>\n",
       "      <td>Young, handsome, muscular Joe Buck (Jon Voight...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12417</th>\n",
       "      <td>The master of cheap erotic horror, Rolfe Kanef...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment  polarity\n",
       "6801   Unlike endemol USA's two other current game sh...         4         0\n",
       "24977  This film is a stunning piece that will convin...         9         1\n",
       "254    0.5/10. This movie has absolutely nothing good...         1         0\n",
       "11145  Young, handsome, muscular Joe Buck (Jon Voight...        10         1\n",
       "12417  The master of cheap erotic horror, Rolfe Kanef...         2         0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15617</th>\n",
       "      <td>I had long wanted to watch this romantic drama...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8469</th>\n",
       "      <td>A remarkable film, bringing to the surface all...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>Emilio Estevez actually directed a good movie-...</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2986</th>\n",
       "      <td>This is the funniest movie I have ever seen. H...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20480</th>\n",
       "      <td>A magazine columnist who writes about life on ...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment  polarity\n",
       "15617  I had long wanted to watch this romantic drama...         8         1\n",
       "8469   A remarkable film, bringing to the surface all...        10         1\n",
       "1673   Emilio Estevez actually directed a good movie-...         7         1\n",
       "2986   This is the funniest movie I have ever seen. H...        10         1\n",
       "20480  A magazine columnist who writes about life on ...         8         1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "prRQM8pDi8xI",
    "outputId": "34445cb8-2be0-4379-fdbc-7794091f6049"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'sentiment', 'polarity'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfRnHSz3iSXz"
   },
   "source": [
    "For us, our input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IuMOGwFui4it"
   },
   "outputs": [],
   "source": [
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V399W0rqNJ-Z"
   },
   "source": [
    "#Data Preprocessing\n",
    "We'll need to transform our data into a format BERT understands. This involves two steps. First, we create  `InputExample`'s using the constructor provided in the BERT library.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe. \n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is `text_b` a translation of `text_a`? Is `text_b` an answer to the question asked by `text_a`?). This doesn't apply to our task, so we can leave `text_b` blank.\n",
    "- `label` is the label for our example, i.e. True, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9gEt5SmM6i6"
   },
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SCZWZtKxObjh"
   },
   "source": [
    "Next, we need to preprocess our data so that it matches the data BERT was trained on. For this, we'll need to do a couple of things (but don't worry--this is also included in the Python library):\n",
    "\n",
    "\n",
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the [readme](https://github.com/google-research/bert))\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the [BERT paper](https://arxiv.org/pdf/1810.04805.pdf))\n",
    "\n",
    "Happily, we don't have to worry about most of these details.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMWiDtpyQSoU"
   },
   "source": [
    "To start, we'll need to load a vocabulary file and lowercasing information directly from the BERT tf hub module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IhJSe0QHNG7U",
    "outputId": "20b28cc7-3cb3-4ce6-bfff-a7847ce3bbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z4oFkhpZBDKm"
   },
   "source": [
    "Great--we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "dsBo6RCtQmwx",
    "outputId": "9af8c917-90ec-4fe9-897b-79dc89ca88e1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OEzfFIt6GIc"
   },
   "source": [
    "Using our tokenizer, we'll call `run_classifier.convert_examples_to_features` on our InputExamples to convert them into features BERT understands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1261
    },
    "colab_type": "code",
    "id": "LL5W8gEGRTAf",
    "outputId": "65001dda-155b-48fc-b5fc-1e4cabc8dfbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/run_classifier.py:774: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] unlike end ##em ##ol usa ' s two other current game shows ( deal or no deal and 1 vs . 100 ) , the pacing in this show is way too slow for what is happening on the screen . < br / > < br / > don ##d and 1 vs . 100 can get away with slow pacing because the games can change pace - - or end - - at any moment . there is risk involved in every action the player takes , the rewards are wildly variable , and it is difficult for the players to leave with a significant amount of money . suspense is usually put to good use . < br / > < br / [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] unlike end ##em ##ol usa ' s two other current game shows ( deal or no deal and 1 vs . 100 ) , the pacing in this show is way too slow for what is happening on the screen . < br / > < br / > don ##d and 1 vs . 100 can get away with slow pacing because the games can change pace - - or end - - at any moment . there is risk involved in every action the player takes , the rewards are wildly variable , and it is difficult for the players to leave with a significant amount of money . suspense is usually put to good use . < br / > < br / [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4406 2203 6633 4747 3915 1005 1055 2048 2060 2783 2208 3065 1006 3066 2030 2053 3066 1998 1015 5443 1012 2531 1007 1010 1996 15732 1999 2023 2265 2003 2126 2205 4030 2005 2054 2003 6230 2006 1996 3898 1012 1026 7987 1013 1028 1026 7987 1013 1028 2123 2094 1998 1015 5443 1012 2531 2064 2131 2185 2007 4030 15732 2138 1996 2399 2064 2689 6393 1011 1011 2030 2203 1011 1011 2012 2151 2617 1012 2045 2003 3891 2920 1999 2296 2895 1996 2447 3138 1010 1996 19054 2024 13544 8023 1010 1998 2009 2003 3697 2005 1996 2867 2000 2681 2007 1037 3278 3815 1997 2769 1012 23873 2003 2788 2404 2000 2204 2224 1012 1026 7987 1013 1028 1026 7987 1013 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 4406 2203 6633 4747 3915 1005 1055 2048 2060 2783 2208 3065 1006 3066 2030 2053 3066 1998 1015 5443 1012 2531 1007 1010 1996 15732 1999 2023 2265 2003 2126 2205 4030 2005 2054 2003 6230 2006 1996 3898 1012 1026 7987 1013 1028 1026 7987 1013 1028 2123 2094 1998 1015 5443 1012 2531 2064 2131 2185 2007 4030 15732 2138 1996 2399 2064 2689 6393 1011 1011 2030 2203 1011 1011 2012 2151 2617 1012 2045 2003 3891 2920 1999 2296 2895 1996 2447 3138 1010 1996 19054 2024 13544 8023 1010 1998 2009 2003 3697 2005 1996 2867 2000 2681 2007 1037 3278 3815 1997 2769 1012 23873 2003 2788 2404 2000 2204 2224 1012 1026 7987 1013 1028 1026 7987 1013 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this film is a stunning piece that will convince even the most skeptical viewer that gerard de ##par ##die ##u is one of the finest film actors of the last 50 years . his performance shocks , entertain ##s , disgust ##s and charms you while leaving you breathless . this film was shot in the very early days of his film career and is very raw , but still is able to convey the mastery of de ##par ##die ##u . a must - see for any de ##par ##die ##u fan and by far his best early work . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this film is a stunning piece that will convince even the most skeptical viewer that gerard de ##par ##die ##u is one of the finest film actors of the last 50 years . his performance shocks , entertain ##s , disgust ##s and charms you while leaving you breathless . this film was shot in the very early days of his film career and is very raw , but still is able to convey the mastery of de ##par ##die ##u . a must - see for any de ##par ##die ##u fan and by far his best early work . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2143 2003 1037 14726 3538 2008 2097 8054 2130 1996 2087 18386 13972 2008 11063 2139 19362 10265 2226 2003 2028 1997 1996 10418 2143 5889 1997 1996 2197 2753 2086 1012 2010 2836 28215 1010 20432 2015 1010 12721 2015 1998 24044 2017 2096 2975 2017 16701 1012 2023 2143 2001 2915 1999 1996 2200 2220 2420 1997 2010 2143 2476 1998 2003 2200 6315 1010 2021 2145 2003 2583 2000 16636 1996 26364 1997 2139 19362 10265 2226 1012 1037 2442 1011 2156 2005 2151 2139 19362 10265 2226 5470 1998 2011 2521 2010 2190 2220 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2143 2003 1037 14726 3538 2008 2097 8054 2130 1996 2087 18386 13972 2008 11063 2139 19362 10265 2226 2003 2028 1997 1996 10418 2143 5889 1997 1996 2197 2753 2086 1012 2010 2836 28215 1010 20432 2015 1010 12721 2015 1998 24044 2017 2096 2975 2017 16701 1012 2023 2143 2001 2915 1999 1996 2200 2220 2420 1997 2010 2143 2476 1998 2003 2200 6315 1010 2021 2145 2003 2583 2000 16636 1996 26364 1997 2139 19362 10265 2226 1012 1037 2442 1011 2156 2005 2151 2139 19362 10265 2226 5470 1998 2011 2521 2010 2190 2220 2147 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 0 . 5 / 10 . this movie has absolutely nothing good about it . the acting is among the worst i have ever seen , what is really amazing is that everyone is awful , not just a few here and there , everyone . the direction is a joke , the low budget is hopeless ##ly evident , the score is awful , i wouldn ' t say the movie was edited , brutally chopped would be a more appropriate phrase . it combines serial killings , voodoo and tar ##ot cards . dumb . dumb . dumb . it is not scary at all , the special effects are hopeless ##ly lame . laugh ##ably bad throughout . the writing was app ##all [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] 0 . 5 / 10 . this movie has absolutely nothing good about it . the acting is among the worst i have ever seen , what is really amazing is that everyone is awful , not just a few here and there , everyone . the direction is a joke , the low budget is hopeless ##ly evident , the score is awful , i wouldn ' t say the movie was edited , brutally chopped would be a more appropriate phrase . it combines serial killings , voodoo and tar ##ot cards . dumb . dumb . dumb . it is not scary at all , the special effects are hopeless ##ly lame . laugh ##ably bad throughout . the writing was app ##all [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1014 1012 1019 1013 2184 1012 2023 3185 2038 7078 2498 2204 2055 2009 1012 1996 3772 2003 2426 1996 5409 1045 2031 2412 2464 1010 2054 2003 2428 6429 2003 2008 3071 2003 9643 1010 2025 2074 1037 2261 2182 1998 2045 1010 3071 1012 1996 3257 2003 1037 8257 1010 1996 2659 5166 2003 20625 2135 10358 1010 1996 3556 2003 9643 1010 1045 2876 1005 1056 2360 1996 3185 2001 5493 1010 23197 24881 2052 2022 1037 2062 6413 7655 1012 2009 13585 7642 16431 1010 21768 1998 16985 4140 5329 1012 12873 1012 12873 1012 12873 1012 2009 2003 2025 12459 2012 2035 1010 1996 2569 3896 2024 20625 2135 20342 1012 4756 8231 2919 2802 1012 1996 3015 2001 10439 8095 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1014 1012 1019 1013 2184 1012 2023 3185 2038 7078 2498 2204 2055 2009 1012 1996 3772 2003 2426 1996 5409 1045 2031 2412 2464 1010 2054 2003 2428 6429 2003 2008 3071 2003 9643 1010 2025 2074 1037 2261 2182 1998 2045 1010 3071 1012 1996 3257 2003 1037 8257 1010 1996 2659 5166 2003 20625 2135 10358 1010 1996 3556 2003 9643 1010 1045 2876 1005 1056 2360 1996 3185 2001 5493 1010 23197 24881 2052 2022 1037 2062 6413 7655 1012 2009 13585 7642 16431 1010 21768 1998 16985 4140 5329 1012 12873 1012 12873 1012 12873 1012 2009 2003 2025 12459 2012 2035 1010 1996 2569 3896 2024 20625 2135 20342 1012 4756 8231 2919 2802 1012 1996 3015 2001 10439 8095 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] young , handsome , muscular joe buck ( jon vo ##ight ) moves from texas to new york thinking he ' ll make a living by being a stud . he gets there and finds out quickly that it isn ' t going to be easy - - he goes through one de ##grad ##ing experience after another . at the end of his rope he hooks up with crippled , sl ##ea ##zy rats ##o ri ##zzo ( dustin hoffman ) . together they try to survive and get out of the city and move to florida . but will they make it ? < br / > < br / > very dark , disturbing yet fascinating movie . director john sc ##hel ##sing [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] young , handsome , muscular joe buck ( jon vo ##ight ) moves from texas to new york thinking he ' ll make a living by being a stud . he gets there and finds out quickly that it isn ' t going to be easy - - he goes through one de ##grad ##ing experience after another . at the end of his rope he hooks up with crippled , sl ##ea ##zy rats ##o ri ##zzo ( dustin hoffman ) . together they try to survive and get out of the city and move to florida . but will they make it ? < br / > < br / > very dark , disturbing yet fascinating movie . director john sc ##hel ##sing [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2402 1010 8502 1010 13472 3533 10131 1006 6285 29536 18743 1007 5829 2013 3146 2000 2047 2259 3241 2002 1005 2222 2191 1037 2542 2011 2108 1037 16054 1012 2002 4152 2045 1998 4858 2041 2855 2008 2009 3475 1005 1056 2183 2000 2022 3733 1011 1011 2002 3632 2083 2028 2139 16307 2075 3325 2044 2178 1012 2012 1996 2203 1997 2010 8164 2002 18008 2039 2007 24433 1010 22889 5243 9096 11432 2080 15544 12036 1006 24337 15107 1007 1012 2362 2027 3046 2000 5788 1998 2131 2041 1997 1996 2103 1998 2693 2000 3516 1012 2021 2097 2027 2191 2009 1029 1026 7987 1013 1028 1026 7987 1013 1028 2200 2601 1010 14888 2664 17160 3185 1012 2472 2198 8040 16001 7741 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2402 1010 8502 1010 13472 3533 10131 1006 6285 29536 18743 1007 5829 2013 3146 2000 2047 2259 3241 2002 1005 2222 2191 1037 2542 2011 2108 1037 16054 1012 2002 4152 2045 1998 4858 2041 2855 2008 2009 3475 1005 1056 2183 2000 2022 3733 1011 1011 2002 3632 2083 2028 2139 16307 2075 3325 2044 2178 1012 2012 1996 2203 1997 2010 8164 2002 18008 2039 2007 24433 1010 22889 5243 9096 11432 2080 15544 12036 1006 24337 15107 1007 1012 2362 2027 3046 2000 5788 1998 2131 2041 1997 1996 2103 1998 2693 2000 3516 1012 2021 2097 2027 2191 2009 1029 1026 7987 1013 1028 1026 7987 1013 1028 2200 2601 1010 14888 2664 17160 3185 1012 2472 2198 8040 16001 7741 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the master of cheap erotic horror , rolf ##e kane ##fs ##ky , finally makes a movie that doesn ' t go straight to the playboy channel . \" the ha ##zing \" borrow ##s heavily from everything that came before it from nightmare on elm street to evil dead , but still manages to do it with enough humor to make it watch ##able . . . just barely . the characters are cardboard , the dialogue is wooden , the story is paper - thin and the actors couldn ' t act their way out of a grocery bag . put that all together and you have a pulp ##y ball of mu ##lch for a movie . sometimes , when i ' m [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] the master of cheap erotic horror , rolf ##e kane ##fs ##ky , finally makes a movie that doesn ' t go straight to the playboy channel . \" the ha ##zing \" borrow ##s heavily from everything that came before it from nightmare on elm street to evil dead , but still manages to do it with enough humor to make it watch ##able . . . just barely . the characters are cardboard , the dialogue is wooden , the story is paper - thin and the actors couldn ' t act their way out of a grocery bag . put that all together and you have a pulp ##y ball of mu ##lch for a movie . sometimes , when i ' m [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 3040 1997 10036 14253 5469 1010 23381 2063 8472 10343 4801 1010 2633 3084 1037 3185 2008 2987 1005 1056 2175 3442 2000 1996 18286 3149 1012 1000 1996 5292 6774 1000 17781 2015 4600 2013 2673 2008 2234 2077 2009 2013 10103 2006 17709 2395 2000 4763 2757 1010 2021 2145 9020 2000 2079 2009 2007 2438 8562 2000 2191 2009 3422 3085 1012 1012 1012 2074 4510 1012 1996 3494 2024 19747 1010 1996 7982 2003 4799 1010 1996 2466 2003 3259 1011 4857 1998 1996 5889 2481 1005 1056 2552 2037 2126 2041 1997 1037 13025 4524 1012 2404 2008 2035 2362 1998 2017 2031 1037 16016 2100 3608 1997 14163 29358 2005 1037 3185 1012 2823 1010 2043 1045 1005 1049 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1996 3040 1997 10036 14253 5469 1010 23381 2063 8472 10343 4801 1010 2633 3084 1037 3185 2008 2987 1005 1056 2175 3442 2000 1996 18286 3149 1012 1000 1996 5292 6774 1000 17781 2015 4600 2013 2673 2008 2234 2077 2009 2013 10103 2006 17709 2395 2000 4763 2757 1010 2021 2145 9020 2000 2079 2009 2007 2438 8562 2000 2191 2009 3422 3085 1012 1012 1012 2074 4510 1012 1996 3494 2024 19747 1010 1996 7982 2003 4799 1010 1996 2466 2003 3259 1011 4857 1998 1996 5889 2481 1005 1056 2552 2037 2126 2041 1997 1037 13025 4524 1012 2404 2008 2035 2362 1998 2017 2031 1037 16016 2100 3608 1997 14163 29358 2005 1037 3185 1012 2823 1010 2043 1045 1005 1049 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i had long wanted to watch this romantic drama ( with a wwii setting ) and , now that i have , all i can say is that it ' s a ve ##rita ##ble masterpiece of russian cinema ! < br / > < br / > soviet films are known for their over ##ze ##alo ##us prop ##aga ##ndi ##st approach but , thankfully , this one ' s free of such emphasis - with the interest firmly on the central tragic romance between a promising artist and a viva ##cious girl , doomed by the outbreak of war for which he gladly volunteers but from which he ' ll never return . the girl ( a remarkable performance from ta ##tya ##na sam [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i had long wanted to watch this romantic drama ( with a wwii setting ) and , now that i have , all i can say is that it ' s a ve ##rita ##ble masterpiece of russian cinema ! < br / > < br / > soviet films are known for their over ##ze ##alo ##us prop ##aga ##ndi ##st approach but , thankfully , this one ' s free of such emphasis - with the interest firmly on the central tragic romance between a promising artist and a viva ##cious girl , doomed by the outbreak of war for which he gladly volunteers but from which he ' ll never return . the girl ( a remarkable performance from ta ##tya ##na sam [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2018 2146 2359 2000 3422 2023 6298 3689 1006 2007 1037 25755 4292 1007 1998 1010 2085 2008 1045 2031 1010 2035 1045 2064 2360 2003 2008 2009 1005 1055 1037 2310 17728 3468 17743 1997 2845 5988 999 1026 7987 1013 1028 1026 7987 1013 1028 3354 3152 2024 2124 2005 2037 2058 4371 23067 2271 17678 16098 16089 3367 3921 2021 1010 16047 1010 2023 2028 1005 1055 2489 1997 2107 7902 1011 2007 1996 3037 7933 2006 1996 2430 13800 7472 2090 1037 10015 3063 1998 1037 20022 18436 2611 1010 20076 2011 1996 8293 1997 2162 2005 2029 2002 24986 7314 2021 2013 2029 2002 1005 2222 2196 2709 1012 1996 2611 1006 1037 9487 2836 2013 11937 21426 2532 3520 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2018 2146 2359 2000 3422 2023 6298 3689 1006 2007 1037 25755 4292 1007 1998 1010 2085 2008 1045 2031 1010 2035 1045 2064 2360 2003 2008 2009 1005 1055 1037 2310 17728 3468 17743 1997 2845 5988 999 1026 7987 1013 1028 1026 7987 1013 1028 3354 3152 2024 2124 2005 2037 2058 4371 23067 2271 17678 16098 16089 3367 3921 2021 1010 16047 1010 2023 2028 1005 1055 2489 1997 2107 7902 1011 2007 1996 3037 7933 2006 1996 2430 13800 7472 2090 1037 10015 3063 1998 1037 20022 18436 2611 1010 20076 2011 1996 8293 1997 2162 2005 2029 2002 24986 7314 2021 2013 2029 2002 1005 2222 2196 2709 1012 1996 2611 1006 1037 9487 2836 2013 11937 21426 2532 3520 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a remarkable film , bringing to the surface all sorts of feelings i had when i was much , much younger . i loved it , and the elton john music . i remember seeing in in the movies when i was a kid , and for some reason ( limited release ? ) i ' ve never known anyone else who saw this film when it was released . < br / > < br / > the dreams it inspired in me from decades ago have never left me , and seeing the film again recently brought it all rushing back , i confess , however , that my kids ( in their 20 ' s ) have not experienced a similar emotional rush [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a remarkable film , bringing to the surface all sorts of feelings i had when i was much , much younger . i loved it , and the elton john music . i remember seeing in in the movies when i was a kid , and for some reason ( limited release ? ) i ' ve never known anyone else who saw this film when it was released . < br / > < br / > the dreams it inspired in me from decades ago have never left me , and seeing the film again recently brought it all rushing back , i confess , however , that my kids ( in their 20 ' s ) have not experienced a similar emotional rush [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 9487 2143 1010 5026 2000 1996 3302 2035 11901 1997 5346 1045 2018 2043 1045 2001 2172 1010 2172 3920 1012 1045 3866 2009 1010 1998 1996 19127 2198 2189 1012 1045 3342 3773 1999 1999 1996 5691 2043 1045 2001 1037 4845 1010 1998 2005 2070 3114 1006 3132 2713 1029 1007 1045 1005 2310 2196 2124 3087 2842 2040 2387 2023 2143 2043 2009 2001 2207 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 5544 2009 4427 1999 2033 2013 5109 3283 2031 2196 2187 2033 1010 1998 3773 1996 2143 2153 3728 2716 2009 2035 8375 2067 1010 1045 18766 1010 2174 1010 2008 2026 4268 1006 1999 2037 2322 1005 1055 1007 2031 2025 5281 1037 2714 6832 5481 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 9487 2143 1010 5026 2000 1996 3302 2035 11901 1997 5346 1045 2018 2043 1045 2001 2172 1010 2172 3920 1012 1045 3866 2009 1010 1998 1996 19127 2198 2189 1012 1045 3342 3773 1999 1999 1996 5691 2043 1045 2001 1037 4845 1010 1998 2005 2070 3114 1006 3132 2713 1029 1007 1045 1005 2310 2196 2124 3087 2842 2040 2387 2023 2143 2043 2009 2001 2207 1012 1026 7987 1013 1028 1026 7987 1013 1028 1996 5544 2009 4427 1999 2033 2013 5109 3283 2031 2196 2187 2033 1010 1998 3773 1996 2143 2153 3728 2716 2009 2035 8375 2067 1010 1045 18766 1010 2174 1010 2008 2026 4268 1006 1999 2037 2322 1005 1055 1007 2031 2025 5281 1037 2714 6832 5481 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] emilio este ##vez actually directed a good movie - - who would ##a thought ? i sat through two previous films este ##vez directed - - \" wisdom \" ( with then girlfriend demi moore ) and \" men at work \" ( with brother charlie sheen ) . they are lou ##sy films - - - badly acted , directed , stupid and offensive . este ##vez is a good actor but lou ##sy as a director . i turned this on in pure curious ##ity - - it has a great cast and i had nothing else to do . damned if it didn ' t pull me in . < br / > < br / > it concerns este ##vez coming home [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] emilio este ##vez actually directed a good movie - - who would ##a thought ? i sat through two previous films este ##vez directed - - \" wisdom \" ( with then girlfriend demi moore ) and \" men at work \" ( with brother charlie sheen ) . they are lou ##sy films - - - badly acted , directed , stupid and offensive . este ##vez is a good actor but lou ##sy as a director . i turned this on in pure curious ##ity - - it has a great cast and i had nothing else to do . damned if it didn ' t pull me in . < br / > < br / > it concerns este ##vez coming home [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 18644 28517 26132 2941 2856 1037 2204 3185 1011 1011 2040 2052 2050 2245 1029 1045 2938 2083 2048 3025 3152 28517 26132 2856 1011 1011 1000 9866 1000 1006 2007 2059 6513 27668 5405 1007 1998 1000 2273 2012 2147 1000 1006 2007 2567 4918 20682 1007 1012 2027 2024 10223 6508 3152 1011 1011 1011 6649 6051 1010 2856 1010 5236 1998 5805 1012 28517 26132 2003 1037 2204 3364 2021 10223 6508 2004 1037 2472 1012 1045 2357 2023 2006 1999 5760 8025 3012 1011 1011 2009 2038 1037 2307 3459 1998 1045 2018 2498 2842 2000 2079 1012 9636 2065 2009 2134 1005 1056 4139 2033 1999 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 5936 28517 26132 2746 2188 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 18644 28517 26132 2941 2856 1037 2204 3185 1011 1011 2040 2052 2050 2245 1029 1045 2938 2083 2048 3025 3152 28517 26132 2856 1011 1011 1000 9866 1000 1006 2007 2059 6513 27668 5405 1007 1998 1000 2273 2012 2147 1000 1006 2007 2567 4918 20682 1007 1012 2027 2024 10223 6508 3152 1011 1011 1011 6649 6051 1010 2856 1010 5236 1998 5805 1012 28517 26132 2003 1037 2204 3364 2021 10223 6508 2004 1037 2472 1012 1045 2357 2023 2006 1999 5760 8025 3012 1011 1011 2009 2038 1037 2307 3459 1998 1045 2018 2498 2842 2000 2079 1012 9636 2065 2009 2134 1005 1056 4139 2033 1999 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 5936 28517 26132 2746 2188 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this is the fun ##nies ##t movie i have ever seen . however , i have laughed harder at plenty of movies . this is because best in show ' s brilliance lies not in slap ##stick or one - liner ##s , but in sophisticated and layered verbal wit . the improvised dialogue is is so quick that you end up laughing not at each individual joke , but only until after several jokes build on one another , each di ##sar ##ming your senses until the jokes climax and you can ' t help letting loose . < br / > < br / > it ' s a well - shot film , but what makes it extraordinary is the acting . i [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] this is the fun ##nies ##t movie i have ever seen . however , i have laughed harder at plenty of movies . this is because best in show ' s brilliance lies not in slap ##stick or one - liner ##s , but in sophisticated and layered verbal wit . the improvised dialogue is is so quick that you end up laughing not at each individual joke , but only until after several jokes build on one another , each di ##sar ##ming your senses until the jokes climax and you can ' t help letting loose . < br / > < br / > it ' s a well - shot film , but what makes it extraordinary is the acting . i [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2003 1996 4569 15580 2102 3185 1045 2031 2412 2464 1012 2174 1010 1045 2031 4191 6211 2012 7564 1997 5691 1012 2023 2003 2138 2190 1999 2265 1005 1055 28850 3658 2025 1999 14308 21354 2030 2028 1011 11197 2015 1010 2021 1999 12138 1998 21323 12064 15966 1012 1996 19641 7982 2003 2003 2061 4248 2008 2017 2203 2039 5870 2025 2012 2169 3265 8257 1010 2021 2069 2127 2044 2195 13198 3857 2006 2028 2178 1010 2169 4487 10286 6562 2115 9456 2127 1996 13198 14463 1998 2017 2064 1005 1056 2393 5599 6065 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 1005 1055 1037 2092 1011 2915 2143 1010 2021 2054 3084 2009 9313 2003 1996 3772 1012 1045 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2023 2003 1996 4569 15580 2102 3185 1045 2031 2412 2464 1012 2174 1010 1045 2031 4191 6211 2012 7564 1997 5691 1012 2023 2003 2138 2190 1999 2265 1005 1055 28850 3658 2025 1999 14308 21354 2030 2028 1011 11197 2015 1010 2021 1999 12138 1998 21323 12064 15966 1012 1996 19641 7982 2003 2003 2061 4248 2008 2017 2203 2039 5870 2025 2012 2169 3265 8257 1010 2021 2069 2127 2044 2195 13198 3857 2006 2028 2178 1010 2169 4487 10286 6562 2115 9456 2127 1996 13198 14463 1998 2017 2064 1005 1056 2393 5599 6065 1012 1026 7987 1013 1028 1026 7987 1013 1028 2009 1005 1055 1037 2092 1011 2915 2143 1010 2021 2054 3084 2009 9313 2003 1996 3772 1012 1045 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a magazine columnist who writes about life on her farm house when in fact she lives in a ny apartment must come up with a plan when she learns that her publisher and a war hero will spend christmas with her . after a slow start , it turns into an entertaining little screw ##ball comedy , thanks to a fine cast . in a big departure from her previous role as a femme fatal ##e in \" double ind ##em ##nity , \" stan ##wy ##ck displays a nice comedic flair . morgan is smooth as the af ##fa ##ble war hero while greens ##tree ##t is well cast as the publisher . however , sa ##kal ##l steals the film as a chef trying [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a magazine columnist who writes about life on her farm house when in fact she lives in a ny apartment must come up with a plan when she learns that her publisher and a war hero will spend christmas with her . after a slow start , it turns into an entertaining little screw ##ball comedy , thanks to a fine cast . in a big departure from her previous role as a femme fatal ##e in \" double ind ##em ##nity , \" stan ##wy ##ck displays a nice comedic flair . morgan is smooth as the af ##fa ##ble war hero while greens ##tree ##t is well cast as the publisher . however , sa ##kal ##l steals the film as a chef trying [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 2932 13317 2040 7009 2055 2166 2006 2014 3888 2160 2043 1999 2755 2016 3268 1999 1037 6396 4545 2442 2272 2039 2007 1037 2933 2043 2016 10229 2008 2014 6674 1998 1037 2162 5394 2097 5247 4234 2007 2014 1012 2044 1037 4030 2707 1010 2009 4332 2046 2019 14036 2210 11224 7384 4038 1010 4283 2000 1037 2986 3459 1012 1999 1037 2502 6712 2013 2014 3025 2535 2004 1037 26893 10611 2063 1999 1000 3313 27427 6633 22758 1010 1000 9761 18418 3600 8834 1037 3835 21699 22012 1012 5253 2003 5744 2004 1996 21358 7011 3468 2162 5394 2096 15505 13334 2102 2003 2092 3459 2004 1996 6674 1012 2174 1010 7842 12902 2140 15539 1996 2143 2004 1037 10026 2667 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 2932 13317 2040 7009 2055 2166 2006 2014 3888 2160 2043 1999 2755 2016 3268 1999 1037 6396 4545 2442 2272 2039 2007 1037 2933 2043 2016 10229 2008 2014 6674 1998 1037 2162 5394 2097 5247 4234 2007 2014 1012 2044 1037 4030 2707 1010 2009 4332 2046 2019 14036 2210 11224 7384 4038 1010 4283 2000 1037 2986 3459 1012 1999 1037 2502 6712 2013 2014 3025 2535 2004 1037 26893 10611 2063 1999 1000 3313 27427 6633 22758 1010 1000 9761 18418 3600 8834 1037 3835 21699 22012 1012 5253 2003 5744 2004 1996 21358 7011 3468 2162 5394 2096 15505 13334 2102 2003 2092 3459 2004 1996 6674 1012 2174 1010 7842 12902 2140 15539 1996 2143 2004 1037 10026 2667 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ccp5trMwRtmr"
   },
   "source": [
    "#Creating a model\n",
    "\n",
    "Now that we've prepared our data, let's focus on building a model. `create_model` does just this below. First, it loads the BERT tf hub module again (this time to extract the computation graph). Next, it creates a single new layer that will be trained to adapt BERT to our sentiment task (i.e. classifying whether a movie review is positive or negative). This strategy of using a mostly trained model is called [fine-tuning](http://wiki.fast.ai/index.php/Fine_tuning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6o2a5ZIvRcJq"
   },
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qpE0ZIDOCQzE"
   },
   "source": [
    "Next we'll wrap our model function in a `model_fn_builder` function that adapts our model to work for training, evaluation, and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnH-AnOQ9KKW"
   },
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OjwJ4bTeWXD8"
   },
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 3.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "emHf9GhfWBZ_"
   },
   "outputs": [],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_features) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oEJldMr3WYZa"
   },
   "outputs": [],
   "source": [
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "colab_type": "code",
    "id": "q_WebpS1X97v",
    "outputId": "1648932a-7391-49d3-8af7-52d514e226e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6889dcbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': './output', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 500, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f6889dcbe10>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model_fn = model_fn_builder(\n",
    "    num_labels=len(label_list),\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_steps=num_train_steps,\n",
    "    num_warmup_steps=num_warmup_steps)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "    model_fn=model_fn,\n",
    "    config=run_config,\n",
    "    params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NOO3RfG1DYLo"
   },
   "source": [
    "Next we create an input builder function that takes our training feature set (`train_features`) and produces a generator. This is a pretty standard design pattern for working with Tensorflow [Estimators](https://www.tensorflow.org/guide/estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Pv2bAlOX_-K"
   },
   "outputs": [],
   "source": [
    "# Create an input function for training. drop_remainder = True for using TPUs.\n",
    "train_input_fn = bert.run_classifier.input_fn_builder(\n",
    "    features=train_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6Nukby2EB6-"
   },
   "source": [
    "Now we train our model! For me, using a Colab notebook running on Google's GPUs, my training time was about 14 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "nucD4gluYJmK",
    "outputId": "5d728e72-4631-42bf-c48d-3f51d4b968ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-19-ca03218f28a6>:34: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/bert/optimization.py:70: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/contrib/metrics/python/metrics/classification.py:162: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into ./output/model.ckpt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 0 into ./output/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.7039824, step = 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.7039824, step = 1\n"
     ]
    }
   ],
   "source": [
    "print(f'Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CmbLTVniARy3"
   },
   "source": [
    "Now let's use our test data to see how well our model did:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JIhejfpyJ8Bx"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5cd28ff7efaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m test_input_fn = bert.run_classifier.input_fn_builder(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mis_training\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     drop_remainder=False)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bert' is not defined"
     ]
    }
   ],
   "source": [
    "test_input_fn = run_classifier.input_fn_builder(\n",
    "    features=test_features,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 445
    },
    "colab_type": "code",
    "id": "PPVEXhNjYXC-",
    "outputId": "dd5482cd-c558-465f-c854-ec11a0175316"
   },
   "outputs": [],
   "source": [
    "estimator.evaluate(input_fn=test_input_fn, steps=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ueKsULteiz1B"
   },
   "source": [
    "Now let's write code to make predictions on new sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OsrbTD2EJTVl"
   },
   "outputs": [],
   "source": [
    "def getPrediction(in_sentences):\n",
    "    labels = [\"Negative\", \"Positive\"]\n",
    "    input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] # here, \"\" is just a dummy label\n",
    "    input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "    predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "    predictions = estimator.predict(predict_input_fn)\n",
    "    return [(sentence, prediction['probabilities'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-thbodgih_VJ"
   },
   "outputs": [],
   "source": [
    "pred_sentences = [\n",
    "  \"That movie was absolutely awful\",\n",
    "  \"The acting was a bit lacking\",\n",
    "  \"The film was creative and surprising\",\n",
    "  \"Absolutely fantastic!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 649
    },
    "colab_type": "code",
    "id": "QrZmvZySKQTm",
    "outputId": "3891fafb-a460-4eb8-fa6c-335a5bbc10e5"
   },
   "outputs": [],
   "source": [
    "predictions = getPrediction(pred_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MXkRiEBUqN3n"
   },
   "source": [
    "Voila! We have a sentiment classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "ERkTE8-7oQLZ",
    "outputId": "26c33224-dc2c-4b3d-f7b4-ac3ef0a58b27"
   },
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Predicting Movie Reviews with BERT on TF Hub.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
