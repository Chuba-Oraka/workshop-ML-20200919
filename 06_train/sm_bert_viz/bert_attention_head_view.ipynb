{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SageMaker debugger to monitor attentions in BERT model training\n",
    "\n",
    "[BERT](https://arxiv.org/abs/1810.04805) is a deep bidirectional transformer model that achieves state-of the art results in NLP tasks like question answering, text classification and others.\n",
    "In this notebook we will use [GluonNLP](https://gluon-nlp.mxnet.io/) to finetune a pretrained BERT model on the [Stanford Question and Answering dataset](https://web.stanford.edu/class/cs224n/reports/default/15848195.pdf) and we will use [SageMaker Debugger](https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html) to monitor model training in real-time. \n",
    "\n",
    "The paper [Visualizing Attention in Transformer-Based Language Representation Models [1]](https://arxiv.org/pdf/1904.02679.pdf) shows that plotting attentions and individual neurons in the query and key vectors can help to identify causes of incorrect model predictions.\n",
    "With SageMaker Debugger we can easily retrieve those tensors and plot them in real-time as training progresses which may help to understand what the model is learning. \n",
    "\n",
    "The animation below shows the attention scores of the first 20 input tokens for the first 10 iterations in the training.\n",
    "\n",
    "<img src='images/attention_scores.gif' width='350' /> \n",
    "Fig. 1: Attention scores of the first head in the 7th layer \n",
    "\n",
    "[1] *Visualizing Attention in Transformer-Based Language Representation Models*:  Jesse Vig, 2019, 1904.02679, arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smdebug in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (0.7.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from smdebug) (3.11.3)\n",
      "Requirement already satisfied: boto3>=1.10.32 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from smdebug) (1.12.24)\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from smdebug) (1.16.4)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from smdebug) (20.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.6.0->smdebug) (39.1.0)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.6.0->smdebug) (1.11.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.24 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (1.15.24)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (0.9.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from packaging->smdebug) (2.2.0)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3>=1.10.32->smdebug) (1.23)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3>=1.10.32->smdebug) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3>=1.10.32->smdebug) (2.7.3)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker training\n",
    "The following code defines the SageMaker Estimator. The entry point script [train.py](entry_point/train.py) defines the model training. It downloads a BERT model from the GluonNLP model zoo and finetunes the model on the Stanford Question Answering dataset. The training script follows the official GluonNLP [example](https://github.com/dmlc/gluon-nlp/blob/v0.8.x/scripts/bert/finetune_squad.py) on finetuning BERT.\n",
    "\n",
    "For demonstration purposes we will train only on a subset of the data (`train_dataset_size`) and perform evaluation on a single batch (`val_dataset_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.debugger import DebuggerHookConfig, CollectionConfig\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "LOCATION_IN_BUCKET = 'smdebug-output'\n",
    "s3_bucket_for_tensors = 's3://{BUCKET_NAME}/{LOCATION_IN_BUCKET}'.format(BUCKET_NAME=BUCKET_NAME, LOCATION_IN_BUCKET=LOCATION_IN_BUCKET)\n",
    "\n",
    "mxnet_estimator = MXNet(entry_point='train.py',\n",
    "                            source_dir='entry_point',\n",
    "                            role=role,\n",
    "                            train_instance_type='ml.p3.8xlarge',\n",
    "                            train_instance_count=1,\n",
    "                            framework_version='1.6.0',\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 3, \n",
    "                                               'batch_size': 16,\n",
    "                                               'learning_rate': 5e-5,\n",
    "                                               'train_dataset_size': 1024,\n",
    "                                               'val_dataset_size': 16},\n",
    "                            debugger_hook_config = DebuggerHookConfig(\n",
    "                              s3_output_path=s3_bucket_for_tensors,  \n",
    "                              collection_configs=[\n",
    "                                CollectionConfig(\n",
    "                                    name=\"all\",\n",
    "                                    parameters={\"include_regex\": \n",
    "                                                \".*multiheadattentioncell0_output_1|.*key_output|.*query_output\",\n",
    "                                                \"train.save_steps\": \"0\",\n",
    "                                                \"eval.save_interval\": \"1\"}\n",
    "                                    )\n",
    "                                 ]\n",
    "                               )\n",
    "                            )                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Debugger will monitor by default collections such as gradients, weights and biases. The default `save_interval` is 100 steps. A step presents the work done by the training job for one batch (i.e. forward and backward pass). \n",
    "\n",
    "In this example we are also interested in attention scores, query and key output tensors. We can emit them by just defining a new [collection](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection). In this example we call the collection `all` and define the corresponding regex. We save every iteration during validation phase (`eval.save_interval`) and only the first iteration during training phase (`train.save_steps`).\n",
    "\n",
    "\n",
    "We also add the following lines in the validation loop to record the string representation of input tokens:\n",
    "```python\n",
    "if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):  \n",
    "   hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the S3 location of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors are stored in: s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output\n"
     ]
    }
   ],
   "source": [
    "path = mxnet_estimator.latest_job_debugger_artifacts_path()\n",
    "print('Tensors are stored in: {}'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training job name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: mxnet-training-2020-04-14-21-50-48-832\n"
     ]
    }
   ],
   "source": [
    "job_name = mxnet_estimator.latest_training_job.name\n",
    "print('Training job name: {}'.format(job_name))\n",
    "\n",
    "client = mxnet_estimator.sagemaker_session.sagemaker_client\n",
    "\n",
    "description = client.describe_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the tensors from S3 once the training job is in status Training or Completed. In the following code cell we check the job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Starting]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Downloading]\n",
      "Current job status: [PrimaryStatus: InProgress, SecondaryStatus: Training]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if description['TrainingJobStatus'] != 'Completed':\n",
    "    while description['SecondaryStatus'] not in {'Training', 'Completed'}:\n",
    "        description = client.describe_training_job(TrainingJobName=job_name)\n",
    "        primary_status = description['TrainingJobStatus']\n",
    "        secondary_status = description['SecondaryStatus']\n",
    "        print('Current job status: [PrimaryStatus: {}, SecondaryStatus: {}]'.format(primary_status, secondary_status))\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tensors and visualize BERT model training in real-time\n",
    "In this section, we will retrieve the tensors of our training job and create the attention-head view and neuron view as described in [Visualizing Attention in Transformer-Based Language Representation Models [1]](https://arxiv.org/pdf/1904.02679.pdf).\n",
    "\n",
    "First we create the [trial](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md#Trial) that points to the tensors in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14 21:53:49.860 ip-172-16-9-13:30591 INFO s3_trial.py:42] Loading trial debug-output at path s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output\n",
      "[2020-04-14 21:54:10.505 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:12.546 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:14.598 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:16.648 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:18.691 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:20.730 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:22.769 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:24.808 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:26.844 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:28.896 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:30.934 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:32.978 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:35.017 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:37.065 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:39.104 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:41.225 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:43.375 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:45.418 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:47.481 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:49.519 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:51.563 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:53.611 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:55.658 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:54:57.699 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14 21:54:59.737 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:01.783 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:03.826 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:05.948 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:07.986 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:10.033 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:12.118 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:14.169 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:16.222 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:18.259 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:20.304 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:22.358 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:24.489 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n",
      "[2020-04-14 21:55:26.530 ip-172-16-9-13:30591 WARNING trial.py:148] Waiting to read collections files generated by the training job,from s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-21-50-48-832/debug-output/. If this has been a while, you might want to check that the trial is pointed at the right path.\n"
     ]
    }
   ],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "trial = create_trial( path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertencoder0_position_weight\n",
      "bertencoder0_transformer0_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer0_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer0_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer0_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer0_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer0_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer0_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer0_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer0_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer0_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer0_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer0_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer0_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer0_proj_bias\n",
      "bertencoder0_transformer0_proj_weight\n",
      "bertencoder0_transformer10_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer10_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer10_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer10_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer10_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer10_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer10_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer10_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer10_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer10_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer10_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer10_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer10_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer10_proj_bias\n",
      "bertencoder0_transformer10_proj_weight\n",
      "bertencoder0_transformer11_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer11_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer11_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer11_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer11_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer11_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer11_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer11_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer11_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer11_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer11_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer11_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer11_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer11_proj_bias\n",
      "bertencoder0_transformer11_proj_weight\n",
      "bertencoder0_transformer1_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer1_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer1_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer1_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer1_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer1_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer1_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer1_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer1_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer1_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer1_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer1_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer1_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer1_proj_bias\n",
      "bertencoder0_transformer1_proj_weight\n",
      "bertencoder0_transformer2_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer2_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer2_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer2_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer2_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer2_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer2_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer2_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer2_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer2_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer2_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer2_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer2_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer2_proj_bias\n",
      "bertencoder0_transformer2_proj_weight\n",
      "bertencoder0_transformer3_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer3_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer3_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer3_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer3_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer3_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer3_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer3_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer3_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer3_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer3_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer3_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer3_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer3_proj_bias\n",
      "bertencoder0_transformer3_proj_weight\n",
      "bertencoder0_transformer4_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer4_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer4_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer4_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer4_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer4_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer4_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer4_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer4_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer4_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer4_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer4_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer4_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer4_proj_bias\n",
      "bertencoder0_transformer4_proj_weight\n",
      "bertencoder0_transformer5_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer5_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer5_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer5_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer5_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer5_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer5_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer5_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer5_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer5_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer5_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer5_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer5_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer5_proj_bias\n",
      "bertencoder0_transformer5_proj_weight\n",
      "bertencoder0_transformer6_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer6_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer6_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer6_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer6_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer6_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer6_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer6_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer6_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer6_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer6_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer6_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer6_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer6_proj_bias\n",
      "bertencoder0_transformer6_proj_weight\n",
      "bertencoder0_transformer7_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer7_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer7_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer7_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer7_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer7_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer7_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer7_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer7_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer7_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer7_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer7_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer7_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer7_proj_bias\n",
      "bertencoder0_transformer7_proj_weight\n",
      "bertencoder0_transformer8_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer8_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer8_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer8_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer8_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer8_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer8_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer8_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer8_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer8_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer8_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer8_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer8_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer8_proj_bias\n",
      "bertencoder0_transformer8_proj_weight\n",
      "bertencoder0_transformer9_bertpositionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer9_bertpositionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer9_bertpositionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer9_bertpositionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer9_multiheadattentioncell0_key_bias\n",
      "bertencoder0_transformer9_multiheadattentioncell0_key_output_0\n",
      "bertencoder0_transformer9_multiheadattentioncell0_key_weight\n",
      "bertencoder0_transformer9_multiheadattentioncell0_output_1\n",
      "bertencoder0_transformer9_multiheadattentioncell0_query_bias\n",
      "bertencoder0_transformer9_multiheadattentioncell0_query_output_0\n",
      "bertencoder0_transformer9_multiheadattentioncell0_query_weight\n",
      "bertencoder0_transformer9_multiheadattentioncell0_value_bias\n",
      "bertencoder0_transformer9_multiheadattentioncell0_value_weight\n",
      "bertencoder0_transformer9_proj_bias\n",
      "bertencoder0_transformer9_proj_weight\n",
      "bertforqa0_dense0_bias\n",
      "bertforqa0_dense0_weight\n",
      "bertforqaloss0_output_0\n",
      "bertmodel0_token_type_embed_embedding0_weight\n",
      "bertmodel0_word_embed_embedding0_weight\n",
      "gradient/bertencoder0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_position_weight\n",
      "gradient/bertencoder0_transformer0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer0_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer0_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer0_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer0_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer0_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer0_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer0_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer0_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer0_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer0_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer0_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer0_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer0_proj_bias\n",
      "gradient/bertencoder0_transformer0_proj_weight\n",
      "gradient/bertencoder0_transformer10_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer10_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer10_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer10_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer10_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer10_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer10_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer10_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer10_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer10_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer10_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer10_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer10_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer10_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer10_proj_bias\n",
      "gradient/bertencoder0_transformer10_proj_weight\n",
      "gradient/bertencoder0_transformer11_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer11_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer11_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer11_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer11_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer11_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer11_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer11_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer11_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer11_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer11_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer11_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer11_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer11_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer11_proj_bias\n",
      "gradient/bertencoder0_transformer11_proj_weight\n",
      "gradient/bertencoder0_transformer1_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer1_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer1_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer1_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer1_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer1_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer1_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer1_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer1_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer1_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer1_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer1_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer1_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer1_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer1_proj_bias\n",
      "gradient/bertencoder0_transformer1_proj_weight\n",
      "gradient/bertencoder0_transformer2_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer2_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer2_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer2_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer2_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer2_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer2_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer2_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer2_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer2_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer2_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer2_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer2_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer2_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer2_proj_bias\n",
      "gradient/bertencoder0_transformer2_proj_weight\n",
      "gradient/bertencoder0_transformer3_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer3_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer3_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer3_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer3_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer3_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer3_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer3_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer3_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer3_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer3_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer3_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer3_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer3_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer3_proj_bias\n",
      "gradient/bertencoder0_transformer3_proj_weight\n",
      "gradient/bertencoder0_transformer4_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer4_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer4_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer4_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer4_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer4_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer4_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer4_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer4_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer4_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer4_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer4_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer4_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer4_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer4_proj_bias\n",
      "gradient/bertencoder0_transformer4_proj_weight\n",
      "gradient/bertencoder0_transformer5_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer5_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer5_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer5_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer5_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer5_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer5_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer5_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer5_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer5_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer5_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer5_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer5_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer5_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer5_proj_bias\n",
      "gradient/bertencoder0_transformer5_proj_weight\n",
      "gradient/bertencoder0_transformer6_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer6_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer6_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer6_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer6_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer6_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer6_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer6_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer6_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer6_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer6_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer6_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer6_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer6_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer6_proj_bias\n",
      "gradient/bertencoder0_transformer6_proj_weight\n",
      "gradient/bertencoder0_transformer7_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer7_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer7_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer7_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer7_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer7_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer7_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer7_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer7_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer7_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer7_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer7_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer7_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer7_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer7_proj_bias\n",
      "gradient/bertencoder0_transformer7_proj_weight\n",
      "gradient/bertencoder0_transformer8_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer8_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer8_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer8_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer8_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer8_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer8_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer8_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer8_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer8_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer8_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer8_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer8_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer8_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer8_proj_bias\n",
      "gradient/bertencoder0_transformer8_proj_weight\n",
      "gradient/bertencoder0_transformer9_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer9_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer9_bertpositionwiseffn0_bertlayernorm0_beta\n",
      "gradient/bertencoder0_transformer9_bertpositionwiseffn0_bertlayernorm0_gamma\n",
      "gradient/bertencoder0_transformer9_bertpositionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer9_bertpositionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer9_bertpositionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer9_bertpositionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer9_multiheadattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer9_multiheadattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer9_multiheadattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer9_multiheadattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer9_multiheadattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer9_multiheadattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer9_proj_bias\n",
      "gradient/bertencoder0_transformer9_proj_weight\n",
      "gradient/bertforqa0_dense0_bias\n",
      "gradient/bertforqa0_dense0_weight\n",
      "gradient/bertmodel0_token_type_embed_embedding0_weight\n",
      "gradient/bertmodel0_word_embed_embedding0_weight\n"
     ]
    }
   ],
   "source": [
    "for i in trial.tensor_names():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import a script that implements the visualization for attentation head view in Bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1001\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1001\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1001\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1001\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1001\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1001' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1001\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"1002\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"1002\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '1002' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-1.0.4.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-1.0.4.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-1.0.4.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import attention_head_view, neuron_view\n",
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the tensors from the validation phase. In the next cell we check if such tensors are already available or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors from validation phase not available yet\n",
      "Tensors from validation phase not available yet\n",
      "Tensors from validation phase not available yet\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from smdebug import modes\n",
    "\n",
    "while (True):\n",
    "    if len(trial.steps(modes.EVAL)) == 0:\n",
    "        print(\"Tensors from validation phase not available yet\")\n",
    "    else:\n",
    "        step = trial.steps(modes.EVAL)[0]\n",
    "        break\n",
    "    time.sleep(15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the validation phase started, we can retrieve the tensors from S3. In particular we are interested in outputs of the attention cells which gives the attention score. First we get the tensor names of the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14 21:56:12.731 ip-172-16-9-13:30591 WARNING trial.py:376] No tensors matching the regex pattern:.*dotproductselfattentioncell0_output_1 given were saved\n"
     ]
    }
   ],
   "source": [
    "tensor_names = []\n",
    "\n",
    "for tname in sorted(trial.tensor_names(regex='.*dotproductselfattentioncell0_output_1')):\n",
    "    tensor_names.append(tname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we iterate over the available tensors of the validation phase. We retrieve tensor values with `trial.tensor(tname).value(step, modes.EVAL)`. Note: if training is still in progress, not all steps will be available yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tensors from step 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-fbd7385f66df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnum_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tname' is not defined"
     ]
    }
   ],
   "source": [
    "steps = trial.steps(modes.EVAL)\n",
    "tensors = {}\n",
    "\n",
    "for step in steps:\n",
    "    print(\"Reading tensors from step\", step)\n",
    "    for tname in tensor_names: \n",
    "        if tname not in tensors:\n",
    "            tensors[tname]={}\n",
    "        tensors[tname][step] = trial.tensor(tname).value(step, modes.EVAL)\n",
    "num_heads = tensors[tname][step].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the query and key output tensor names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = []\n",
    "layer_names = {}\n",
    "\n",
    "for index, (key, query) in enumerate(zip(trial.tensor_names(regex='.*key_output_'), trial.tensor_names(regex='.*query_output_'))):\n",
    "    layers.append([key,query])\n",
    "    layer_names[key.split('_')[1]] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also retrieve the string representation of the input tokens that were input into our model during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = trial.tensor('input_tokens').value(0, modes.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Head View\n",
    "\n",
    "The attention-head view shows the attention scores between different tokens. The thicker the line the higher the score. For demonstration purposes, we will limit the visualization to the first 20 tokens. We can select different attention heads and different layers. As training progresses attention scores change and we can check that by selecting a different step. \n",
    "\n",
    "**Note:** The following cells run fine in Jupyter. If you are using JupyterLab and encounter issues with the jupyter widgets (e.g. dropdown menu not displaying), check the subsection in the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 20\n",
    "view = attention_head_view.AttentionHeadView(input_tokens, \n",
    "                                             tensors,  \n",
    "                                             step=trial.steps(modes.EVAL)[0],\n",
    "                                             layer='bertencoder0_transformer0_multiheadattentioncell0_output_1',\n",
    "                                             n_tokens=n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(view.select_layer, layer=tensor_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(view.select_head, head=np.arange(num_heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(view.select_step, step=trial.steps(modes.EVAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell updates the dictionary `tensors`  with the latest tensors from the training the job. Once the dict is updated we can go to above code cell `attention_head_view.AttentionHeadView` and re-execute this and subsequent cells in order to plot latest attentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_steps = trial.steps(modes.EVAL)\n",
    "new_steps = list(set(all_steps).symmetric_difference(set(steps)))\n",
    "\n",
    "for step in new_steps: \n",
    "    for tname in tensor_names:  \n",
    "        if tname not in tensors:\n",
    "            tensors[tname]={}\n",
    "        tensors[tname][step] = trial.tensor(tname).value(step, modes.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron view\n",
    "\n",
    "To create the neuron view as described in paper [Visualizing Attention in Transformer-Based Language Representation Models [1]](https://arxiv.org/pdf/1904.02679.pdf), we need to retrieve the queries and keys from the model. The tensors are reshaped and transposed to have the shape: *batch size, number of attention heads, sequence length, attention head size*\n",
    "\n",
    "**Note:** The following cells run fine in Jupyter. If you are using JupyterLab and encounter issues with the jupyter widgets (e.g. dropdown menu not displaying), check the subsection in the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {}\n",
    "steps = trial.steps(modes.EVAL)\n",
    "\n",
    "for step in steps:\n",
    "    print(\"Reading tensors from step\", step)\n",
    "    \n",
    "    for tname in trial.tensor_names(regex='.*query_output'):\n",
    "       query = trial.tensor(tname).value(step, modes.EVAL)\n",
    "       query = query.reshape((query.shape[0], query.shape[1], num_heads, -1))\n",
    "       query = query.transpose(0,2,1,3)\n",
    "       if tname not in queries:\n",
    "            queries[tname] = {}\n",
    "       queries[tname][step] = query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the key vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {}\n",
    "steps = trial.steps(modes.EVAL)\n",
    "\n",
    "for step in steps:\n",
    "    print(\"Reading tensors from step\", step)\n",
    "    \n",
    "    for tname in trial.tensor_names(regex='.*key_output'):\n",
    "       key = trial.tensor(tname).value(step, modes.EVAL)\n",
    "       key = key.reshape((key.shape[0], key.shape[1], num_heads, -1))\n",
    "       key = key.transpose(0,2,1,3)\n",
    "       if tname not in keys:\n",
    "            keys[tname] = {}\n",
    "       keys[tname][step] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now select different query vectors and see how they produce different attention scores. We can also select different steps to see how attention scores, query and key vectors change as training progresses. The neuron view shows:\n",
    "* Query\n",
    "* Key\n",
    "* Query x Key (element wise product)\n",
    "* Query * Key (dot product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = neuron_view.NeuronView(input_tokens, \n",
    "                              keys=keys, \n",
    "                              queries=queries, \n",
    "                              layers=layers, \n",
    "                              step=trial.steps(modes.EVAL)[0], \n",
    "                              n_tokens=n_tokens,\n",
    "                              layer_names=layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(view.select_query, query=np.arange(n_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(view.select_layer, layer=layer_names.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(view.select_step, step=trial.steps(modes.EVAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Jupyter widgets in JupyterLab\n",
    "\n",
    "If you encounter issues with this notebook in JupyterLab, you may have to install JupyterLab extensions. You can do this by defining a SageMaker [Lifecycle configuration](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html). A lifecycle configuration is a shell script that runs when you either create a notebook instance or whenever you start an instance. You can create a Lifecycle configuration directly in the SageMaker console (more details [here](https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/)) When selecting `Start notebook`, copy and paste the following code. Once the configuration is created attach it to your notebook instance and start the instance.\n",
    "\n",
    "```sh\n",
    "#!/bin/bash\n",
    "\n",
    "set -e\n",
    "\n",
    "# OVERVIEW\n",
    "# This script installs a single jupyter notebook extension package in SageMaker Notebook Instance\n",
    "# For more details of the example extension, see https://github.com/jupyter-widgets/ipywidgets\n",
    "\n",
    "sudo -u ec2-user -i <<'EOF'\n",
    "\n",
    "# PARAMETERS\n",
    "PIP_PACKAGE_NAME=ipywidgets\n",
    "EXTENSION_NAME=widgetsnbextension\n",
    "\n",
    "source /home/ec2-user/anaconda3/bin/activate JupyterSystemEnv\n",
    "\n",
    "pip install $PIP_PACKAGE_NAME\n",
    "jupyter nbextension enable $EXTENSION_NAME --py --sys-prefix\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "# run the command in background to avoid timeout \n",
    "nohup jupyter labextension install @bokeh/jupyter_bokeh &\n",
    "\n",
    "source /home/ec2-user/anaconda3/bin/deactivate\n",
    "\n",
    "EOF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
