{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SageMaker debugger to monitor attentions in BERT model training\n",
    "\n",
    "[BERT](https://arxiv.org/abs/1810.04805) is a deep bidirectional transformer model that achieves state-of the art results in NLP tasks like question answering, text classification and others.\n",
    "In this notebook we will use [GluonNLP](https://gluon-nlp.mxnet.io/) to finetune a pretrained BERT model on the [Stanford Question and Answering dataset](https://web.stanford.edu/class/cs224n/reports/default/15848195.pdf) and we will use [SageMaker Debugger](https://docs.aws.amazon.com/sagemaker/latest/dg/train-debugger.html) to monitor model training in real-time. \n",
    "\n",
    "The paper [Visualizing Attention in Transformer-Based Language Representation Models [1]](https://arxiv.org/pdf/1904.02679.pdf) shows that plotting attentions and individual neurons in the query and key vectors can help to identify causes of incorrect model predictions.\n",
    "With SageMaker Debugger we can easily retrieve those tensors and plot them in real-time as training progresses which may help to understand what the model is learning. \n",
    "\n",
    "The animation below shows the attention scores of the first 20 input tokens for the first 10 iterations in the training.\n",
    "\n",
    "<img src='images/attention_scores.gif' width='350' /> \n",
    "Fig. 1: Attention scores of the first head in the 7th layer \n",
    "\n",
    "[1] *Visualizing Attention in Transformer-Based Language Representation Models*:  Jesse Vig, 2019, 1904.02679, arXiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting smdebug\n",
      "  Using cached https://files.pythonhosted.org/packages/1c/95/83b46ff798c2be59a1a78e02170f9547381f5b088d80b9befdfe35505399/smdebug-0.7.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from smdebug) (1.16.4)\n",
      "Requirement already satisfied: boto3>=1.10.32 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from smdebug) (1.12.24)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from smdebug) (20.1)\n",
      "Collecting protobuf>=3.6.0 (from smdebug)\n",
      "  Using cached https://files.pythonhosted.org/packages/57/02/5432412c162989260fab61fa65e0a490c1872739eb91a659896e4d554b26/protobuf-3.11.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.24 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from boto3>=1.10.32->smdebug) (1.15.24)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from packaging->smdebug) (1.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from packaging->smdebug) (2.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from protobuf>=3.6.0->smdebug) (39.1.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3>=1.10.32->smdebug) (2.7.3)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3>=1.10.32->smdebug) (1.23)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3>=1.10.32->smdebug) (0.14)\n",
      "Installing collected packages: protobuf, smdebug\n",
      "  Found existing installation: protobuf 3.5.2\n",
      "    Uninstalling protobuf-3.5.2:\n",
      "      Successfully uninstalled protobuf-3.5.2\n",
      "Successfully installed protobuf-3.11.3 smdebug-0.7.2\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.0.2 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "boto_session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session(boto_session=boto_session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SageMaker training\n",
    "The following code defines the SageMaker Estimator. The entry point script [train.py](entry_point/train.py) defines the model training. It downloads a BERT model from the GluonNLP model zoo and finetunes the model on the Stanford Question Answering dataset. The training script follows the official GluonNLP [example](https://github.com/dmlc/gluon-nlp/blob/v0.8.x/scripts/bert/finetune_squad.py) on finetuning BERT.\n",
    "\n",
    "For demonstration purposes we will train only on a subset of the data (`train_dataset_size`) and perform evaluation on a single batch (`val_dataset_size`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "from sagemaker.debugger import DebuggerHookConfig, CollectionConfig\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "BUCKET_NAME = sagemaker_session.default_bucket()\n",
    "LOCATION_IN_BUCKET = 'smdebug-output'\n",
    "s3_bucket_for_tensors = 's3://{BUCKET_NAME}/{LOCATION_IN_BUCKET}'.format(BUCKET_NAME=BUCKET_NAME, LOCATION_IN_BUCKET=LOCATION_IN_BUCKET)\n",
    "\n",
    "mxnet_estimator = MXNet(entry_point='train.py',\n",
    "                            source_dir='entry_point',\n",
    "                            role=role,\n",
    "                            train_instance_type='ml.p3.8xlarge',\n",
    "                            train_instance_count=1,\n",
    "                            framework_version='1.6.0',\n",
    "                            py_version='py3',\n",
    "                            hyperparameters = {'epochs': 3, \n",
    "                                               'batch_size': 16,\n",
    "                                               'learning_rate': 5e-5,\n",
    "                                               'train_dataset_size': 1024,\n",
    "                                               'val_dataset_size': 16},\n",
    "                            debugger_hook_config = DebuggerHookConfig(\n",
    "                              s3_output_path=s3_bucket_for_tensors,  \n",
    "                              collection_configs=[\n",
    "                                CollectionConfig(\n",
    "                                    name=\"all\",\n",
    "                                    parameters={\"include_regex\": \n",
    "                                                \".*multiheadattentioncell0_output_1|.*key_output|.*query_output\",\n",
    "                                                \"train.save_steps\": \"0\",\n",
    "                                                \"eval.save_interval\": \"1\"}\n",
    "                                    )\n",
    "                                 ]\n",
    "                               )\n",
    "                            )                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker Debugger will monitor by default collections such as gradients, weights and biases. The default `save_interval` is 100 steps. A step presents the work done by the training job for one batch (i.e. forward and backward pass). \n",
    "\n",
    "In this example we are also interested in attention scores, query and key output tensors. We can emit them by just defining a new [collection](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/api.md#collection). In this example we call the collection `all` and define the corresponding regex. We save every iteration during validation phase (`eval.save_interval`) and only the first iteration during training phase (`train.save_steps`).\n",
    "\n",
    "\n",
    "We also add the following lines in the validation loop to record the string representation of input tokens:\n",
    "```python\n",
    "if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):  \n",
    "   hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mxnet_estimator.fit(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the S3 location of tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensors are stored in: s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-02-56-18-775/debug-output\n"
     ]
    }
   ],
   "source": [
    "path = mxnet_estimator.latest_job_debugger_artifacts_path()\n",
    "print('Tensors are stored in: {}'.format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the training job name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training job name: mxnet-training-2020-04-14-02-56-18-775\n"
     ]
    }
   ],
   "source": [
    "job_name = mxnet_estimator.latest_training_job.name\n",
    "print('Training job name: {}'.format(job_name))\n",
    "\n",
    "client = mxnet_estimator.sagemaker_session.sagemaker_client\n",
    "\n",
    "description = client.describe_training_job(TrainingJobName=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the tensors from S3 once the training job is in status Training or Completed. In the following code cell we check the job status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "if description['TrainingJobStatus'] != 'Completed':\n",
    "    while description['SecondaryStatus'] not in {'Training', 'Completed'}:\n",
    "        description = client.describe_training_job(TrainingJobName=job_name)\n",
    "        primary_status = description['TrainingJobStatus']\n",
    "        secondary_status = description['SecondaryStatus']\n",
    "        print('Current job status: [PrimaryStatus: {}, SecondaryStatus: {}]'.format(primary_status, secondary_status))\n",
    "        time.sleep(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tensors and visualize BERT model training in real-time\n",
    "In this section, we will retrieve the tensors of our training job and create the attention-head view and neuron view as described in [Visualizing Attention in Transformer-Based Language Representation Models [1]](https://arxiv.org/pdf/1904.02679.pdf).\n",
    "\n",
    "First we create the [trial](https://github.com/awslabs/sagemaker-debugger/blob/master/docs/analysis.md#Trial) that points to the tensors in S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14 03:32:05.946 ip-172-16-9-13:5751 INFO s3_trial.py:42] Loading trial debug-output at path s3://sagemaker-us-east-1-828802286385/smdebug-output/mxnet-training-2020-04-14-02-56-18-775/debug-output\n"
     ]
    }
   ],
   "source": [
    "from smdebug.trials import create_trial\n",
    "\n",
    "trial = create_trial( path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bertencoder0_position_weight\n",
      "bertencoder0_transformer0_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer0_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer0_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer0_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer0_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer0_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer0_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer0_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer0_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer0_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer0_proj_bias\n",
      "bertencoder0_transformer0_proj_weight\n",
      "bertencoder0_transformer10_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer10_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer10_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer10_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer10_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer10_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer10_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer10_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer10_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer10_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer10_proj_bias\n",
      "bertencoder0_transformer10_proj_weight\n",
      "bertencoder0_transformer11_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer11_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer11_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer11_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer11_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer11_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer11_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer11_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer11_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer11_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer11_proj_bias\n",
      "bertencoder0_transformer11_proj_weight\n",
      "bertencoder0_transformer1_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer1_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer1_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer1_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer1_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer1_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer1_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer1_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer1_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer1_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer1_proj_bias\n",
      "bertencoder0_transformer1_proj_weight\n",
      "bertencoder0_transformer2_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer2_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer2_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer2_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer2_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer2_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer2_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer2_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer2_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer2_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer2_proj_bias\n",
      "bertencoder0_transformer2_proj_weight\n",
      "bertencoder0_transformer3_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer3_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer3_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer3_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer3_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer3_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer3_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer3_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer3_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer3_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer3_proj_bias\n",
      "bertencoder0_transformer3_proj_weight\n",
      "bertencoder0_transformer4_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer4_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer4_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer4_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer4_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer4_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer4_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer4_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer4_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer4_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer4_proj_bias\n",
      "bertencoder0_transformer4_proj_weight\n",
      "bertencoder0_transformer5_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer5_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer5_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer5_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer5_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer5_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer5_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer5_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer5_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer5_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer5_proj_bias\n",
      "bertencoder0_transformer5_proj_weight\n",
      "bertencoder0_transformer6_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer6_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer6_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer6_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer6_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer6_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer6_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer6_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer6_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer6_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer6_proj_bias\n",
      "bertencoder0_transformer6_proj_weight\n",
      "bertencoder0_transformer7_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer7_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer7_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer7_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer7_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer7_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer7_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer7_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer7_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer7_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer7_proj_bias\n",
      "bertencoder0_transformer7_proj_weight\n",
      "bertencoder0_transformer8_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer8_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer8_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer8_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer8_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer8_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer8_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer8_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer8_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer8_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer8_proj_bias\n",
      "bertencoder0_transformer8_proj_weight\n",
      "bertencoder0_transformer9_dotproductselfattentioncell0_key_bias\n",
      "bertencoder0_transformer9_dotproductselfattentioncell0_key_weight\n",
      "bertencoder0_transformer9_dotproductselfattentioncell0_query_bias\n",
      "bertencoder0_transformer9_dotproductselfattentioncell0_query_weight\n",
      "bertencoder0_transformer9_dotproductselfattentioncell0_value_bias\n",
      "bertencoder0_transformer9_dotproductselfattentioncell0_value_weight\n",
      "bertencoder0_transformer9_positionwiseffn0_ffn_1_bias\n",
      "bertencoder0_transformer9_positionwiseffn0_ffn_1_weight\n",
      "bertencoder0_transformer9_positionwiseffn0_ffn_2_bias\n",
      "bertencoder0_transformer9_positionwiseffn0_ffn_2_weight\n",
      "bertencoder0_transformer9_proj_bias\n",
      "bertencoder0_transformer9_proj_weight\n",
      "bertforqa0_dense0_bias\n",
      "bertforqa0_dense0_weight\n",
      "bertforqaloss0_output_0\n",
      "bertmodel0_token_type_embed_embedding0_weight\n",
      "bertmodel0_word_embed_embedding0_weight\n",
      "gradient/bertencoder0_layernorm0_beta\n",
      "gradient/bertencoder0_layernorm0_gamma\n",
      "gradient/bertencoder0_position_weight\n",
      "gradient/bertencoder0_transformer0_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer0_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer0_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer0_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer0_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer0_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer0_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer0_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer0_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer0_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer0_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer0_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer0_proj_bias\n",
      "gradient/bertencoder0_transformer0_proj_weight\n",
      "gradient/bertencoder0_transformer10_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer10_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer10_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer10_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer10_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer10_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer10_layernorm0_beta\n",
      "gradient/bertencoder0_transformer10_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer10_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer10_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer10_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer10_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer10_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer10_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer10_proj_bias\n",
      "gradient/bertencoder0_transformer10_proj_weight\n",
      "gradient/bertencoder0_transformer11_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer11_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer11_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer11_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer11_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer11_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer11_layernorm0_beta\n",
      "gradient/bertencoder0_transformer11_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer11_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer11_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer11_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer11_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer11_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer11_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer11_proj_bias\n",
      "gradient/bertencoder0_transformer11_proj_weight\n",
      "gradient/bertencoder0_transformer1_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer1_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer1_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer1_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer1_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer1_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer1_layernorm0_beta\n",
      "gradient/bertencoder0_transformer1_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer1_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer1_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer1_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer1_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer1_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer1_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer1_proj_bias\n",
      "gradient/bertencoder0_transformer1_proj_weight\n",
      "gradient/bertencoder0_transformer2_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer2_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer2_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer2_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer2_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer2_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer2_layernorm0_beta\n",
      "gradient/bertencoder0_transformer2_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer2_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer2_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer2_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer2_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer2_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer2_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer2_proj_bias\n",
      "gradient/bertencoder0_transformer2_proj_weight\n",
      "gradient/bertencoder0_transformer3_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer3_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer3_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer3_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer3_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer3_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer3_layernorm0_beta\n",
      "gradient/bertencoder0_transformer3_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer3_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer3_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer3_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer3_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer3_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer3_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer3_proj_bias\n",
      "gradient/bertencoder0_transformer3_proj_weight\n",
      "gradient/bertencoder0_transformer4_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer4_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer4_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer4_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer4_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer4_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer4_layernorm0_beta\n",
      "gradient/bertencoder0_transformer4_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer4_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer4_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer4_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer4_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer4_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer4_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer4_proj_bias\n",
      "gradient/bertencoder0_transformer4_proj_weight\n",
      "gradient/bertencoder0_transformer5_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer5_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer5_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer5_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer5_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer5_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer5_layernorm0_beta\n",
      "gradient/bertencoder0_transformer5_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer5_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer5_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer5_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer5_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer5_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer5_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer5_proj_bias\n",
      "gradient/bertencoder0_transformer5_proj_weight\n",
      "gradient/bertencoder0_transformer6_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer6_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer6_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer6_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer6_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer6_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer6_layernorm0_beta\n",
      "gradient/bertencoder0_transformer6_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer6_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer6_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer6_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer6_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer6_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer6_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer6_proj_bias\n",
      "gradient/bertencoder0_transformer6_proj_weight\n",
      "gradient/bertencoder0_transformer7_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer7_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer7_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer7_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer7_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer7_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer7_layernorm0_beta\n",
      "gradient/bertencoder0_transformer7_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer7_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer7_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer7_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer7_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer7_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer7_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer7_proj_bias\n",
      "gradient/bertencoder0_transformer7_proj_weight\n",
      "gradient/bertencoder0_transformer8_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer8_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer8_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer8_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer8_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer8_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer8_layernorm0_beta\n",
      "gradient/bertencoder0_transformer8_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer8_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer8_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer8_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer8_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer8_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer8_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer8_proj_bias\n",
      "gradient/bertencoder0_transformer8_proj_weight\n",
      "gradient/bertencoder0_transformer9_dotproductselfattentioncell0_key_bias\n",
      "gradient/bertencoder0_transformer9_dotproductselfattentioncell0_key_weight\n",
      "gradient/bertencoder0_transformer9_dotproductselfattentioncell0_query_bias\n",
      "gradient/bertencoder0_transformer9_dotproductselfattentioncell0_query_weight\n",
      "gradient/bertencoder0_transformer9_dotproductselfattentioncell0_value_bias\n",
      "gradient/bertencoder0_transformer9_dotproductselfattentioncell0_value_weight\n",
      "gradient/bertencoder0_transformer9_layernorm0_beta\n",
      "gradient/bertencoder0_transformer9_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer9_positionwiseffn0_ffn_1_bias\n",
      "gradient/bertencoder0_transformer9_positionwiseffn0_ffn_1_weight\n",
      "gradient/bertencoder0_transformer9_positionwiseffn0_ffn_2_bias\n",
      "gradient/bertencoder0_transformer9_positionwiseffn0_ffn_2_weight\n",
      "gradient/bertencoder0_transformer9_positionwiseffn0_layernorm0_beta\n",
      "gradient/bertencoder0_transformer9_positionwiseffn0_layernorm0_gamma\n",
      "gradient/bertencoder0_transformer9_proj_bias\n",
      "gradient/bertencoder0_transformer9_proj_weight\n",
      "gradient/bertforqa0_dense0_bias\n",
      "gradient/bertforqa0_dense0_weight\n",
      "gradient/bertmodel0_token_type_embed_embedding0_weight\n",
      "gradient/bertmodel0_word_embed_embedding0_weight\n",
      "input_tokens\n"
     ]
    }
   ],
   "source": [
    "for i in trial.tensor_names():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we import a script that implements the visualization for attentation head view in Bokeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import attention_head_view, neuron_view\n",
    "from ipywidgets import interactive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the tensors from the validation phase. In the next cell we check if such tensors are already available or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14 03:32:09.874 ip-172-16-9-13:5751 INFO trial.py:198] Training has ended, will refresh one final time in 1 sec.\n",
      "[2020-04-14 03:32:10.928 ip-172-16-9-13:5751 INFO trial.py:210] Loaded all steps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from smdebug import modes\n",
    "\n",
    "while (True):\n",
    "    if len(trial.steps(modes.EVAL)) == 0:\n",
    "        print(\"Tensors from validation phase not available yet\")\n",
    "    else:\n",
    "        step = trial.steps(modes.EVAL)[0]\n",
    "        break\n",
    "    time.sleep(15) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the validation phase started, we can retrieve the tensors from S3. In particular we are interested in outputs of the attention cells which gives the attention score. First we get the tensor names of the attention scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14 04:13:36.672 ip-172-16-9-13:5751 WARNING trial.py:376] No tensors matching the regex pattern:.*dotproductselfattentioncell0_output_1 given were saved\n"
     ]
    }
   ],
   "source": [
    "tensor_names = []\n",
    "\n",
    "for tname in sorted(trial.tensor_names(regex='.*dotproductselfattentioncell0_output_1')):\n",
    "    tensor_names.append(tname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we iterate over the available tensors of the validation phase. We retrieve tensor values with `trial.tensor(tname).value(step, modes.EVAL)`. Note: if training is still in progress, not all steps will be available yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tensors from step 0\n",
      "Reading tensors from step 1\n",
      "Reading tensors from step 2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tname' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-fbd7385f66df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnum_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tname' is not defined"
     ]
    }
   ],
   "source": [
    "steps = trial.steps(modes.EVAL)\n",
    "tensors = {}\n",
    "\n",
    "for step in steps:\n",
    "    print(\"Reading tensors from step\", step)\n",
    "    for tname in tensor_names: \n",
    "        if tname not in tensors:\n",
    "            tensors[tname]={}\n",
    "        tensors[tname][step] = trial.tensor(tname).value(step, modes.EVAL)\n",
    "num_heads = tensors[tname][step].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get the query and key output tensor names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2020-04-14 03:33:35.765 ip-172-16-9-13:5751 WARNING trial.py:376] No tensors matching the regex pattern:.*key_output_ given were saved\n",
      "[2020-04-14 03:33:35.769 ip-172-16-9-13:5751 WARNING trial.py:376] No tensors matching the regex pattern:.*query_output_ given were saved\n"
     ]
    }
   ],
   "source": [
    "layers = []\n",
    "layer_names = {}\n",
    "\n",
    "for index, (key, query) in enumerate(zip(trial.tensor_names(regex='.*key_output_'), trial.tensor_names(regex='.*query_output_'))):\n",
    "    layers.append([key,query])\n",
    "    layer_names[key.split('_')[1]] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also retrieve the string representation of the input tokens that were input into our model during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = trial.tensor('input_tokens').value(0, modes.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Head View\n",
    "\n",
    "The attention-head view shows the attention scores between different tokens. The thicker the line the higher the score. For demonstration purposes, we will limit the visualization to the first 20 tokens. We can select different attention heads and different layers. As training progresses attention scores change and we can check that by selecting a different step. \n",
    "\n",
    "**Note:** The following cells run fine in Jupyter. If you are using JupyterLab and encounter issues with the jupyter widgets (e.g. dropdown menu not displaying), check the subsection in the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'bertencoder0_transformer0_multiheadattentioncell0_output_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-992e55bafc92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m                                              \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                              \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bertencoder0_transformer0_multiheadattentioncell0_output_1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                              n_tokens=n_tokens)\n\u001b[0m",
      "\u001b[0;32m~/SageMaker/workshop/06_train/bert_attention_head_view/utils/attention_head_view.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_tokens, tensors, layer, step, n_tokens)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SageMaker/workshop/06_train/bert_attention_head_view/utils/attention_head_view.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m#plot attention weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bertencoder0_transformer0_multiheadattentioncell0_output_1'"
     ]
    }
   ],
   "source": [
    "n_tokens = 20\n",
    "view = attention_head_view.AttentionHeadView(input_tokens, \n",
    "                                             tensors,  \n",
    "                                             step=trial.steps(modes.EVAL)[0],\n",
    "                                             layer='bertencoder0_transformer0_multiheadattentioncell0_output_1',\n",
    "                                             n_tokens=n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'view' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-33b4db752a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_layer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'view' is not defined"
     ]
    }
   ],
   "source": [
    "interactive(view.select_layer, layer=tensor_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'view' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-90bb9be5f8ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_head\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'view' is not defined"
     ]
    }
   ],
   "source": [
    "interactive(view.select_head, head=np.arange(num_heads))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'view' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-066a24855e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minteractive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEVAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'view' is not defined"
     ]
    }
   ],
   "source": [
    "interactive(view.select_step, step=trial.steps(modes.EVAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell updates the dictionary `tensors`  with the latest tensors from the training the job. Once the dict is updated we can go to above code cell `attention_head_view.AttentionHeadView` and re-execute this and subsequent cells in order to plot latest attentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_steps = trial.steps(modes.EVAL)\n",
    "new_steps = list(set(all_steps).symmetric_difference(set(steps)))\n",
    "\n",
    "for step in new_steps: \n",
    "    for tname in tensor_names:  \n",
    "        if tname not in tensors:\n",
    "            tensors[tname]={}\n",
    "        tensors[tname][step] = trial.tensor(tname).value(step, modes.EVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neuron view\n",
    "\n",
    "To create the neuron view as described in paper [Visualizing Attention in Transformer-Based Language Representation Models [1]](https://arxiv.org/pdf/1904.02679.pdf), we need to retrieve the queries and keys from the model. The tensors are reshaped and transposed to have the shape: *batch size, number of attention heads, sequence length, attention head size*\n",
    "\n",
    "**Note:** The following cells run fine in Jupyter. If you are using JupyterLab and encounter issues with the jupyter widgets (e.g. dropdown menu not displaying), check the subsection in the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = {}\n",
    "steps = trial.steps(modes.EVAL)\n",
    "\n",
    "for step in steps:\n",
    "    print(\"Reading tensors from step\", step)\n",
    "    \n",
    "    for tname in trial.tensor_names(regex='.*query_output'):\n",
    "       query = trial.tensor(tname).value(step, modes.EVAL)\n",
    "       query = query.reshape((query.shape[0], query.shape[1], num_heads, -1))\n",
    "       query = query.transpose(0,2,1,3)\n",
    "       if tname not in queries:\n",
    "            queries[tname] = {}\n",
    "       queries[tname][step] = query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the key vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = {}\n",
    "steps = trial.steps(modes.EVAL)\n",
    "\n",
    "for step in steps:\n",
    "    print(\"Reading tensors from step\", step)\n",
    "    \n",
    "    for tname in trial.tensor_names(regex='.*key_output'):\n",
    "       key = trial.tensor(tname).value(step, modes.EVAL)\n",
    "       key = key.reshape((key.shape[0], key.shape[1], num_heads, -1))\n",
    "       key = key.transpose(0,2,1,3)\n",
    "       if tname not in keys:\n",
    "            keys[tname] = {}\n",
    "       keys[tname][step] = key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now select different query vectors and see how they produce different attention scores. We can also select different steps to see how attention scores, query and key vectors change as training progresses. The neuron view shows:\n",
    "* Query\n",
    "* Key\n",
    "* Query x Key (element wise product)\n",
    "* Query * Key (dot product)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = neuron_view.NeuronView(input_tokens, \n",
    "                              keys=keys, \n",
    "                              queries=queries, \n",
    "                              layers=layers, \n",
    "                              step=trial.steps(modes.EVAL)[0], \n",
    "                              n_tokens=n_tokens,\n",
    "                              layer_names=layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(view.select_query, query=np.arange(n_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(view.select_layer, layer=layer_names.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive(view.select_step, step=trial.steps(modes.EVAL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Jupyter widgets in JupyterLab\n",
    "\n",
    "If you encounter issues with this notebook in JupyterLab, you may have to install JupyterLab extensions. You can do this by defining a SageMaker [Lifecycle configuration](https://docs.aws.amazon.com/sagemaker/latest/dg/notebook-lifecycle-config.html). A lifecycle configuration is a shell script that runs when you either create a notebook instance or whenever you start an instance. You can create a Lifecycle configuration directly in the SageMaker console (more details [here](https://aws.amazon.com/blogs/machine-learning/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access/)) When selecting `Start notebook`, copy and paste the following code. Once the configuration is created attach it to your notebook instance and start the instance.\n",
    "\n",
    "```sh\n",
    "#!/bin/bash\n",
    "\n",
    "set -e\n",
    "\n",
    "# OVERVIEW\n",
    "# This script installs a single jupyter notebook extension package in SageMaker Notebook Instance\n",
    "# For more details of the example extension, see https://github.com/jupyter-widgets/ipywidgets\n",
    "\n",
    "sudo -u ec2-user -i <<'EOF'\n",
    "\n",
    "# PARAMETERS\n",
    "PIP_PACKAGE_NAME=ipywidgets\n",
    "EXTENSION_NAME=widgetsnbextension\n",
    "\n",
    "source /home/ec2-user/anaconda3/bin/activate JupyterSystemEnv\n",
    "\n",
    "pip install $PIP_PACKAGE_NAME\n",
    "jupyter nbextension enable $EXTENSION_NAME --py --sys-prefix\n",
    "jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "# run the command in background to avoid timeout \n",
    "nohup jupyter labextension install @bokeh/jupyter_bokeh &\n",
    "\n",
    "source /home/ec2-user/anaconda3/bin/deactivate\n",
    "\n",
    "EOF\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
