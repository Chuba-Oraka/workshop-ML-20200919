import sys
import subprocess
import os
import argparse
import json
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.2.0-rc1'])
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'bert-for-tf2'])
subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'sentencepiece'])
import tensorflow as tf
print(tf.__version__)
import boto3
import pandas as pd
import math
import datetime
from tqdm import tqdm
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
import bert
from bert import BertModelLayer
from bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights
from bert.tokenization.bert_tokenization import FullTokenizer
from sklearn.metrics import confusion_matrix, classification_report

#train = pd.read_csv('./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz', delimiter='\t')
# test = pd.read_csv('./data/amazon_reviews_us_Digital_Software_v1_00.tsv.gz', delimiter='\t')

os.system('rm uncased_L-12_H-768_A-12.zip')
os.system('rm -rf uncased_L-12_H-768_A-12')
os.system('wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip')

import zipfile
with zipfile.ZipFile('uncased_L-12_H-768_A-12.zip', 'r') as zip_ref:
  zip_ref.extractall('.')
os.system('ls -al ./uncased_L-12_H-768_A-12')

bert_ckpt_dir = './uncased_L-12_H-768_A-12'
bert_ckpt_file = os.path.join(bert_ckpt_dir, "bert_model.ckpt")
bert_config_file = os.path.join(bert_ckpt_dir, "bert_config.json")

train_tfrecord_file = './data/train/amazon_reviews_us_Digital_Software_v1_00.tfrecord'
test_tfrecord_file = './data/test/amazon_reviews_us_Digital_Software_v1_00.tfrecord'

tokenizer = FullTokenizer(os.path.join(bert_ckpt_dir, "vocab.txt"))
pad_id, cls_id, sep_id = tokenizer.convert_tokens_to_ids(["[PAD]", "[CLS]", "[SEP]"])
print("pad cls sep:", pad_id, cls_id, sep_id)

MAX_SEQ_LEN=128
BATCH_SIZE=8
CLASSES=[1, 2, 3, 4, 5]

###########################
# TODO:  TSV to TFRecord

from functools import partial
from glob import glob
from multiprocessing import Pool

# def load_sample(path):
#     """Loads Amazon Customer Reviews Dataset from a file."""
#     label   = path.split('/')[-2]
#     with open(path, "r") as f:
#         content = f.read()
#     return content, label
def load_sample(path):
    df = pd.read_csv(path, delimiter='\t')
    df.shape
    df.head()    
    return df['review_body'], df['star_rating']


def preprocess_sample(content, label):
    content = content.replace("<br />", " ")
    return content, label


def encode_sample(content, label, tokenizer):
    content = tokenizer.tokenize(content)
    content = tokenizer.convert_tokens_to_ids(content)
    label = int(label == "pos")
    return content, label


def serialize_example(token_ids, label):
    feature = {
        "token_ids": tf.train.Feature(int64_list=tf.train.Int64List(value=token_ids)),
        "label":     tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))
    }
    proto = tf.train.Example(features=tf.train.Features(feature=feature))
    return proto.SerializeToString()


def to_tfrecord(file_path, tokenizer):
    sample = load_sample(file_path)
    sample = preprocess_sample(*sample)
    sample = encode_sample(*sample, tokenizer=tokenizer)
    sample = serialize_example(*sample)
    return sample


def convert_to_tfrecord_file(file_name, data_dir, serializer_fn):
    with tf.io.TFRecordWriter(file_name) as writer:    
        all_files = glob(os.path.join(data_dir, "*"))
        with Pool() as pool:
            protos = pool.imap_unordered(serializer_fn, all_files)
            for proto in tqdm(protos, total=len(all_files)):
                writer.write(proto)

                
def serialize_to_tfrecord(ds_file):
    return to_tfrecord(ds_file, tokenizer)
###########################


def tfrecord_to_dataset(filenames):
    ds = tf.data.TFRecordDataset(filenames)
    feature_description = {
      "token_ids": tf.io.VarLenFeature(tf.int64),
      "text_b": tf.io.VarLenFeature(tf.int64),
      "label": tf.io.FixedLenFeature([], tf.int64, default_value=-1)
    }

    def parse_proto(proto):
        example = tf.io.parse_single_example(proto, feature_description)
        print(example)
        token_ids, label = example["token_ids"], example["label"]
        token_ids = tf.sparse.to_dense(token_ids)
        return token_ids, label

    return ds.map(parse_proto)


def create_pad_example_fn(pad_len, 
                          pad_id=pad_id, 
                          cls_id=cls_id, 
                          sep_id=sep_id,
                          trim_beginning=True):
    def pad_example(x, label):
        seq_len = pad_len - 2
        x = x[-seq_len:] if trim_beginning else x[:seq_len]
        x = tf.pad(x, [[0, seq_len - tf.shape(x)[-1]]], constant_values=pad_id)
        x = tf.concat([[cls_id], x, [sep_id]], axis=-1)
        return  tf.reshape(x, (pad_len,)), tf.reshape(label, ())

    return pad_example


# Deprecate this (CSV)
# class ClassificationData:
#   TEXT_COLUMN = 'review_body'
#   LABEL_COLUMN = 'star_rating'

#   def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):
#     self.tokenizer = tokenizer
#     self.max_seq_len = 0
#     self.classes = classes
    
#     ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])

# #    print('max seq_len', self.max_seq_len)
#     self.max_seq_len = min(self.max_seq_len, max_seq_len)
#     self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])

#   def _prepare(self, df):
#     x, y = [], []
    
#     for _, row in tqdm(df.iterrows()):
#       text, label = row[ClassificationData.TEXT_COLUMN], row[ClassificationData.LABEL_COLUMN]
#       tokens = self.tokenizer.tokenize(text)
#       tokens = ["[CLS]"] + tokens + ["[SEP]"]
#       token_ids = self.tokenizer.convert_tokens_to_ids(tokens)
#       self.max_seq_len = max(self.max_seq_len, len(token_ids))
#       x.append(token_ids)
#       y.append(self.classes.index(label))

#     return np.array(x), np.array(y)

#   def _pad(self, ids):
#     x = []
#     for input_ids in ids:
#       input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]
#       input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))
#       x.append(np.array(input_ids))
#     return np.array(x)

# tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, "vocab.txt"))

# tokenizer.tokenize("I can't wait to visit Bulgaria again!")

# tokens = tokenizer.tokenize("I can't wait to visit Bulgaria again!")
# tokenizer.convert_tokens_to_ids(tokens)


def flatten_layers(root_layer):
    if isinstance(root_layer, keras.layers.Layer):
        yield root_layer
    for layer in root_layer._layers:
        for sub_layer in flatten_layers(layer):
            yield sub_layer


def freeze_bert_layers(l_bert):
    """
    Freezes all but LayerNorm and adapter layers - see arXiv:1902.00751.
    """
    for layer in flatten_layers(l_bert):
        if layer.name in ["LayerNorm", "adapter-down", "adapter-up"]:
            layer.trainable = True
        elif len(layer._layers) == 0:
            layer.trainable = False
        l_bert.embeddings_layer.trainable = False


def create_learning_rate_scheduler(max_learn_rate=5e-5,
                                   end_learn_rate=1e-7,
                                   warmup_epoch_count=10,
                                   total_epoch_count=90):

    def lr_scheduler(epoch):
        if epoch < warmup_epoch_count:
            res = (max_learn_rate/warmup_epoch_count) * (epoch + 1)
        else:
            res = max_learn_rate*math.exp(math.log(end_learn_rate/max_learn_rate)*(epoch-warmup_epoch_count+1)/(total_epoch_count-warmup_epoch_count+1))
        return float(res)
    learning_rate_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)

    return learning_rate_scheduler


def create_model(max_seq_len, bert_ckpt_file, adapter_size):
    with tf.io.gfile.GFile(bert_config_file, "r") as reader:
        bc = StockBertConfig.from_json_string(reader.read())
        bert_params = map_stock_config_to_params(bc)
        bert_params.adapter_size = adapter_size
        bert = BertModelLayer.from_params(bert_params, name="bert")
        
    input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name="input_ids")
    bert_output = bert(input_ids)

    print("bert shape", bert_output.shape)

    cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)
    cls_out = keras.layers.Dropout(0.5)(cls_out)
    logits = keras.layers.Dense(units=768, activation="tanh")(cls_out)
    logits = keras.layers.Dropout(0.5)(logits)
    logits = keras.layers.Dense(units=len(CLASSES), activation="softmax")(logits)

    model = keras.Model(inputs=input_ids, outputs=logits)
    model.build(input_shape=(None, max_seq_len))
    
    load_stock_weights(bert, bert_ckpt_file)

    # freeze weights since we are fine-tuning
    if adapter_size is not None:
        freeze_bert_layers(bert)
        
    return model


def to_model_ds(tfrecord_path, 
                batch_size=BATCH_SIZE, 
                max_seq_len=MAX_SEQ_LEN,
                shuffle=False,
                drop_remainder=True):
#    ds.cache()
#    ds = ds.map(create_pad_example_fn(pad_len=max_seq_len))
#    ds.repeat()
#    ds = ds.batch(batch_size, drop_remainder=drop_remainder)
#    return ds

    filenames = glob.glob('{}/*.tfrecord'.format(tfrecord_path))
   
    ds = tfrecord_to_dataset(filenames)
    ds = ds.cache()
    ds = ds.map(create_pad_example_fn(pad_len=MAX_SEQ_LEN))
    if shuffle:
        ds = ds.shuffle(buffer_size=25000, seed=4711, reshuffle_each_iteration=True)
    ds = ds.repeat()
    ds = ds.batch(BATCH_SIZE, drop_remainder=True)

    return ds
 
    
if __name__ == '__main__':
    parser = argparse.ArgumentParser()

#    parser.add_argument('--model-type', type=str, default='bert')
#    parser.add_argument('--model-name', type=str, default='bert-base-cased')
    parser.add_argument('--train-data', type=str, default=os.environ['SM_CHANNEL_TRAIN'])
    parser.add_argument('--validation-data', type=str, default=os.environ['SM_CHANNEL_VALIDATION'])
    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])
    parser.add_argument('--hosts', type=list, default=json.loads(os.environ['SM_HOSTS']))
    parser.add_argument('--current-host', type=str, default=os.environ['SM_CURRENT_HOST'])
    parser.add_argument('--num-gpus', type=int, default=os.environ['SM_NUM_GPUS'])

    args, _ = parser.parse_known_args()   
#    model_type = args.model_type
#    model_name = args.model_name
    train_data = args.train_data
    validation_data = args.validation_data
    model_dir = args.model_dir
    hosts = args.hosts
    current_host = args.current_host
    num_gpus = args.num_gpus

    data_dir = './data'

    convert_to_tfrecord_file(train_tfrecord_file,
                             data_dir,
                             serialize_to_tfrecord)
#    convert_to_tfrecord_file(test_tfrecord_file,
#                             data_dir,
#                             serialize_to_tfrecord)

    model = create_model(max_seq_len=MAX_SEQ_LEN, bert_ckpt_file=bert_ckpt_file, adapter_size=64)

    model.summary()

    model.compile(
      optimizer=keras.optimizers.Adam(1e-5),
      loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      metrics=[keras.metrics.SparseCategoricalAccuracy(name="acc")]
    )

    log_dir = "log/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%s")
    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=log_dir)

#    train_ds = tfrecord_to_dataset([train_tfrecord_file])
#    train_ds = train_ds.map(create_pad_example_fn(pad_len=MAX_SEQ_LEN))
#    train_ds = train_ds.cache()
#    train_ds = train_ds.shuffle(buffer_size=2500, seed=4711, reshuffle_each_iteration=True)
#    train_ds = train_ds.repeat()
#    train_ds = train_ds.batch(BATCH_SIZE, drop_remainder=True)

    train_ds = to_model_ds(train_data, shuffle=True)

    history = model.fit(
      train_ds,
      batch_size=BATCH_SIZE,
      shuffle=True,
      steps_per_epoch=10,
      epochs=1,
     callbacks=[
    # create_learning_rate_scheduler(max_learn_rate=1e-5,
    #                                             end_learn_rate=1e-7,
    #                                             warmup_epoch_count=20,
    #                                             total_epoch_count=total_epoch_count),
    #             keras.callbacks.EarlyStopping(patience=20, restore_best_weights=True),
                 tensorboard_callback]
    )

    _, train_acc = model.evaluate(to_model_ds(train_data), steps=10)
    _, test_acc = model.evaluate(to_model_ds(test_test), steps=10)

    print("train acc", train_acc)
    print(" test acc", test_acc)

    #y_pred = model.predict(features.test_x).argmax(axis=-1)

    #print(classification_report(features.test_y, y_pred)) #, target_names=CLASSES))

    #cm = confusion_matrix(features.test_y, y_pred)
    #df_cm = pd.DataFrame(cm, index=CLASSES, columns=CLASSES)

    sentences = [
      "This is just OK.",
      "This sucks.",
      "This is great."
    ]

    pred_tokens = map(tokenizer.tokenize, sentences)
    pred_tokens = map(lambda tok: ["[CLS]"] + tok + ["[SEP]"], pred_tokens)
    pred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))

    pred_token_ids = map(lambda tids: tids +[0]*(MAX_SEQ_LEN-len(tids)),pred_token_ids)
    pred_token_ids = np.array(list(pred_token_ids))

    predictions = model.predict(pred_token_ids).argmax(axis=-1)

    for review_body, star_rating in zip(sentences, predictions):
      print("review_body:", review_body, "\star_rating:", CLASSES[star_rating])
      print()

    model.save('/opt/ml/model/0/', save_format='tf')
    model.save('/opt/ml/model/bert_reviews.h5')
    model.save_weights('/opt/ml/model/bert_reviews_weights.h5')    
    
