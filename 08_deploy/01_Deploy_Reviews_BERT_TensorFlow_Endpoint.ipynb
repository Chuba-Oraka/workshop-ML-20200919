{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_job_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-training-2020-04-30-03-09-12-331\n"
     ]
    }
   ],
   "source": [
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "# Following this example:\n",
    "#    https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/tensorflow/deploying_tensorflow_serving.rst#making-predictions-against-a-sagemaker-endpoint\n",
    "    \n",
    "# For network isolation mode:\n",
    "#    If you are working in a network-isolation situation or if you don't \n",
    "#    want to install dependencies at runtime every time your endpoint \n",
    "#    starts or a batch transform job runs, you might want to put pre-downloaded \n",
    "#    dependencies under a lib directory and this directory as dependency. The container \n",
    "#    adds the modules to the Python path. Note that if both lib and requirements.txt are\n",
    "#    present in the model archive, the requirements.txt is ignored:\n",
    "\n",
    "# If you change SAGEMAKER_TFS_DEFAULT_MODEL_NAME to something other than 'saved_model', you may see the dreaded ping error in the logs error\n",
    "env = {\n",
    "  'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'saved_model' # <== change this when using multi-model,\n",
    "                                                    #     but watch out for the dreaded ping/ error \n",
    "                                                    #     if the model name doesn't exist\n",
    "}\n",
    "\n",
    "model = Model(entry_point='inference.py',\n",
    "              source_dir='src_inference',\n",
    "              model_data='s3://{}/{}/output/model.tar.gz'.format(bucket, training_job_name),\n",
    "              role=role,\n",
    "              framework_version=\"2.1.0\",\n",
    "              env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sagemaker.tensorflow.serving.Model'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictor = model.deploy(initial_instance_count=1, \n",
    "                         instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the training model (not useful here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tar -xvzf ./model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy the Inference Model (useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$bucket/$predictor.endpoint/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir ./tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_single_example(text, \n",
    "                           max_seq_length,\n",
    "                           tokenizer):\n",
    "\n",
    "  tokens_a = tokenizer.tokenize(text)\n",
    "  print('Length of tokens_a {}'.format(len(tokens_a)))\n",
    "\n",
    "  # Account for [CLS] and [SEP] with \"- 2\"\n",
    "  if len(tokens_a) > max_seq_length - 2:\n",
    "    tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "  # The convention in BERT is:\n",
    "  # (a) For sequence pairs:\n",
    "  #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "  #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1  \n",
    "  # (b) For single sequences:\n",
    "  #  tokens:   [CLS] the dog is hairy . [SEP]  \n",
    "  #  type_ids: 0     0   0   0  0     0 0\n",
    "  #\n",
    "  # Where \"type_ids\" are used to indicate whether this is the first\n",
    "  # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "  # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "  # embedding vector (and position vector). This is not *strictly* necessary\n",
    "  # since the [SEP] token unambiguously separates the sequences, but it makes  \n",
    "  # it easier for the model to learn the concept of sequences.\n",
    "  #  \n",
    "  # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "  # used as the \"sentence vector\". Note that this only makes sense because  \n",
    "  # the entire model is fine-tuned.\n",
    "  #\n",
    "  tokens = []  \n",
    "  segment_ids = []\n",
    "  tokens.append(\"[CLS]\")\n",
    "  segment_ids.append(0)\n",
    "  for token in tokens_a:\n",
    "    tokens.append(token)\n",
    "    segment_ids.append(0)  \n",
    "  tokens.append(\"[SEP]\")\n",
    "  segment_ids.append(0)\n",
    "\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    \n",
    "  # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "  # tokens are attended to.\n",
    "  input_mask = [1] * len(input_ids)\n",
    "\n",
    "  # Zero-pad up to the sequence length.\n",
    "  while len(input_ids) < max_seq_length:\n",
    "    input_ids.append(0)\n",
    "    input_mask.append(0)\n",
    "    segment_ids.append(0)\n",
    "\n",
    "  assert len(input_ids) == max_seq_length\n",
    "  assert len(input_mask) == max_seq_length\n",
    "  assert len(segment_ids) == max_seq_length\n",
    "\n",
    "  return input_ids, input_mask, segment_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_mask, segment_ids = convert_single_example(text=\"\"\"I loved it!  I will recommend this to everyone.\"\"\",\n",
    "                                                            max_seq_length=128,\n",
    "                                                            tokenizer=tokenizer)\n",
    "\n",
    "instances = [{\"input_ids\": input_ids, \n",
    "              \"input_mask\": input_mask, \n",
    "              \"segment_ids\": segment_ids}]\n",
    "data = {\"instances\": instances}\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "log_probabilities = predictor.predict(data)['predictions'][0]\n",
    "print('Log Probabilities: {}'.format(log_probabilities))\n",
    "\n",
    "softmax = tf.nn.softmax(log_probabilities)\n",
    "print('Softmax: {}'.format(softmax))\n",
    "\n",
    "predicted_class_idx = tf.argmax(softmax, axis=-1, output_type=tf.int32)\n",
    "print('Predicted Class Idx: {}'.format(predicted_class_idx))\n",
    "\n",
    "classes = [1, 2, 3, 4, 5]\n",
    "\n",
    "predicted_class = classes[predicted_class_idx]\n",
    "print('Predicted Class: {}'.format(predicted_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, input_mask, segment_ids = convert_single_example(text=\"\"\"Really bad.  I hope they don't make this anymore.\"\"\",\n",
    "                                                            max_seq_length=128,\n",
    "                                                            tokenizer=tokenizer)\n",
    "\n",
    "instances = [{\"input_ids\": input_ids, \n",
    "              \"input_mask\": input_mask, \n",
    "              \"segment_ids\": segment_ids}]\n",
    "data = {\"instances\": instances}\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "log_probabilities = predictor.predict(data)['predictions'][0]\n",
    "print('Log Probabilities: {}'.format(log_probabilities))\n",
    "\n",
    "softmax = tf.nn.softmax(log_probabilities)\n",
    "print('Softmax: {}'.format(softmax))\n",
    "\n",
    "predicted_class_idx = tf.argmax(softmax, axis=-1, output_type=tf.int32)\n",
    "print('Predicted Class Idx: {}'.format(predicted_class_idx))\n",
    "\n",
    "classes = [1, 2, 3, 4, 5]\n",
    "\n",
    "predicted_class = classes[predicted_class_idx]\n",
    "print('Predicted Class: {}'.format(predicted_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
