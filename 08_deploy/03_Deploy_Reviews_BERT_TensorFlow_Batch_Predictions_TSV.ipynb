{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: sagemaker==1.56.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.56.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.56.1) (0.1.5)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.56.1) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.56.1) (1.5.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3>=1.10.44 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.56.1) (1.12.39)\n",
      "Requirement already satisfied, skipping upgrade: smdebug-rulesconfig==0.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.56.1) (0.1.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.56.1) (1.18.3)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.56.1) (20.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker==1.56.1) (3.11.3)\n",
      "Requirement already satisfied, skipping upgrade: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker==1.56.1) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker==1.56.1) (3.0.0)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.16.0,>=1.15.39 in /home/ec2-user/.local/lib/python3.6/site-packages (from boto3>=1.10.44->sagemaker==1.56.1) (1.15.49)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.10.44->sagemaker==1.56.1) (0.3.3)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.10.44->sagemaker==1.56.1) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker==1.56.1) (2.2.0)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker==1.56.1) (46.1.3)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.10.44->sagemaker==1.56.1) (1.23)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.10.44->sagemaker==1.56.1) (0.14)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.10.44->sagemaker==1.56.1) (2.7.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker==1.56.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\r\n",
      "---------------------------------- ----------\r\n",
      "absl-py                            0.9.0\r\n",
      "alabaster                          0.7.10\r\n",
      "anaconda-client                    1.6.14\r\n",
      "anaconda-project                   0.8.2\r\n",
      "asn1crypto                         0.24.0\r\n",
      "astor                              0.8.1\r\n",
      "astroid                            1.6.3\r\n",
      "astropy                            3.0.2\r\n",
      "attrs                              18.1.0\r\n",
      "Automat                            0.3.0\r\n",
      "autovizwidget                      0.15.0\r\n",
      "awscli                             1.18.49\r\n",
      "Babel                              2.5.3\r\n",
      "backcall                           0.1.0\r\n",
      "backports.shutil-get-terminal-size 1.0.0\r\n",
      "bcrypt                             3.1.7\r\n",
      "beautifulsoup4                     4.6.0\r\n",
      "bitarray                           0.8.1\r\n",
      "bkcharts                           0.2\r\n",
      "blaze                              0.11.3\r\n",
      "bleach                             2.1.3\r\n",
      "bokeh                              1.0.4\r\n",
      "boto                               2.48.0\r\n",
      "boto3                              1.12.39\r\n",
      "botocore                           1.15.49\r\n",
      "Bottleneck                         1.2.1\r\n",
      "cached-property                    1.5.1\r\n",
      "cachetools                         4.1.0\r\n",
      "certifi                            2019.11.28\r\n",
      "cffi                               1.11.5\r\n",
      "characteristic                     14.3.0\r\n",
      "chardet                            3.0.4\r\n",
      "click                              6.7\r\n",
      "cloudpickle                        0.5.3\r\n",
      "clyent                             1.2.2\r\n",
      "colorama                           0.3.9\r\n",
      "contextlib2                        0.5.5\r\n",
      "cryptography                       2.9\r\n",
      "cycler                             0.10.0\r\n",
      "Cython                             0.28.4\r\n",
      "cytoolz                            0.9.0.1\r\n",
      "dask                               0.17.5\r\n",
      "dataclasses                        0.7\r\n",
      "datashape                          0.5.4\r\n",
      "decorator                          4.3.0\r\n",
      "defusedxml                         0.6.0\r\n",
      "distributed                        1.21.8\r\n",
      "docker                             4.2.0\r\n",
      "docker-compose                     1.25.5\r\n",
      "dockerpty                          0.4.1\r\n",
      "docopt                             0.6.2\r\n",
      "docutils                           0.14\r\n",
      "entrypoints                        0.2.3\r\n",
      "enum34                             1.1.9\r\n",
      "environment-kernels                1.1.1\r\n",
      "et-xmlfile                         1.0.1\r\n",
      "fastcache                          1.0.2\r\n",
      "filelock                           3.0.4\r\n",
      "Flask                              1.0.2\r\n",
      "Flask-Cors                         3.0.4\r\n",
      "gast                               0.2.2\r\n",
      "gevent                             1.3.0\r\n",
      "glob2                              0.6\r\n",
      "gmpy2                              2.0.8\r\n",
      "google-auth                        1.14.1\r\n",
      "google-auth-oauthlib               0.4.1\r\n",
      "google-pasta                       0.2.0\r\n",
      "greenlet                           0.4.13\r\n",
      "grpcio                             1.28.1\r\n",
      "h5py                               2.8.0\r\n",
      "hdijupyterutils                    0.15.0\r\n",
      "heapdict                           1.0.0\r\n",
      "html5lib                           1.0.1\r\n",
      "idna                               2.6\r\n",
      "imageio                            2.3.0\r\n",
      "imagesize                          1.0.0\r\n",
      "importlib-metadata                 1.5.0\r\n",
      "ipykernel                          4.8.2\r\n",
      "ipyparallel                        6.2.2\r\n",
      "ipython                            6.4.0\r\n",
      "ipython-genutils                   0.2.0\r\n",
      "ipywidgets                         7.4.0\r\n",
      "isort                              4.3.4\r\n",
      "itsdangerous                       0.24\r\n",
      "jdcal                              1.4\r\n",
      "jedi                               0.12.0\r\n",
      "Jinja2                             2.10\r\n",
      "jmespath                           0.9.4\r\n",
      "joblib                             0.14.1\r\n",
      "jsonschema                         2.6.0\r\n",
      "jupyter                            1.0.0\r\n",
      "jupyter-client                     5.2.3\r\n",
      "jupyter-console                    5.2.0\r\n",
      "jupyter-core                       4.4.0\r\n",
      "jupyterlab                         0.32.1\r\n",
      "jupyterlab-launcher                0.10.5\r\n",
      "Keras-Applications                 1.0.8\r\n",
      "Keras-Preprocessing                1.1.0\r\n",
      "kiwisolver                         1.0.1\r\n",
      "lazy-object-proxy                  1.3.1\r\n",
      "llvmlite                           0.23.1\r\n",
      "locket                             0.2.0\r\n",
      "lxml                               4.2.1\r\n",
      "Markdown                           3.2.1\r\n",
      "MarkupSafe                         1.0\r\n",
      "matplotlib                         3.0.3\r\n",
      "mccabe                             0.6.1\r\n",
      "mistune                            0.8.3\r\n",
      "mkl-fft                            1.0.0\r\n",
      "mkl-random                         1.0.1\r\n",
      "mock                               4.0.1\r\n",
      "more-itertools                     4.1.0\r\n",
      "mpmath                             1.0.0\r\n",
      "msgpack                            0.6.0\r\n",
      "msgpack-python                     0.5.6\r\n",
      "multipledispatch                   0.5.0\r\n",
      "nb-conda                           2.2.1\r\n",
      "nb-conda-kernels                   2.2.2\r\n",
      "nbconvert                          5.4.1\r\n",
      "nbformat                           4.4.0\r\n",
      "networkx                           2.1\r\n",
      "nltk                               3.3\r\n",
      "nose                               1.3.7\r\n",
      "notebook                           5.5.0\r\n",
      "numba                              0.38.0\r\n",
      "numexpr                            2.6.5\r\n",
      "numpy                              1.18.3\r\n",
      "numpydoc                           0.8.0\r\n",
      "oauthlib                           3.1.0\r\n",
      "odo                                0.5.1\r\n",
      "olefile                            0.45.1\r\n",
      "opencv-python                      3.4.2.17\r\n",
      "openpyxl                           2.5.3\r\n",
      "opt-einsum                         3.2.1\r\n",
      "packaging                          20.1\r\n",
      "pandas                             0.24.2\r\n",
      "pandocfilters                      1.4.2\r\n",
      "paramiko                           2.7.1\r\n",
      "parso                              0.2.0\r\n",
      "partd                              0.3.8\r\n",
      "path.py                            11.0.1\r\n",
      "pathlib2                           2.3.2\r\n",
      "patsy                              0.5.0\r\n",
      "pep8                               1.7.1\r\n",
      "pexpect                            4.5.0\r\n",
      "pickleshare                        0.7.4\r\n",
      "Pillow                             5.1.0\r\n",
      "pip                                20.1\r\n",
      "pkginfo                            1.4.2\r\n",
      "plotly                             4.5.2\r\n",
      "pluggy                             0.6.0\r\n",
      "ply                                3.11\r\n",
      "prompt-toolkit                     1.0.15\r\n",
      "protobuf                           3.11.3\r\n",
      "protobuf3-to-dict                  0.1.5\r\n",
      "psutil                             5.4.5\r\n",
      "psycopg2                           2.7.5\r\n",
      "ptyprocess                         0.5.2\r\n",
      "py                                 1.5.3\r\n",
      "py4j                               0.10.7\r\n",
      "pyasn1                             0.4.8\r\n",
      "pyasn1-modules                     0.2.8\r\n",
      "pycodestyle                        2.4.0\r\n",
      "pycosat                            0.6.3\r\n",
      "pycparser                          2.18\r\n",
      "pycrypto                           2.6.1\r\n",
      "pycurl                             7.43.0.1\r\n",
      "pyflakes                           1.6.0\r\n",
      "pygal                              2.4.0\r\n",
      "Pygments                           2.2.0\r\n",
      "pykerberos                         1.2.1\r\n",
      "pylint                             1.8.4\r\n",
      "PyNaCl                             1.3.0\r\n",
      "pyodbc                             4.0.23\r\n",
      "pyOpenSSL                          18.0.0\r\n",
      "pyparsing                          2.2.0\r\n",
      "PySocks                            1.6.8\r\n",
      "pyspark                            2.3.4\r\n",
      "pytest                             3.5.1\r\n",
      "pytest-arraydiff                   0.2\r\n",
      "pytest-astropy                     0.3.0\r\n",
      "pytest-doctestplus                 0.1.3\r\n",
      "pytest-openfiles                   0.3.0\r\n",
      "pytest-remotedata                  0.2.1\r\n",
      "python-dateutil                    2.7.3\r\n",
      "pytz                               2018.4\r\n",
      "PyWavelets                         0.5.2\r\n",
      "PyYAML                             5.3.1\r\n",
      "pyzmq                              17.0.0\r\n",
      "QtAwesome                          0.4.4\r\n",
      "qtconsole                          4.3.1\r\n",
      "QtPy                               1.4.1\r\n",
      "regex                              2020.4.4\r\n",
      "requests                           2.23.0\r\n",
      "requests-kerberos                  0.12.0\r\n",
      "requests-oauthlib                  1.3.0\r\n",
      "retrying                           1.3.3\r\n",
      "rope                               0.10.7\r\n",
      "rsa                                3.4.2\r\n",
      "ruamel-yaml                        0.15.35\r\n",
      "s3fs                               0.1.5\r\n",
      "s3transfer                         0.3.3\r\n",
      "sacremoses                         0.0.41\r\n",
      "sagemaker                          1.56.1\r\n",
      "sagemaker-experiments              0.1.11\r\n",
      "sagemaker-pyspark                  1.3.0\r\n",
      "scikit-image                       0.13.1\r\n",
      "scikit-learn                       0.20.3\r\n",
      "scipy                              1.4.1\r\n",
      "seaborn                            0.8.1\r\n",
      "Send2Trash                         1.5.0\r\n",
      "sentencepiece                      0.1.86\r\n",
      "setuptools                         46.1.3\r\n",
      "simplegeneric                      0.8.1\r\n",
      "singledispatch                     3.4.0.3\r\n",
      "six                                1.14.0\r\n",
      "smdebug                            0.7.2\r\n",
      "smdebug-rulesconfig                0.1.2\r\n",
      "snowballstemmer                    1.2.1\r\n",
      "sortedcollections                  0.6.1\r\n",
      "sortedcontainers                   1.5.10\r\n",
      "sparkmagic                         0.12.5\r\n",
      "Sphinx                             1.7.4\r\n",
      "sphinxcontrib-websupport           1.0.1\r\n",
      "spyder                             3.2.8\r\n",
      "SQLAlchemy                         1.2.11\r\n",
      "statsmodels                        0.9.0\r\n",
      "stepfunctions                      1.0.0.8\r\n",
      "sympy                              1.1.1\r\n",
      "tables                             3.4.3\r\n",
      "tabulate                           0.8.7\r\n",
      "TBB                                0.1\r\n",
      "tblib                              1.3.2\r\n",
      "tensorboard                        2.1.1\r\n",
      "tensorflow                         2.1.0\r\n",
      "tensorflow-estimator               2.1.0\r\n",
      "termcolor                          1.1.0\r\n",
      "terminado                          0.8.1\r\n",
      "testpath                           0.3.1\r\n",
      "texttable                          1.6.2\r\n",
      "tokenizers                         0.5.2\r\n",
      "toolz                              0.9.0\r\n",
      "tornado                            5.0.2\r\n",
      "tqdm                               4.45.0\r\n",
      "traitlets                          4.3.2\r\n",
      "transformers                       2.8.0\r\n",
      "typing                             3.6.4\r\n",
      "unicodecsv                         0.14.1\r\n",
      "urllib3                            1.23\r\n",
      "wcwidth                            0.1.7\r\n",
      "webencodings                       0.5.1\r\n",
      "websocket-client                   0.57.0\r\n",
      "Werkzeug                           0.14.1\r\n",
      "wheel                              0.31.1\r\n",
      "widgetsnbextension                 3.4.2\r\n",
      "wrapt                              1.12.1\r\n",
      "xlrd                               1.1.0\r\n",
      "XlsxWriter                         1.0.4\r\n",
      "xlwt                               1.3.0\r\n",
      "zict                               0.1.3\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipp                               3.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Batch Transform Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-training-2020-04-30-03-09-12-331\n"
     ]
    }
   ],
   "source": [
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-04-30-03-09-12-331/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow/\n",
      "tensorflow/saved_model/\n",
      "tensorflow/saved_model/0/\n",
      "tensorflow/saved_model/0/saved_model.pb\n",
      "tensorflow/saved_model/0/variables/\n",
      "tensorflow/saved_model/0/variables/variables.index\n",
      "tensorflow/saved_model/0/variables/variables.data-00000-of-00002\n",
      "tensorflow/saved_model/0/variables/variables.data-00001-of-00002\n",
      "tensorflow/saved_model/0/assets/\n",
      "transformers/\n",
      "transformers/fine-tuned/\n",
      "transformers/fine-tuned/config.json\n",
      "transformers/fine-tuned/tf_model.h5\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-06 02:52:56.449285: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer.so.6'; dlerror: libnvinfer.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-05-06 02:52:56.449423: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libnvinfer_plugin.so.6'; dlerror: libnvinfer_plugin.so.6: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.0/lib64:/usr/local/cuda-10.0/extras/CUPTI/lib64:/usr/local/cuda-10.0/lib:/usr/local/cuda-10.0/efa/lib:/opt/amazon/efa/lib:/opt/amazon/efa/lib64:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:/usr/lib64/openmpi/lib/:/usr/local/lib:/usr/lib:/usr/local/mpi/lib:/lib/:\n",
      "2020-05-06 02:52:56.449439: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:30] Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['__saved_model_init_op']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['__saved_model_init_op'] tensor_info:\n",
      "        dtype: DT_INVALID\n",
      "        shape: unknown_rank\n",
      "        name: NoOp\n",
      "  Method name is: \n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['input_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 128)\n",
      "        name: serving_default_input_ids:0\n",
      "    inputs['input_mask'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 128)\n",
      "        name: serving_default_input_mask:0\n",
      "    inputs['segment_ids'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1, 128)\n",
      "        name: serving_default_segment_ids:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['output_1'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 5)\n",
      "        name: StatefulPartitionedCall:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "\n",
      "Defined Functions:\n",
      "  Function Name: '__call__'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "\n",
      "  Function Name: '_default_save_signature'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "\n",
      "  Function Name: 'call_and_return_all_conditional_losses'\n",
      "    Option #1\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #2\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #3\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n",
      "    Option #4\n",
      "      Callable with:\n",
      "        Argument #1\n",
      "          DType: dict\n",
      "          Value: {'segment_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/segment_ids'), 'input_mask': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_mask'), 'input_ids': TensorSpec(shape=(None, 128), dtype=tf.int64, name='inputs/input_ids')}\n",
      "        Named Argument #1\n",
      "          DType: str\n",
      "          Value: ['t', 'r', 'a', 'i', 'n', 'i', 'n', 'g']\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --all --dir ./tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\r\n",
      "classes=[\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "max_seq_length=\u001b[34m128\u001b[39;49;00m\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_handler\u001b[39;49;00m(data, context):\r\n",
      "    transformed_instances = []\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(data))\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(data)\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m instance \u001b[35min\u001b[39;49;00m data:\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(instance))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(instance)\r\n",
      "\r\n",
      "        data_str = instance.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(data_str))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(data_str)\r\n",
      "\r\n",
      "        data_str_split = data_str.split(\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(data_str_split))\r\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[36mlen\u001b[39;49;00m(data_str_split) >= \u001b[34m13\u001b[39;49;00m):\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(data_str_split[\u001b[34m13\u001b[39;49;00m])\r\n",
      "\r\n",
      "        tokens_a = tokenizer.tokenize(data_str_split[\u001b[34m13\u001b[39;49;00m])\r\n",
      "\r\n",
      "        \u001b[37m# Account for [CLS] and [SEP] with \"- 2\"\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(tokens_a) > max_seq_length - \u001b[34m2\u001b[39;49;00m:\r\n",
      "            tokens_a = tokens_a[\u001b[34m0\u001b[39;49;00m:(max_seq_length - \u001b[34m2\u001b[39;49;00m)]\r\n",
      "\r\n",
      "        tokens = []  \r\n",
      "        segment_ids = []\r\n",
      "        tokens.append(\u001b[33m\"\u001b[39;49;00m\u001b[33m[CLS]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        segment_ids.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[34mfor\u001b[39;49;00m token \u001b[35min\u001b[39;49;00m tokens_a:\r\n",
      "            tokens.append(token)\r\n",
      "            segment_ids.append(\u001b[34m0\u001b[39;49;00m)  \r\n",
      "        tokens.append(\u001b[33m\"\u001b[39;49;00m\u001b[33m[SEP]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        segment_ids.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
      "\r\n",
      "        input_mask = [\u001b[34m1\u001b[39;49;00m] * \u001b[36mlen\u001b[39;49;00m(input_ids)\r\n",
      "\r\n",
      "        \u001b[37m# Zero-pad up to the sequence length.\u001b[39;49;00m\r\n",
      "        \u001b[34mwhile\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(input_ids) < max_seq_length:\r\n",
      "            input_ids.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "            input_mask.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "            segment_ids.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(input_ids) == max_seq_length\r\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(input_mask) == max_seq_length\r\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(segment_ids) == max_seq_length\r\n",
      "\r\n",
      "        transformed_instance = { \r\n",
      "                                 \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: input_ids, \r\n",
      "                                 \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: input_mask, \r\n",
      "                                 \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: segment_ids\r\n",
      "                               }\r\n",
      "    \r\n",
      "        transformed_instances.append(transformed_instance)\r\n",
      "\r\n",
      "    transformed_data = {\u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: transformed_instances}\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(transformed_data)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m json.dumps(transformed_data)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_handler\u001b[39;49;00m(response, context):\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(response))\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(response)\r\n",
      "\r\n",
      "    response_json = response.json()\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(response_json))\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(response_json)\r\n",
      "\r\n",
      "    log_probabilities = response_json[\u001b[33m\"\u001b[39;49;00m\u001b[33mpredictions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "\r\n",
      "    predicted_classes = []\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m log_probability \u001b[35min\u001b[39;49;00m log_probabilities:\r\n",
      "        softmax = tf.nn.softmax(log_probability)    \r\n",
      "        predicted_class_idx = tf.argmax(softmax, axis=-\u001b[34m1\u001b[39;49;00m, output_type=tf.int32)\r\n",
      "        predicted_class = classes[predicted_class_idx]\r\n",
      "        predicted_classes.append(predicted_class)\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(predicted_classes)\r\n",
      "    predicted_classes_json = json.dumps(predicted_classes)\r\n",
      "\r\n",
      "    response_content_type = context.accept_header\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m predicted_classes_json, response_content_type\r\n",
      "\r\n",
      "    \r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\r\n",
      "\r\n",
      "    f = \u001b[36mopen\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mdata/amazon_reviews_us_Digital_Software_v1_00_noheader_predict.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mr\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \r\n",
      "    lines = f.readlines()\r\n",
      "\r\n",
      "    instances = []\r\n",
      "\r\n",
      "    i = \u001b[34m0\u001b[39;49;00m\r\n",
      "    \u001b[34mfor\u001b[39;49;00m line \u001b[35min\u001b[39;49;00m lines:\r\n",
      "        instances.append(line.replace(\u001b[33m'\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "        i = i + \u001b[34m1\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m i == \u001b[34m3\u001b[39;49;00m: \r\n",
      "            \u001b[34mbreak\u001b[39;49;00m \r\n",
      "\r\n",
      "\u001b[37m#    instances = [\"This is great!\", \u001b[39;49;00m\r\n",
      "\u001b[37m#                 \"This is terrible.\"]\u001b[39;49;00m\r\n",
      "\r\n",
      "    predicted_classes = input_handler(data=instances, context=\u001b[36mNone\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(predicted_classes)\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./src_csv/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "# If you change SAGEMAKER_TFS_DEFAULT_MODEL_NAME to something other than 'saved_model', you may see the dreaded ping error in the logs error\n",
    "batch_env = {\n",
    "  'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'saved_model', # <== change this when using multi-model,\n",
    "                                                     #     but watch out for the dreaded ping/ error \n",
    "                                                     #     if the model name doesn't exist\n",
    "  'SAGEMAKER_TFS_ENABLE_BATCHING': 'true',\n",
    "  'SAGEMAKER_TFS_BATCH_TIMEOUT_MICROS': '50000',\n",
    "  'SAGEMAKER_TFS_MAX_BATCH_SIZE': '100000'\n",
    "}\n",
    "\n",
    "batch_model = Model(entry_point='inference.py', # <== This cannot change. See https://github.com/aws/sagemaker-python-sdk/issues/1451\n",
    "                    source_dir='src_csv',       \n",
    "                    model_data='s3://{}/{}/output/model.tar.gz'.format(bucket, training_job_name),\n",
    "                    role=role,\n",
    "                    framework_version='2.1.0',\n",
    "                    env=batch_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_predictor = batch_model.transformer(instance_count=1,\n",
    "                                          strategy='MultiRecord', \n",
    "                                          instance_type='ml.c5.18xlarge',\n",
    "                                          accept='text/csv',\n",
    "                                          assemble_with='Line',\n",
    "                                          max_payload=100, # This is in Megabytes (not number of records)\n",
    "                                          env=batch_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_csv_s3_uri = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictor.transform(data=predict_csv_s3_uri,\n",
    "                          split_type='Line',\n",
    "                          compression_type='Gzip',\n",
    "                          content_type='text/csv',\n",
    "                          experiment_config=None,\n",
    "                          wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region=us-east-1#/transform-jobs/tensorflow-inference-2020-05-06-03-21-3-2020-05-06-03-21-33-861?region=us-east-1&tab=Monitor\">Batch Prediction Job</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/transform-jobs/{}?region={}&tab=Monitor\">Batch Prediction Job</a></b>'.format(region, batch_predictor.latest_transform_job.job_name, region)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#logStream:group=/aws/sagemaker/TransformJobs;prefix=tensorflow-inference-2020-05-06-03-21-3-2020-05-06-03-21-33-861;streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TransformJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'.format(region, batch_predictor.latest_transform_job.job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>Review <a href=\"https://console.aws.amazon.com/s3/buckets/sagemaker-us-east-1-835319576252/tensorflow-inference-2020-05-06-03-21-3-2020-05-06-03-21-33-861/?region=us-east-1\">Batch Prediction S3 Output</a></b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/s3/buckets/{}/{}/?region={}\">Batch Prediction S3 Output</a></b>'.format(bucket, batch_predictor.latest_transform_job.job_name, region)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for batch prediction job: tensorflow-inference-2020-05-06-03-21-3-2020-05-06-03-21-33-861\n",
      "...................................................................*\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Transform job tensorflow-inference-2020-05-06-03-21-3-2020-05-06-03-21-33-861: Failed. Reason: AlgorithmError: See job logs for more information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-079b54dbb061>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Waiting for batch prediction job: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_transform_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbatch_predictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/transformer.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;34m\"\"\"Placeholder docstring\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_last_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_transform_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/transformer.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_transform_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mwait_for_transform_job\u001b[0;34m(self, job, poll)\u001b[0m\n\u001b[1;32m   2588\u001b[0m         \"\"\"\n\u001b[1;32m   2589\u001b[0m         \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_wait_until\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_transform_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TransformJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   2636\u001b[0m                 ),\n\u001b[1;32m   2637\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2638\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2639\u001b[0m             )\n\u001b[1;32m   2640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Transform job tensorflow-inference-2020-05-06-03-21-3-2020-05-06-03-21-33-861: Failed. Reason: AlgorithmError: See job logs for more information"
     ]
    }
   ],
   "source": [
    "print('Waiting for batch prediction job: ' + batch_predictor.latest_transform_job.job_name)\n",
    "\n",
    "batch_predictor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait Until the ^^ Batch Transform ^^ Completes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Output Data\n",
    "\n",
    "After the transform job has completed, download the output data from S3.\n",
    "\n",
    "For each file in the input data, we have a corresponding file with a \".out\" extension.  This .out file contains the predicted labels for each input row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the output data from S3 to local filesystem\n",
    "batch_prediction_output_s3_uri = batch_predictor.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "aws s3 cp --recursive $batch_prediction_output_s3_uri/ ./batch_prediction_output\n",
    "\n",
    "ls ./batch_prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "history": [],
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
