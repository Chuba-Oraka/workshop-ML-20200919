{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you train your model, you can deploy it to get predictions in one of two ways:\n",
    "\n",
    "* Deploy the model as an HTTPS endpoint using Amazon SageMaker hosting services.\n",
    "* Perform batch predictions for an entire dataset using Amazon SageMaker batch transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Model as an HTTPS Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amazon SageMaker provides model hosting services for model deployment, as shown in the following diagram. \n",
    "Amazon SageMaker provides an HTTPS endpoint where your machine learning model is available to provide inferences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/realtime_inference.png\" width=\"70%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "predictor = model.deploy(initial_instance_count=1,\n",
    "                         content_type='text/csv',\n",
    "                         instance_type='ml.t2.medium')\n",
    "```\n",
    "                            \n",
    "The `deploy()` method creates the deployable model, configures the Amazon SageMaker hosting services endpoint, and launches the endpoint to host the model. It also returns a `sagemaker.predictor.RealTimePredictor` object, which you can use to get inferences from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Levels Of Customization \n",
    "\n",
    "* Option 1: Pre-built code and pre-built algorithm container\n",
    "\n",
    "* Option 2: Bring your own code and pre-built framework container\n",
    "\n",
    "* Option 3: Bring your own code and custom container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Batch Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use batch transform when:\n",
    "* You want to get inferences for an entire dataset\n",
    "* You don't need a persistent endpoint that applications (for example, web or mobile apps) can call to get inferences\n",
    "* You don't need the subsecond latency that Amazon SageMaker hosted endpoints provide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/batch_inference.png\" width=\"80%\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# The location of the test dataset\n",
    "batch_input = 's3://{}/{}/test/examples'.format(bucket, prefix)\n",
    "\n",
    "# The location to store the results of the batch transform job\n",
    "batch_output = 's3://{}/{}/batch-inference'.format(bucket, prefix)\n",
    "\n",
    "transformer = model.transformer(instance_count=1, instance_type='ml.m4.xlarge', output_path=batch_output)\n",
    "\n",
    "transformer.transform(data=batch_input, data_type='S3Prefix', content_type='text/csv', split_type='Line')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
