{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Predictions\n",
    "![](img/batch_transform_tensorflow.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Batch Transform Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%store -r training_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-training-2020-05-22-22-14-01-963\n"
     ]
    }
   ],
   "source": [
    "print(training_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-835319576252/tensorflow-training-2020-05-22-22-14-01-963/output/model.tar.gz to ./model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://$bucket/$training_job_name/output/model.tar.gz ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!tar -xvzf ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir ./tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\r\n",
      "review_body_column_idx_tsv = \u001b[34m13\u001b[39;49;00m\r\n",
      "\r\n",
      "classes=[\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "\r\n",
      "max_seq_length=\u001b[34m128\u001b[39;49;00m\r\n",
      "\r\n",
      "tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_handler\u001b[39;49;00m(data, context):\r\n",
      "    transformed_instances = []\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(data))\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(data)\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m instance \u001b[35min\u001b[39;49;00m data:\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(instance))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(instance)\r\n",
      "\r\n",
      "        data_str = instance.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(data_str))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(data_str)\r\n",
      "\r\n",
      "        data_str_split = data_str.split(\u001b[33m'\u001b[39;49;00m\u001b[33m\\t\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[36mlen\u001b[39;49;00m(data_str_split))\r\n",
      "        \u001b[34mif\u001b[39;49;00m (\u001b[36mlen\u001b[39;49;00m(data_str_split) >= review_body_column_idx_tsv):\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(data_str_split[review_body_column_idx_tsv])\r\n",
      "\r\n",
      "        tokens_a = tokenizer.tokenize(data_str_split[review_body_column_idx_tsv])\r\n",
      "\r\n",
      "        \u001b[37m# Account for [CLS] and [SEP] with \"- 2\"\u001b[39;49;00m\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(tokens_a) > max_seq_length - \u001b[34m2\u001b[39;49;00m:\r\n",
      "            tokens_a = tokens_a[\u001b[34m0\u001b[39;49;00m:(max_seq_length - \u001b[34m2\u001b[39;49;00m)]\r\n",
      "\r\n",
      "        tokens = []  \r\n",
      "        segment_ids = []\r\n",
      "        tokens.append(\u001b[33m\"\u001b[39;49;00m\u001b[33m[CLS]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        segment_ids.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "        \u001b[34mfor\u001b[39;49;00m token \u001b[35min\u001b[39;49;00m tokens_a:\r\n",
      "            tokens.append(token)\r\n",
      "            segment_ids.append(\u001b[34m0\u001b[39;49;00m)  \r\n",
      "        tokens.append(\u001b[33m\"\u001b[39;49;00m\u001b[33m[SEP]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\r\n",
      "        segment_ids.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\r\n",
      "\r\n",
      "        input_mask = [\u001b[34m1\u001b[39;49;00m] * \u001b[36mlen\u001b[39;49;00m(input_ids)\r\n",
      "\r\n",
      "        \u001b[37m# Zero-pad up to the sequence length.\u001b[39;49;00m\r\n",
      "        \u001b[34mwhile\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(input_ids) < max_seq_length:\r\n",
      "            input_ids.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "            input_mask.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "            segment_ids.append(\u001b[34m0\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(input_ids) == max_seq_length\r\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(input_mask) == max_seq_length\r\n",
      "        \u001b[34massert\u001b[39;49;00m \u001b[36mlen\u001b[39;49;00m(segment_ids) == max_seq_length\r\n",
      "\r\n",
      "        transformed_instance = { \r\n",
      "                                 \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: input_ids, \r\n",
      "                                 \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: input_mask, \r\n",
      "                                 \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: segment_ids\r\n",
      "                               }\r\n",
      "    \r\n",
      "        transformed_instances.append(transformed_instance)\r\n",
      "\r\n",
      "    transformed_data = {\u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: transformed_instances}\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(transformed_data)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m json.dumps(transformed_data)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_handler\u001b[39;49;00m(response, context):\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(response))\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(response)\r\n",
      "\r\n",
      "    response_json = response.json()\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[36mtype\u001b[39;49;00m(response_json))\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(response_json)\r\n",
      "\r\n",
      "    log_probabilities = response_json[\u001b[33m\"\u001b[39;49;00m\u001b[33mpredictions\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m]\r\n",
      "\r\n",
      "    predicted_classes = []\r\n",
      "\r\n",
      "    \u001b[34mfor\u001b[39;49;00m log_probability \u001b[35min\u001b[39;49;00m log_probabilities:\r\n",
      "        softmax = tf.nn.softmax(log_probability)    \r\n",
      "        predicted_class_idx = tf.argmax(softmax, axis=-\u001b[34m1\u001b[39;49;00m, output_type=tf.int32)\r\n",
      "        predicted_class = classes[predicted_class_idx]\r\n",
      "        predicted_classes.append(predicted_class)\r\n",
      "\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(predicted_classes)\r\n",
      "    predicted_classes_json = json.dumps(predicted_classes)\r\n",
      "\r\n",
      "    response_content_type = context.accept_header\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m predicted_classes_json, response_content_type\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./src_tsv/inference.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "batch_env = {\n",
    "  'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'saved_model',\n",
    "  'SAGEMAKER_TFS_ENABLE_BATCHING': 'true',\n",
    "  'SAGEMAKER_TFS_BATCH_TIMEOUT_MICROS': '50000', # microseconds\n",
    "  'SAGEMAKER_TFS_MAX_BATCH_SIZE': '10000',\n",
    "  'SAGEMAKER_MODEL_SERVER_TIMEOUT': '3600' # Seconds\n",
    "}\n",
    "\n",
    "batch_model = Model(entry_point='inference.py',\n",
    "                    source_dir='src_tsv',       \n",
    "                    model_data='s3://{}/{}/output/model.tar.gz'.format(bucket, training_job_name),\n",
    "                    role=role,\n",
    "                    framework_version='2.1.0',\n",
    "                    env=batch_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To increase performance, you can increase the MaxConcurrentTransforms parameter.  Tune this on a single instance before trying to scale out the number of instances - especially if you have a small file count, the multiple instances can be a big waste.  Note that `max_concurrent_transforms * max_payload <= 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_predictor = batch_model.transformer(strategy='MultiRecord', \n",
    "                                          instance_type='ml.c5.18xlarge',\n",
    "                                          instance_count=1,\n",
    "                                          accept='text/csv',\n",
    "                                          assemble_with='Line',\n",
    "                                          max_concurrent_transforms=1,\n",
    "                                          max_payload=1, # This is in Megabytes (not number of records)\n",
    "                                          env=batch_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_csv_s3_uri = 's3://{}/amazon-reviews-pds/tsv/'.format(bucket)\n",
    "print(predict_csv_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive $predict_csv_s3_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predictor.transform(data=predict_csv_s3_uri,\n",
    "                          split_type='Line',\n",
    "                          compression_type='Gzip',\n",
    "                          content_type='text/csv',\n",
    "                          experiment_config=None,\n",
    "                          wait=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/sagemaker/home?region={}#/transform-jobs/{}?region={}&tab=Monitor\">Batch Prediction Job</a></b>'.format(region, batch_predictor.latest_transform_job.job_name, region)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/cloudwatch/home?region={}#logStream:group=/aws/sagemaker/TransformJobs;prefix={};streamFilter=typeLogStreamPrefix\">CloudWatch Logs</a></b>'.format(region, batch_predictor.latest_transform_job.job_name)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML('<b>Review <a href=\"https://console.aws.amazon.com/s3/buckets/{}/{}/?region={}\">Batch Prediction S3 Output</a></b>'.format(bucket, batch_predictor.latest_transform_job.job_name, region)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Waiting for batch prediction job: ' + batch_predictor.latest_transform_job.job_name)\n",
    "\n",
    "batch_predictor.wait(logs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the ^^ Batch Transform Job ^^ Completes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Output Data\n",
    "\n",
    "After the transform job has completed, download the output data from S3.\n",
    "\n",
    "For each file in the input data, we have a corresponding file with a \".out\" extension.  This .out file contains the predicted labels for each input row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the output data from S3 to local filesystem\n",
    "batch_prediction_output_s3_uri = batch_predictor.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp --recursive $batch_prediction_output_s3_uri/ batch_prediction_output/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls batch_prediction_output/"
   ]
  }
 ],
 "metadata": {
  "history": [],
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
