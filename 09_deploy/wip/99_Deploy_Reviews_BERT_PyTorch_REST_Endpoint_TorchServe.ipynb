{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    " \n",
    "* ## https://medium.com/analytics-vidhya/deploy-huggingface-s-bert-to-production-with-pytorch-serve-27b068026d18 \n",
    "\n",
    "* ## https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying our BERT PyTorch Model as REST EndPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==2.8.0\n",
    "!pip install -q torch==1.5.0 --upgrade --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchserve in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch-model-archiver in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.1b20200704)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchserve) (0.18.2)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchserve) (5.1.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchserve) (5.4.5)\n",
      "Requirement already satisfied: enum-compat in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch-model-archiver) (0.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone the TorchServe repository and install torch-model-archiver\n",
    "\n",
    "You'll use `torch-model-archiver` to create a model archive file (.mar). The .mar model archive file contains model checkpoints along with itâ€™s `state_dict` (dictionary object that maps each layer to its parameter tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./src_torchserve/serve/model-archiver\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch-model-archiver==0.1.1b20200704) (0.18.2)\n",
      "Requirement already satisfied: enum-compat in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch-model-archiver==0.1.1b20200704) (0.0.3)\n",
      "Building wheels for collected packages: torch-model-archiver\n",
      "  Building wheel for torch-model-archiver (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-model-archiver: filename=torch_model_archiver-0.1.1b20200704-py3-none-any.whl size=15785 sha256=2e7321e25af8b60b2eb4a71a4a2cabd0b3ac20540ae6e623f6274e84f385682e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/00/e2/f8/6382e4aa3a1a20fcfdc7aed73512e6bb8bd55ef9cd0a9c099d\n",
      "Successfully built torch-model-archiver\n",
      "Installing collected packages: torch-model-archiver\n",
      "  Attempting uninstall: torch-model-archiver\n",
      "    Found existing installation: torch-model-archiver 0.1.1b20200704\n",
      "    Uninstalling torch-model-archiver-0.1.1b20200704:\n",
      "      Successfully uninstalled torch-model-archiver-0.1.1b20200704\n",
      "Successfully installed torch-model-archiver-0.1.1b20200704\n"
     ]
    }
   ],
   "source": [
    "!pip install ./src_torchserve/serve/model-archiver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r s3_pytorch_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/models/pytorch/pytorch_model.pt\n"
     ]
    }
   ],
   "source": [
    "print(s3_pytorch_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r s3_transformer_pytorch_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/models/transformer-pytorch/\n"
     ]
    }
   ],
   "source": [
    "print(s3_transformer_pytorch_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-835319576252/models/transformer-pytorch/config.json to Transformer_model/config.json\n",
      "download: s3://sagemaker-us-east-1-835319576252/models/transformer-pytorch/pytorch_model.bin to Transformer_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $s3_transformer_pytorch_model_path ./Transformer_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TorchServe Model Archive File\n",
    "\n",
    "Once, setup_config.json, sample_text.txt and index_to_name.json are set properly, we can go ahead and package the model and start serving it. The artifacts realted to each operation mode (such as sample_text.txt, index_to_name.json) can be place in their respective folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !torch-model-archiver \n",
    "#    --model-name \"bert\" \\\n",
    "#    --version 1.0 \\\n",
    "#    --serialized-file ./bert_model/pytorch_model.bin \\\n",
    "#    --extra-files \"./bert_model/config.json\" \\\n",
    "#    --handler \"./transformers_classifier_torchserve_handler.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DistilBertForSequenceClassification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver \\\n",
    "    --model-name $model_name \\\n",
    "    --version 1.0 \\\n",
    "    --serialized-file Transformer_model/pytorch_model.bin \\\n",
    "    --handler ./src_torchserve/Transformer_handler_generalized.py \\\n",
    "    --extra-files \"./Transformer_model/config.json,./src_torchserve/setup_config.json,./src_torchserve/Seq_classification_artifacts/index_to_name.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DistilBertForSequenceClassification.mar\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./*.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering the Model on TorchServe and Running Inference\n",
    "\n",
    "To register the model on TorchServe using the above model archive file, we run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ./model_store\n",
    "!mv ./DistilBertForSequenceClassification.mar ./model_store/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchServe requires Java 11 which is not installed by default in SageMaker Notebook Instances\n",
    "https://tecadmin.net/install-java-on-amazon-linux/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# sudo amazon-linux-extras install java-openjdk11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash \n",
    "\n",
    "# torchserve \\\n",
    "# --start \\\n",
    "# --model-store ./model_store \\\n",
    "# --models distilbert-pytorch=DistilBertForSequenceClassification.mar &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run the inference using our registered model, open a new terminal and run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -X POST http://127.0.0.1:8080/predictions/distilbert-pytorch -T ./src_torchserve/Seq_classification_artifacts/sample_text.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Model for SageMaker Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload .mar to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchserve_mar = 'DistilBertForSequenceClassification.mar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/models/torchserve/DistilBertForSequenceClassification.mar\n"
     ]
    }
   ],
   "source": [
    "s3_torchserve_mar = 's3://{}/models/torchserve/{}'.format(bucket, torchserve_mar)\n",
    "print(s3_torchserve_mar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: model_store/DistilBertForSequenceClassification.mar to s3://sagemaker-us-east-1-835319576252/models/torchserve/DistilBertForSequenceClassification.mar\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./model_store/$torchserve_mar $s3_torchserve_mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_torchserve_mar' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_torchserve_mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model_store/DistilBertForSequenceClassification.mar\r\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz ./DistilBertForSequenceClassification.tar.gz \\\n",
    "    ./model_store/DistilBertForSequenceClassification.mar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_torchserve_tar = 's3://{}/models/torchserve/DistilBertForSequenceClassification.tar.gz'.format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./DistilBertForSequenceClassification.tar.gz to s3://sagemaker-us-east-1-835319576252/models/torchserve/DistilBertForSequenceClassification.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./DistilBertForSequenceClassification.tar.gz $s3_torchserve_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_torchserve_tar' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_torchserve_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Amazon ECR registry\n",
    "Create a new docker container registry for your torchserve container images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'torchserve' already exists in the registry with id '835319576252'\r\n"
     ]
    }
   ],
   "source": [
    "registry_name = 'torchserve'\n",
    "!aws ecr create-repository --repository-name {registry_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a TorchServe Docker container and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label = 'v1'\n",
    "image = f'{account_id}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  22.71MB\n",
      "Step 1/16 : FROM ubuntu:18.04\n",
      " ---> 8e4ce0a6ce69\n",
      "Step 2/16 : ENV PYTHONUNBUFFERED TRUE\n",
      " ---> Using cache\n",
      " ---> c98cb06ff9fc\n",
      "Step 3/16 : RUN apt-get update &&     DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y     fakeroot     ca-certificates     dpkg-dev     g++     python3-dev     openjdk-11-jdk     curl     vim     && rm -rf /var/lib/apt/lists/*     && cd /tmp     && curl -O https://bootstrap.pypa.io/get-pip.py     && python3 get-pip.py\n",
      " ---> Using cache\n",
      " ---> 953456cd5d70\n",
      "Step 4/16 : RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
      " ---> Using cache\n",
      " ---> 7131decc103f\n",
      "Step 5/16 : RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
      " ---> Using cache\n",
      " ---> 0192cbc4a2d5\n",
      "Step 6/16 : RUN pip install --no-cache-dir psutil                 --no-cache-dir torch                 --no-cache-dir torchvision\n",
      " ---> Using cache\n",
      " ---> 5563022f6670\n",
      "Step 7/16 : ADD serve serve\n",
      " ---> e814eebd4996\n",
      "Step 8/16 : RUN pip install ../serve/\n",
      " ---> Running in f6d0e1d256ad\n",
      "Processing /serve\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from torchserve==0.1.1b20200704) (7.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from torchserve==0.1.1b20200704) (5.7.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torchserve==0.1.1b20200704) (0.18.2)\n",
      "Building wheels for collected packages: torchserve\n",
      "  Building wheel for torchserve (setup.py): started\n",
      "  Building wheel for torchserve (setup.py): still running...\n",
      "  Building wheel for torchserve (setup.py): still running...\n",
      "  Building wheel for torchserve (setup.py): still running...\n",
      "  Building wheel for torchserve (setup.py): still running...\n",
      "  Building wheel for torchserve (setup.py): finished with status 'done'\n",
      "  Created wheel for torchserve: filename=torchserve-0.1.1b20200704-py3-none-any.whl size=4550374 sha256=bf455fe6592200358c255150c81fc51741f2900b0686030744802088980177a6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-fb_thhnp/wheels/e0/2d/47/eeb1e34cdf27eebe4ed67c397489eaadf725da4a8e148eb347\n",
      "Successfully built torchserve\n",
      "Installing collected packages: torchserve\n",
      "Successfully installed torchserve-0.1.1b20200704\n",
      "Removing intermediate container f6d0e1d256ad\n",
      " ---> a29192be8d90\n",
      "Step 9/16 : COPY dockerd-entrypoint.sh /usr/local/bin/dockerd-entrypoint.sh\n",
      " ---> 0c7b57d14f5a\n",
      "Step 10/16 : RUN chmod +x /usr/local/bin/dockerd-entrypoint.sh\n",
      " ---> Running in 63ed8dde04a5\n",
      "Removing intermediate container 63ed8dde04a5\n",
      " ---> f810358312f3\n",
      "Step 11/16 : RUN mkdir -p /home/model-server/ && mkdir -p /home/model-server/tmp\n",
      " ---> Running in d5cc6472ad97\n",
      "Removing intermediate container d5cc6472ad97\n",
      " ---> e8b9ead55b02\n",
      "Step 12/16 : COPY config.properties /home/model-server/config.properties\n",
      " ---> b53671f6cb72\n",
      "Step 13/16 : WORKDIR /home/model-server\n",
      " ---> Running in 9f1d9fc3876e\n",
      "Removing intermediate container 9f1d9fc3876e\n",
      " ---> 2fec2c5f0ed5\n",
      "Step 14/16 : ENV TEMP=/home/model-server/tmp\n",
      " ---> Running in fbec2829ba81\n",
      "Removing intermediate container fbec2829ba81\n",
      " ---> e4c50e9a29e4\n",
      "Step 15/16 : ENTRYPOINT [\"/usr/local/bin/dockerd-entrypoint.sh\"]\n",
      " ---> Running in 0d79a216f82d\n",
      "Removing intermediate container 0d79a216f82d\n",
      " ---> b65f7d5092b6\n",
      "Step 16/16 : CMD [\"serve\"]\n",
      " ---> Running in fd840dacce9f\n",
      "Removing intermediate container fd840dacce9f\n",
      " ---> c74b25502056\n",
      "Successfully built c74b25502056\n",
      "Successfully tagged torchserve:v1\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "The push refers to repository [835319576252.dkr.ecr.us-east-1.amazonaws.com/torchserve]\n",
      "\n",
      "\u001b[1B46f91ec7: Preparing \n",
      "\u001b[1B5176ff39: Preparing \n",
      "\u001b[1B482ce064: Preparing \n",
      "\u001b[1B213caa6a: Preparing \n",
      "\u001b[1B63ac4c52: Preparing \n",
      "\u001b[1B78861c6c: Preparing \n",
      "\u001b[1Bbbd258cb: Preparing \n",
      "\u001b[1B2ac51df5: Preparing \n",
      "\u001b[1Bc96f7f82: Preparing \n",
      "\u001b[1B2bd02f5a: Preparing \n",
      "\u001b[1B00d84994: Preparing \n",
      "\u001b[1B52ea2c16: Preparing \n",
      "\u001b[1Bc9e5703f: Preparing \n",
      "\u001b[10B3ac4c52: Pushed   260.2MB/259.2MB\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[13A\u001b[2K\u001b[11A\u001b[2K\u001b[8A\u001b[2K\u001b[4A\u001b[2K\u001b[1A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[9A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2K\u001b[10A\u001b[2Kv1: digest: sha256:b9576e77c6648c5a7369e9145042ca31f6dbbf402856e414f293617de1e53815 size: 3247\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {registry_name}:{image_label} -f ./src_torchserve/Dockerfile ./src_torchserve\n",
    "!$(aws ecr get-login --no-include-email --region {region})\n",
    "!docker tag {registry_name}:{image_label} {image}\n",
    "!docker push {image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy endpoint and make prediction using Amazon SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s3_torchserve_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "sm_model_name = 'distilbert-pytorch'\n",
    "\n",
    "torchserve_model = Model(model_data = s3_torchserve_tar, \n",
    "                         image = image,\n",
    "                         role  = role,\n",
    "                         predictor_cls=RealTimePredictor,\n",
    "                         name  = sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using already existing model: distilbert-pytorch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'torchserve-endpoint-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "predictor = torchserve_model.deploy(instance_type='ml.c5.4xlarge',\n",
    "                                    initial_instance_count=1,\n",
    "                                    endpoint_name = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the ^^ Endpoint ^^ is Deployed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the TorchServe hosted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = './src_torchserve/sample_text.txt'\n",
    "#with open(file_name, 'rb') as f:\n",
    "#    payload = f.read()\n",
    "#    payload = payload\n",
    "#    \n",
    "#response = predictor.predict(data=payload)\n",
    "#print(*json.loads(response), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{\n  \"code\": 400,\n  \"type\": \"BadRequestException\",\n  \"message\": \"Parameter model_name is required.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/torchserve-endpoint-2020-07-04-20-42-31 in account 835319576252 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-b6246b936441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#           \"This is terrible.\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredicted_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This is great!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpredicted_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{\n  \"code\": 400,\n  \"type\": \"BadRequestException\",\n  \"message\": \"Parameter model_name is required.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/torchserve-endpoint-2020-07-04-20-42-31 in account 835319576252 for more information."
     ]
    }
   ],
   "source": [
    "import json\n",
    "    \n",
    "#reviews = [\"This is great!\", \n",
    "#           \"This is terrible.\"]\n",
    "\n",
    "predicted_classes = predictor.predict(\"This is great!\")\n",
    "\n",
    "for predicted_class, review in zip(predicted_classes, reviews):\n",
    "    print('[Predicted Star Rating: {}]'.format(predicted_class), review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
