{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    " \n",
    "* ## https://medium.com/analytics-vidhya/deploy-huggingface-s-bert-to-production-with-pytorch-serve-27b068026d18 \n",
    "\n",
    "* ## https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying our BERT PyTorch Model as REST EndPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==2.8.0\n",
    "!pip install -q torch==1.5.0 --upgrade --ignore-installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchserve in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.1)\n",
      "Requirement already satisfied: torch-model-archiver in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.1.1b20200704)\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchserve) (0.18.2)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchserve) (5.1.0)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torchserve) (5.4.5)\n",
      "Requirement already satisfied: enum-compat in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch-model-archiver) (0.0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/pytorch/serve.git ./src_torchserve\n",
    "!pip install ./src_torchserve/serve/model-archiver/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone the TorchServe repository and install torch-model-archiver\n",
    "\n",
    "You'll use `torch-model-archiver` to create a model archive file (.mar). The .mar model archive file contains model checkpoints along with it’s `state_dict` (dictionary object that maps each layer to its parameter tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'serve'...\n",
      "remote: Enumerating objects: 266, done.\u001b[K\n",
      "remote: Counting objects: 100% (266/266), done.\u001b[K\n",
      "remote: Compressing objects: 100% (188/188), done.\u001b[K\n",
      "remote: Total 9065 (delta 103), reused 113 (delta 32), pack-reused 8799\u001b[K\n",
      "Receiving objects: 100% (9065/9065), 35.56 MiB | 102.30 MiB/s, done.\n",
      "Resolving deltas: 100% (4853/4853), done.\n",
      "Processing ./serve/model-archiver\n",
      "Requirement already satisfied: future in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from torch-model-archiver==0.1.1b20200704) (0.18.2)\n",
      "Collecting enum-compat\n",
      "  Using cached enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
      "Building wheels for collected packages: torch-model-archiver\n",
      "  Building wheel for torch-model-archiver (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for torch-model-archiver: filename=torch_model_archiver-0.1.1b20200704-py3-none-any.whl size=15785 sha256=c1c0f4d303de415c8f08e76ad3c85bda60a71d6cfe68b939225c6d2d5c1b67cd\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/30/46/a4/0551adcd75fd74e180ed4b8c09ffca21c3f1fb910a31675843\n",
      "Successfully built torch-model-archiver\n",
      "Installing collected packages: enum-compat, torch-model-archiver\n",
      "Successfully installed enum-compat-0.0.3 torch-model-archiver-0.1.1b20200704\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pytorch/serve.git\n",
    "!pip install serve/model-archiver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r s3_pytorch_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-806570384721/models/pytorch/pytorch_model.pt\n"
     ]
    }
   ],
   "source": [
    "print(s3_pytorch_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r s3_transformer_pytorch_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-806570384721/models/transformer-pytorch/\n"
     ]
    }
   ],
   "source": [
    "print(s3_transformer_pytorch_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-1-806570384721/models/transformer-pytorch/config.json to Transformer_model/config.json\n",
      "download: s3://sagemaker-us-east-1-806570384721/models/transformer-pytorch/pytorch_model.bin to Transformer_model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --recursive $s3_transformer_pytorch_model_path ./Transformer_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create TorchServe Model Archive File\n",
    "\n",
    "Once, setup_config.json, sample_text.txt and index_to_name.json are set properly, we can go ahead and package the model and start serving it. The artifacts realted to each operation mode (such as sample_text.txt, index_to_name.json) can be place in their respective folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!torch-model-archiver \n",
    "#    --model-name \"bert\" \\\n",
    "#    --version 1.0 \\\n",
    "#    --serialized-file ./bert_model/pytorch_model.bin \\\n",
    "#    --extra-files \"./bert_model/config.json\" \\\n",
    "#    --handler \"./transformers_classifier_torchserve_handler.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'DistilBertForSequenceClassification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torch-model-archiver \\\n",
    "    --model-name $model_name \\\n",
    "    --version 1.0 \\\n",
    "    --serialized-file Transformer_model/pytorch_model.bin \\\n",
    "    --handler ./src_torchserve/Transformer_handler_generalized.py \\\n",
    "    --extra-files \"./Transformer_model/config.json,./src_torchserve/setup_config.json,./src_torchserve/Seq_classification_artifacts/index_to_name.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./DistilBertForSequenceClassification.mar\n"
     ]
    }
   ],
   "source": [
    "!ls ./*.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Registering the Model on TorchServe and Running Inference\n",
    "\n",
    "To register the model on TorchServe using the above model archive file, we run the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘./model_store’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./model_store\n",
    "!mv ./DistilBertForSequenceClassification.mar ./model_store/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing orphan pid file.\n"
     ]
    }
   ],
   "source": [
    "!torchserve \\\n",
    "--start \\\n",
    "--model-store model_store \\\n",
    "--models distilbert-pytorch=DistilBertForSequenceClassification.mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To run the inference using our registered model, open a new terminal and run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "curl: (7) Failed to connect to 127.0.0.1 port 8080: Connection refused\n"
     ]
    }
   ],
   "source": [
    "!curl -X POST http://127.0.0.1:8080/predictions/distilbert-pytorch -T ./src_torchserve/Seq_classification_artifacts/sample_text.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Model for SageMaker Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload .mar to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "torchserve_mar = 'DistilBertForSequenceClassification.mar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-806570384721/models/torchserve/DistilBertForSequenceClassification.mar\n"
     ]
    }
   ],
   "source": [
    "s3_torchserve_mar = 's3://{}/models/torchserve/{}'.format(bucket, torchserve_mar)\n",
    "print(s3_torchserve_mar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: model_store/DistilBertForSequenceClassification.mar to s3://sagemaker-us-east-1-806570384721/models/torchserve/DistilBertForSequenceClassification.mar\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./model_store/$torchserve_mar $s3_torchserve_mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_torchserve_mar' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_torchserve_mar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./model_store/DistilBertForSequenceClassification.mar\n"
     ]
    }
   ],
   "source": [
    "!tar cvfz ./DistilBertForSequenceClassification.tar.gz \\\n",
    "    ./model_store/DistilBertForSequenceClassification.mar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_torchserve_tar = 's3://{}/models/torchserve/DistilBertForSequenceClassification.tar.gz'.format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./DistilBertForSequenceClassification.tar.gz to s3://sagemaker-us-east-1-806570384721/models/torchserve/DistilBertForSequenceClassification.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp ./DistilBertForSequenceClassification.tar.gz $s3_torchserve_tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 's3_torchserve_tar' (str)\n"
     ]
    }
   ],
   "source": [
    "%store s3_torchserve_tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Amazon ECR registry\n",
    "Create a new docker container registry for your torchserve container images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'torchserve' already exists in the registry with id '806570384721'\n"
     ]
    }
   ],
   "source": [
    "registry_name = 'torchserve'\n",
    "!aws ecr create-repository --repository-name {registry_name}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a TorchServe Docker container and push it to Amazon ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_label = 'v1'\n",
    "image = f'{account_id}.dkr.ecr.{region}.amazonaws.com/{registry_name}:{image_label}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  60.51MB\n",
      "Step 1/16 : FROM ubuntu:18.04\n",
      " ---> 8e4ce0a6ce69\n",
      "Step 2/16 : ENV PYTHONUNBUFFERED TRUE\n",
      " ---> Using cache\n",
      " ---> c7057762ad65\n",
      "Step 3/16 : RUN apt-get update &&     DEBIAN_FRONTEND=noninteractive apt-get install --no-install-recommends -y     fakeroot     ca-certificates     dpkg-dev     g++     python3-dev     openjdk-11-jdk     curl     vim     && rm -rf /var/lib/apt/lists/*     && cd /tmp     && curl -O https://bootstrap.pypa.io/get-pip.py     && python3 get-pip.py\n",
      " ---> Using cache\n",
      " ---> ddeae91e0f53\n",
      "Step 4/16 : RUN update-alternatives --install /usr/bin/python python /usr/bin/python3 1\n",
      " ---> Using cache\n",
      " ---> 5d1f21b4f00d\n",
      "Step 5/16 : RUN update-alternatives --install /usr/local/bin/pip pip /usr/local/bin/pip3 1\n",
      " ---> Using cache\n",
      " ---> 8f36e4816e10\n",
      "Step 6/16 : RUN pip install --no-cache-dir psutil                 --no-cache-dir torch                 --no-cache-dir torchvision\n",
      " ---> Using cache\n",
      " ---> 2cfef5994910\n",
      "Step 7/16 : ADD serve serve\n",
      " ---> b46fedda1ee9\n",
      "Step 8/16 : RUN pip install ../serve/\n",
      " ---> Running in 54ddfba99890\n",
      "Processing /serve\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from torchserve==0.1.1b20200704) (7.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from torchserve==0.1.1b20200704) (5.7.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torchserve==0.1.1b20200704) (0.18.2)\n",
      "Building wheels for collected packages: torchserve\n",
      "  Building wheel for torchserve (setup.py): started\n",
      "  Building wheel for torchserve (setup.py): still running...\n",
      "  Building wheel for torchserve (setup.py): still running...\n",
      "  Building wheel for torchserve (setup.py): still running...\n",
      "  Building wheel for torchserve (setup.py): still running...\n",
      "  Building wheel for torchserve (setup.py): finished with status 'done'\n",
      "  Created wheel for torchserve: filename=torchserve-0.1.1b20200704-py3-none-any.whl size=4550354 sha256=a3e39e69d12d9a4bb232fcfe1a2e6444226416d2cb1cc46596396fa688b8017b\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1amoxo59/wheels/e0/2d/47/eeb1e34cdf27eebe4ed67c397489eaadf725da4a8e148eb347\n",
      "Successfully built torchserve\n",
      "Installing collected packages: torchserve\n",
      "Successfully installed torchserve-0.1.1b20200704\n",
      "Removing intermediate container 54ddfba99890\n",
      " ---> 0f1e35220a41\n",
      "Step 9/16 : COPY dockerd-entrypoint.sh /usr/local/bin/dockerd-entrypoint.sh\n",
      " ---> bcd434599511\n",
      "Step 10/16 : RUN chmod +x /usr/local/bin/dockerd-entrypoint.sh\n",
      " ---> Running in ef088d4a280d\n",
      "Removing intermediate container ef088d4a280d\n",
      " ---> 00bfbf2d24ca\n",
      "Step 11/16 : RUN mkdir -p /home/model-server/ && mkdir -p /home/model-server/tmp\n",
      " ---> Running in 2be59169568d\n",
      "Removing intermediate container 2be59169568d\n",
      " ---> 74a146654464\n",
      "Step 12/16 : COPY config.properties /home/model-server/config.properties\n",
      " ---> 60f782c078b0\n",
      "Step 13/16 : WORKDIR /home/model-server\n",
      " ---> Running in f02d776a7484\n",
      "Removing intermediate container f02d776a7484\n",
      " ---> d7c753055c0e\n",
      "Step 14/16 : ENV TEMP=/home/model-server/tmp\n",
      " ---> Running in b1e247e8a8c2\n",
      "Removing intermediate container b1e247e8a8c2\n",
      " ---> 24917835640e\n",
      "Step 15/16 : ENTRYPOINT [\"/usr/local/bin/dockerd-entrypoint.sh\"]\n",
      " ---> Running in 167837ef3876\n",
      "Removing intermediate container 167837ef3876\n",
      " ---> 0dda97aca91c\n",
      "Step 16/16 : CMD [\"serve\"]\n",
      " ---> Running in ab269df0b93b\n",
      "Removing intermediate container ab269df0b93b\n",
      " ---> cac576394d49\n",
      "Successfully built cac576394d49\n",
      "Successfully tagged torchserve:v1\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "The push refers to repository [806570384721.dkr.ecr.us-east-1.amazonaws.com/torchserve]\n",
      "\n",
      "\u001b[1Bcb8ec611: Preparing \n",
      "\u001b[1B6738e431: Preparing \n",
      "\u001b[1B99583ef6: Preparing \n",
      "\u001b[1B83a81fbb: Preparing \n",
      "\u001b[1Be24cbde5: Preparing \n",
      "\u001b[1B398a1769: Preparing \n",
      "\u001b[1B4aea5cdf: Preparing \n",
      "\u001b[1B64ea88fa: Preparing \n",
      "\u001b[1Bc1fafa2b: Preparing \n",
      "\u001b[1B8a681216: Preparing \n",
      "\u001b[1B00d84994: Preparing \n",
      "\u001b[1B52ea2c16: Preparing \n",
      "\u001b[1Bc9e5703f: Preparing \n",
      "\u001b[10B24cbde5: Pushed   260.2MB/259.2MB\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[13A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[Kv1: digest: sha256:a08de8e9d433220a37a2011bedd2f633e2c9cd09b36abacc79a09d619598633f size: 3247\n"
     ]
    }
   ],
   "source": [
    "!docker build -t {registry_name}:{image_label} -f ./src_torchserve/Dockerfile ./src_torchserve\n",
    "!$(aws ecr get-login --no-include-email --region {region})\n",
    "!docker tag {registry_name}:{image_label} {image}\n",
    "!docker push {image}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy endpoint and make prediction using Amazon SageMaker SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import RealTimePredictor\n",
    "\n",
    "sm_model_name = 'torchserve-distilbert'\n",
    "\n",
    "torchserve_model = Model(model_data = s3_torchserve_tar, \n",
    "                         image = image,\n",
    "                         role  = role,\n",
    "                         predictor_cls=RealTimePredictor,\n",
    "                         name  = sm_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'torchserve-endpoint-' + time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "\n",
    "predictor = torchserve_model.deploy(instance_type='ml.m4.xlarge',\n",
    "                                    initial_instance_count=1,\n",
    "                                    endpoint_name = endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Wait Until the ^^ Endpoint ^^ is Deployed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the TorchServe hosted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_name = './src_torchserve/sample_text.txt'\n",
    "#with open(file_name, 'rb') as f:\n",
    "#    payload = f.read()\n",
    "#    payload = payload\n",
    "#    \n",
    "#response = predictor.predict(data=payload)\n",
    "#print(*json.loads(response), sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{\n  \"code\": 400,\n  \"type\": \"BadRequestException\",\n  \"message\": \"Parameter model_name is required.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/torchserve-endpoint-2020-07-04-19-25-55 in account 806570384721 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-295f1cc04e05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m            \"This is terrible.\"]\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpredicted_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpredicted_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/tensorflow/serving.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CustomAttributes\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/predictor.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, initial_args, target_model)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mrequest_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_request_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_runtime_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrequest_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message \"{\n  \"code\": 400,\n  \"type\": \"BadRequestException\",\n  \"message\": \"Parameter model_name is required.\"\n}\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/torchserve-endpoint-2020-07-04-19-25-55 in account 806570384721 for more information."
     ]
    }
   ],
   "source": [
    "import json\n",
    "    \n",
    "reviews = [\"This is great!\", \n",
    "           \"This is terrible.\"]\n",
    "\n",
    "predicted_classes = predictor.predict(reviews)\n",
    "\n",
    "for predicted_class, review in zip(predicted_classes, reviews):\n",
    "    print('[Predicted Star Rating: {}]'.format(predicted_class), review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
