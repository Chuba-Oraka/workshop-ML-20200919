{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a `TrainingPipeline` with the Step Functions Data Science SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stepfunctions==1.0.0.8\n",
      "  Downloading stepfunctions-1.0.0.8.tar.gz (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 8.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sagemaker>=1.42.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from stepfunctions==1.0.0.8) (1.56.1)\n",
      "Requirement already satisfied: boto3>=1.9.213 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from stepfunctions==1.0.0.8) (1.12.39)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from stepfunctions==1.0.0.8) (5.3.1)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions==1.0.0.8) (0.1.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions==1.0.0.8) (1.18.4)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions==1.0.0.8) (1.4.1)\n",
      "Requirement already satisfied: smdebug-rulesconfig==0.1.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions==1.0.0.8) (0.1.2)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions==1.0.0.8) (1.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions==1.0.0.8) (20.1)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sagemaker>=1.42.8->stepfunctions==1.0.0.8) (3.11.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.213->stepfunctions==1.0.0.8) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.213->stepfunctions==1.0.0.8) (1.15.39)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3>=1.9.213->stepfunctions==1.0.0.8) (0.3.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf3-to-dict>=0.1.5->sagemaker>=1.42.8->stepfunctions==1.0.0.8) (1.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=1.4.0->sagemaker>=1.42.8->stepfunctions==1.0.0.8) (3.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging>=20.0->sagemaker>=1.42.8->stepfunctions==1.0.0.8) (2.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker>=1.42.8->stepfunctions==1.0.0.8) (46.1.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.9.213->stepfunctions==1.0.0.8) (2.7.3)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.9.213->stepfunctions==1.0.0.8) (1.25.9)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.39->boto3>=1.9.213->stepfunctions==1.0.0.8) (0.14)\n",
      "Building wheels for collected packages: stepfunctions\n",
      "  Building wheel for stepfunctions (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stepfunctions: filename=stepfunctions-1.0.0.8-py2.py3-none-any.whl size=69532 sha256=e6480f49062dccb7d80920b460cf31792a701824704eb59f8c6379c3d3b3dd31\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/66/0b/2e/d91e6d0948ef2c46b1b7df28ae2fff3085953f01408eceff35\n",
      "Successfully built stepfunctions\n",
      "Installing collected packages: stepfunctions\n",
      "Successfully installed stepfunctions-1.0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install stepfunctions==1.0.0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stepfunctions\n",
    "import logging\n",
    "\n",
    "from stepfunctions.template.pipeline import TrainingPipeline\n",
    "stepfunctions.set_stream_logger(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updte IAM Roles to Enable Step Functions to Trigger SageMaker Jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Managed Policy SageMaker Notebook Execution Role\n",
    "\n",
    "1. Open the Amazon [SageMaker console](https://console.aws.amazon.com/sagemaker/). \n",
    "2. Select **Notebook instances** and choose the name of your notebook instance\n",
    "3. Under **Permissions and encryption** select the role ARN to view the role on the IAM console\n",
    "4. Choose **Attach policies** and search for `AWSStepFunctionsFullAccess`.\n",
    "5. Select the check box next to `AWSStepFunctionsFullAccess` and choose **Attach policy**\n",
    "\n",
    "![Attach AWSStepFunctionsFullAccess Policy to Notebook Execution Role](img/attach_policies_with_stepfunctions.png)\n",
    "\n",
    "## Create an Execution Role for Step Functions\n",
    "\n",
    "You need a StepFunctionsWorkflowExecutionRole so that you can create and execute workflows in Step Functions.\n",
    "\n",
    "1. Go to the [IAM console](https://console.aws.amazon.com/iam/)\n",
    "2. Select **Roles** and then **Create role**.\n",
    "3. Under **Choose the service that will use this role** select **Step Functions**\n",
    "4. Choose **Next** until you can enter a **Role name**\n",
    "5. Enter a name such as `StepFunctionsWorkflowExecutionRole` and then select **Create role**\n",
    "\n",
    "\n",
    "Attach a policy to the role you created. The following steps attach a policy that provides full access to Step Functions, however as a good practice you should only provide access to the resources you need.  \n",
    "\n",
    "1. Under the **Permissions** tab, click **Add inline policy**\n",
    "2. Enter the following in the **JSON** tab\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:CreateTransformJob\",\n",
    "                \"sagemaker:DescribeTransformJob\",\n",
    "                \"sagemaker:StopTransformJob\",\n",
    "                \"sagemaker:CreateTrainingJob\",\n",
    "                \"sagemaker:DescribeTrainingJob\",\n",
    "                \"sagemaker:StopTrainingJob\",\n",
    "                \"sagemaker:CreateHyperParameterTuningJob\",\n",
    "                \"sagemaker:DescribeHyperParameterTuningJob\",\n",
    "                \"sagemaker:StopHyperParameterTuningJob\",\n",
    "                \"sagemaker:CreateModel\",\n",
    "                \"sagemaker:CreateEndpointConfig\",\n",
    "                \"sagemaker:CreateEndpoint\",\n",
    "                \"sagemaker:DeleteEndpointConfig\",\n",
    "                \"sagemaker:DeleteEndpoint\",\n",
    "                \"sagemaker:UpdateEndpoint\",\n",
    "                \"sagemaker:ListTags\",\n",
    "                \"lambda:InvokeFunction\",\n",
    "                \"sqs:SendMessage\",\n",
    "                \"sns:Publish\",\n",
    "                \"ecs:RunTask\",\n",
    "                \"ecs:StopTask\",\n",
    "                \"ecs:DescribeTasks\",\n",
    "                \"dynamodb:GetItem\",\n",
    "                \"dynamodb:PutItem\",\n",
    "                \"dynamodb:UpdateItem\",\n",
    "                \"dynamodb:DeleteItem\",\n",
    "                \"batch:SubmitJob\",\n",
    "                \"batch:DescribeJobs\",\n",
    "                \"batch:TerminateJob\",\n",
    "                \"glue:StartJobRun\",\n",
    "                \"glue:GetJobRun\",\n",
    "                \"glue:GetJobRuns\",\n",
    "                \"glue:BatchStopJobRun\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"events:PutTargets\",\n",
    "                \"events:PutRule\",\n",
    "                \"events:DescribeRule\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTransformJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTuningJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForECSTaskRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForBatchJobsRule\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "3. Choose **Review policy** and give the policy a name such as `StepFunctionsWorkflowExecutionPolicy`\n",
    "4. Choose **Create policy**. You will be redirected to the details page for the role.\n",
    "5. Copy the **Role ARN** at the top of the **Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepfunctions_role = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"sagemaker:CreateTransformJob\",\n",
    "                \"sagemaker:DescribeTransformJob\",\n",
    "                \"sagemaker:StopTransformJob\",\n",
    "                \"sagemaker:CreateTrainingJob\",\n",
    "                \"sagemaker:DescribeTrainingJob\",\n",
    "                \"sagemaker:StopTrainingJob\",\n",
    "                \"sagemaker:CreateHyperParameterTuningJob\",\n",
    "                \"sagemaker:DescribeHyperParameterTuningJob\",\n",
    "                \"sagemaker:StopHyperParameterTuningJob\",\n",
    "                \"sagemaker:CreateModel\",\n",
    "                \"sagemaker:CreateEndpointConfig\",\n",
    "                \"sagemaker:CreateEndpoint\",\n",
    "                \"sagemaker:DeleteEndpointConfig\",\n",
    "                \"sagemaker:DeleteEndpoint\",\n",
    "                \"sagemaker:UpdateEndpoint\",\n",
    "                \"sagemaker:ListTags\",\n",
    "                \"lambda:InvokeFunction\",\n",
    "                \"sqs:SendMessage\",\n",
    "                \"sns:Publish\",\n",
    "                \"ecs:RunTask\",\n",
    "                \"ecs:StopTask\",\n",
    "                \"ecs:DescribeTasks\",\n",
    "                \"dynamodb:GetItem\",\n",
    "                \"dynamodb:PutItem\",\n",
    "                \"dynamodb:UpdateItem\",\n",
    "                \"dynamodb:DeleteItem\",\n",
    "                \"batch:SubmitJob\",\n",
    "                \"batch:DescribeJobs\",\n",
    "                \"batch:TerminateJob\",\n",
    "                \"glue:StartJobRun\",\n",
    "                \"glue:GetJobRun\",\n",
    "                \"glue:GetJobRuns\",\n",
    "                \"glue:BatchStopJobRun\"\n",
    "            ],\n",
    "            \"Resource\": \"*\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"iam:PassRole\"\n",
    "            ],\n",
    "            \"Resource\": \"*\",\n",
    "            \"Condition\": {\n",
    "                \"StringEquals\": {\n",
    "                    \"iam:PassedToService\": \"sagemaker.amazonaws.com\"\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"events:PutTargets\",\n",
    "                \"events:PutRule\",\n",
    "                \"events:DescribeRule\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTrainingJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTransformJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForSageMakerTuningJobsRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForECSTaskRule\",\n",
    "                \"arn:aws:events:*:*:rule/StepFunctionsGetEventsForBatchJobsRule\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r scikit_processing_job_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker-scikit-learn-2020-05-08-19-54-41-801\n"
     ]
    }
   ],
   "source": [
    "print(scikit_processing_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous Scikit Processing Job Name: sagemaker-scikit-learn-2020-05-08-19-54-41-801\n"
     ]
    }
   ],
   "source": [
    "print('Previous Scikit Processing Job Name: {}'.format(scikit_processing_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_train = '{}/output/bert-train'.format(scikit_processing_job_name)\n",
    "prefix_validation = '{}/output/bert-validation'.format(scikit_processing_job_name)\n",
    "prefix_test = '{}/output/bert-test'.format(scikit_processing_job_name)\n",
    "\n",
    "train_s3_uri = 's3://{}/{}'.format(bucket, prefix_train)\n",
    "validation_s3_uri = 's3://{}/{}'.format(bucket, prefix_validation)\n",
    "test_s3_uri = 's3://{}/{}'.format(bucket, prefix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-05-08-19-54-41-801/output/bert-train\n",
      "2020-05-08 20:03:54     893792 part-algo-1-amazon_reviews_us_Apparel_v1_00.tfrecord\n",
      "2020-05-08 20:03:54     810463 part-algo-1-amazon_reviews_us_Books_v1_01.tfrecord\n",
      "2020-05-08 20:03:54      61503 part-algo-1-amazon_reviews_us_Digital_Music_Purchase_v1_00.tfrecord\n",
      "2020-05-08 20:03:54     109623 part-algo-1-amazon_reviews_us_Furniture_v1_00.tfrecord\n",
      "2020-05-08 20:03:54     308128 part-algo-1-amazon_reviews_us_Home_Improvement_v1_00.tfrecord\n",
      "2020-05-08 20:03:54      45135 part-algo-1-amazon_reviews_us_Luggage_v1_00.tfrecord\n",
      "2020-05-08 20:03:54     100468 part-algo-1-amazon_reviews_us_Musical_Instruments_v1_00.tfrecord\n",
      "2020-05-08 20:03:54     378059 part-algo-1-amazon_reviews_us_Pet_Products_v1_00.tfrecord\n",
      "2020-05-08 20:03:54     572785 part-algo-1-amazon_reviews_us_Toys_v1_00.tfrecord\n",
      "2020-05-08 20:03:54    1470542 part-algo-1-amazon_reviews_us_Wireless_v1_00.tfrecord\n",
      "2020-05-08 20:02:40     362608 part-algo-2-amazon_reviews_us_Automotive_v1_00.tfrecord\n",
      "2020-05-08 20:02:40     451793 part-algo-2-amazon_reviews_us_Books_v1_02.tfrecord\n",
      "2020-05-08 20:02:40      18203 part-algo-2-amazon_reviews_us_Digital_Software_v1_00.tfrecord\n",
      "2020-05-08 20:02:40       4302 part-algo-2-amazon_reviews_us_Gift_Card_v1_00.tfrecord\n",
      "2020-05-08 20:02:40     798394 part-algo-2-amazon_reviews_us_Home_v1_00.tfrecord\n",
      "2020-05-08 20:02:40      14370 part-algo-2-amazon_reviews_us_Major_Appliances_v1_00.tfrecord\n",
      "2020-05-08 20:02:40     344669 part-algo-2-amazon_reviews_us_Office_Products_v1_00.tfrecord\n",
      "2020-05-08 20:02:40     548050 part-algo-2-amazon_reviews_us_Video_DVD_v1_00.tfrecord\n",
      "2020-05-08 20:02:05     255922 part-algo-3-amazon_reviews_us_Baby_v1_00.tfrecord\n",
      "2020-05-08 20:02:05     230043 part-algo-3-amazon_reviews_us_Camera_v1_00.tfrecord\n",
      "2020-05-08 20:02:05     478554 part-algo-3-amazon_reviews_us_Digital_Video_Download_v1_00.tfrecord\n",
      "2020-05-08 20:02:05     260926 part-algo-3-amazon_reviews_us_Grocery_v1_00.tfrecord\n",
      "2020-05-08 20:02:05     242354 part-algo-3-amazon_reviews_us_Jewelry_v1_00.tfrecord\n",
      "2020-05-08 20:02:05     589325 part-algo-3-amazon_reviews_us_Mobile_Apps_v1_00.tfrecord\n",
      "2020-05-08 20:02:05     275728 part-algo-3-amazon_reviews_us_Outdoors_v1_00.tfrecord\n",
      "2020-05-08 20:02:05      63328 part-algo-3-amazon_reviews_us_Software_v1_00.tfrecord\n",
      "2020-05-08 20:02:05     242862 part-algo-3-amazon_reviews_us_Video_Games_v1_00.tfrecord\n"
     ]
    }
   ],
   "source": [
    "print(train_s3_uri)\n",
    "!aws s3 ls $train_s3_uri/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-05-08-19-54-41-801/output/bert-train', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-05-08-19-54-41-801/output/bert-validation', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n",
      "{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': 's3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-05-08-19-54-41-801/output/bert-test', 'S3DataDistributionType': 'ShardedByS3Key'}}}\n"
     ]
    }
   ],
   "source": [
    "s3_input_train_data = sagemaker.s3_input(s3_data=train_s3_uri, distribution='ShardedByS3Key') \n",
    "s3_input_validation_data = sagemaker.s3_input(s3_data=validation_s3_uri, distribution='ShardedByS3Key')\n",
    "s3_input_test_data = sagemaker.s3_input(s3_data=test_s3_uri, distribution='ShardedByS3Key')\n",
    "\n",
    "print(s3_input_train_data.config)\n",
    "print(s3_input_validation_data.config)\n",
    "print(s3_input_test_data.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtime\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m glob\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpprint\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msubprocess\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[37m#subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'tensorflow==2.1.0'])\u001b[39;49;00m\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers==2.8.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msagemaker-tensorflow==2.1.0.1.0.0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "subprocess.check_call([sys.executable, \u001b[33m'\u001b[39;49;00m\u001b[33m-m\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mpip\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33minstall\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33msmdebug==0.7.2\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFDistilBertForSequenceClassification\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TextClassificationPipeline\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers.configuration_distilbert\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertConfig\r\n",
      "\r\n",
      "CLASSES = [\u001b[34m1\u001b[39;49;00m, \u001b[34m2\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m, \u001b[34m4\u001b[39;49;00m, \u001b[34m5\u001b[39;49;00m]\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mselect_data_and_label_from_record\u001b[39;49;00m(record):\r\n",
      "    x = {\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m],\r\n",
      "        \u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: record[\u001b[33m'\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "    }\r\n",
      "\r\n",
      "    y = record[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m (x, y)\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mfile_based_input_dataset_builder\u001b[39;49;00m(channel,\r\n",
      "                                     input_filenames,\r\n",
      "                                     pipe_mode,\r\n",
      "                                     is_training,\r\n",
      "                                     drop_remainder,\r\n",
      "                                     batch_size,\r\n",
      "                                     epochs,\r\n",
      "                                     steps_per_epoch,\r\n",
      "                                     max_seq_length):\r\n",
      "\r\n",
      "    \u001b[37m# For training, we want a lot of parallel reading and shuffling.\u001b[39;49;00m\r\n",
      "    \u001b[37m# For eval, we want no shuffling and parallel reading doesn't matter.\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m pipe_mode:\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using pipe_mode with channel {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(channel))\r\n",
      "        \u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36msagemaker_tensorflow\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m PipeModeDataset\r\n",
      "        dataset = PipeModeDataset(channel=channel,\r\n",
      "                                  record_format=\u001b[33m'\u001b[39;49;00m\u001b[33mTFRecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m***** Using input_filenames {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(input_filenames))\r\n",
      "        dataset = tf.data.TFRecordDataset(input_filenames)\r\n",
      "\r\n",
      "    dataset = dataset.repeat(epochs * steps_per_epoch)\r\n",
      "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
      "\r\n",
      "    name_to_features = {\r\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33minput_mask\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33msegment_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([max_seq_length], tf.int64),\r\n",
      "      \u001b[33m\"\u001b[39;49;00m\u001b[33mlabel_ids\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: tf.io.FixedLenFeature([], tf.int64),\r\n",
      "    }\r\n",
      "\r\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32m_decode_record\u001b[39;49;00m(record, name_to_features):\r\n",
      "        \u001b[33m\"\"\"Decodes a record to a TensorFlow example.\"\"\"\u001b[39;49;00m\r\n",
      "        record = tf.io.parse_single_example(record, name_to_features)\r\n",
      "        \u001b[37m# TODO:  wip/bert/bert_attention_head_view/train.py\u001b[39;49;00m\r\n",
      "        \u001b[37m# Convert input_ids into input_tokens with DistilBert vocabulary \u001b[39;49;00m\r\n",
      "        \u001b[37m#  if hook.get_collections()['all'].save_config.should_save_step(modes.EVAL, hook.mode_steps[modes.EVAL]):\u001b[39;49;00m\r\n",
      "\u001b[37m#              hook._write_raw_tensor_simple(\"input_tokens\", input_tokens)\u001b[39;49;00m\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m record\r\n",
      "    \r\n",
      "    dataset = dataset.apply(\r\n",
      "        tf.data.experimental.map_and_batch(\r\n",
      "          \u001b[34mlambda\u001b[39;49;00m record: _decode_record(record, name_to_features),\r\n",
      "          batch_size=batch_size,\r\n",
      "          drop_remainder=drop_remainder,\r\n",
      "          num_parallel_calls=tf.data.experimental.AUTOTUNE))\r\n",
      "\r\n",
      "    dataset.cache()\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m is_training:\r\n",
      "        dataset = dataset.shuffle(seed=\u001b[34m42\u001b[39;49;00m,\r\n",
      "                                  buffer_size=\u001b[34m1000\u001b[39;49;00m,\r\n",
      "                                  reshuffle_each_iteration=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m dataset\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m'\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "    parser = argparse.ArgumentParser()\r\n",
      "\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_data\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \u001b[37m# This points to the S3 location - this should not be used by our code\u001b[39;49;00m\r\n",
      "    \u001b[37m# We should use /opt/ml/model/ instead\u001b[39;49;00m\r\n",
      "\u001b[37m#     parser.add_argument('--model_dir', \u001b[39;49;00m\r\n",
      "\u001b[37m#                         type=str, \u001b[39;49;00m\r\n",
      "\u001b[37m#                         default=os.environ['SM_MODEL_DIR'])\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    \u001b[37m# This is unused\u001b[39;49;00m\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m,\r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--hosts\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mlist\u001b[39;49;00m, \r\n",
      "                        default=json.loads(os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_HOSTS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]))\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--current_host\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, \r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CURRENT_HOST\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--num_gpus\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, \r\n",
      "                        default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_xla\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--use_amp\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--max_seq_length\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m128\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m128\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_batch_size\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m256\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m2\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.00003\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--epsilon\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m0.00000001\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train_steps_per_epoch\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1000\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1000\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--test_steps\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m,\r\n",
      "                        default=\u001b[34m1000\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--freeze_bert_layer\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--enable_sagemaker_debugger\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m)    \r\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_test\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m)    \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--run_sample_predictions\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                        \u001b[36mtype\u001b[39;49;00m=\u001b[36meval\u001b[39;49;00m,\r\n",
      "                        default=\u001b[36mFalse\u001b[39;49;00m)\r\n",
      "    \r\n",
      "    args, _ = parser.parse_known_args()\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mArgs:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(args)\r\n",
      "    \r\n",
      "    env_var = os.environ \r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mEnvironment Variables:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \r\n",
      "    pprint.pprint(\u001b[36mdict\u001b[39;49;00m(env_var), width = \u001b[34m1\u001b[39;49;00m) \r\n",
      "\r\n",
      "    train_data = args.train_data\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data))\r\n",
      "    validation_data = args.validation_data\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data))\r\n",
      "    test_data = args.test_data\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data))    \r\n",
      "\r\n",
      "\u001b[37m#    model_dir = args.model_dir\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('model_dir {}'.format(model_dir))    \u001b[39;49;00m\r\n",
      "    local_model_dir = os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "\r\n",
      "    output_dir = args.output_dir\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33moutput_dir {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(output_dir))    \r\n",
      "\r\n",
      "    \u001b[37m# This is unused\u001b[39;49;00m\r\n",
      "\u001b[37m#    output_data_dir = args.output_data_dir\u001b[39;49;00m\r\n",
      "\u001b[37m#    print('output_data_dir {}'.format(output_data_dir))    \u001b[39;49;00m\r\n",
      "    hosts = args.hosts\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mhosts {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(hosts))    \r\n",
      "    current_host = args.current_host\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mcurrent_host {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(current_host))    \r\n",
      "    num_gpus = args.num_gpus\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mnum_gpus {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(num_gpus))\r\n",
      "    use_xla = args.use_xla\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_xla {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_xla))    \r\n",
      "    use_amp = args.use_amp\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33muse_amp {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))    \r\n",
      "    max_seq_length = args.max_seq_length\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mmax_seq_length {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(max_seq_length))    \r\n",
      "    train_batch_size = args.train_batch_size\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_batch_size {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_batch_size))    \r\n",
      "    validation_batch_size = args.validation_batch_size\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_batch_size {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_batch_size))    \r\n",
      "    test_batch_size = args.test_batch_size\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_batch_size {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_batch_size))    \r\n",
      "    epochs = args.epochs\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepochs {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epochs))    \r\n",
      "    learning_rate = args.learning_rate\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mlearning_rate {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(learning_rate))    \r\n",
      "    epsilon = args.epsilon\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mepsilon {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(epsilon))    \r\n",
      "    train_steps_per_epoch = args.train_steps_per_epoch\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_steps_per_epoch {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_steps_per_epoch))    \r\n",
      "    validation_steps = args.validation_steps\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_steps {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_steps))    \r\n",
      "    test_steps = args.test_steps\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_steps {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_steps))    \r\n",
      "    freeze_bert_layer = args.freeze_bert_layer\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mfreeze_bert_layer {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(freeze_bert_layer))    \r\n",
      "    enable_sagemaker_debugger = args.enable_sagemaker_debugger\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_sagemaker_debugger {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_sagemaker_debugger))    \r\n",
      "    run_validation = args.run_validation\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_validation {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_validation))    \r\n",
      "    run_test = args.run_test\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_test {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_test))    \r\n",
      "    run_sample_predictions = args.run_sample_predictions\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mrun_sample_predictions {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(run_sample_predictions))    \r\n",
      "\r\n",
      "    \u001b[37m# Determine if PipeMode is enabled \u001b[39;49;00m\r\n",
      "    pipe_mode_str = os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_INPUT_DATA_CONFIG\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    pipe_mode = (pipe_mode_str.find(\u001b[33m'\u001b[39;49;00m\u001b[33mPipe\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) >= \u001b[34m0\u001b[39;49;00m)\r\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mUsing pipe_mode: {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(pipe_mode))\r\n",
      " \r\n",
      "    \u001b[37m# Model Output \u001b[39;49;00m\r\n",
      "    transformer_fine_tuned_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtransformers/fine-tuned/\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    os.makedirs(transformer_fine_tuned_model_path, exist_ok=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# SavedModel Output\u001b[39;49;00m\r\n",
      "    tensorflow_saved_model_path = os.path.join(local_model_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow/saved_model/0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "    os.makedirs(tensorflow_saved_model_path, exist_ok=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[37m# Tensorboard Logs \u001b[39;49;00m\r\n",
      "    tensorboard_logs_path = os.path.join(output_dir, \u001b[33m'\u001b[39;49;00m\u001b[33mtensorboard\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m) \r\n",
      "    os.makedirs(tensorboard_logs_path, exist_ok=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "\r\n",
      "    distributed_strategy = tf.distribute.MirroredStrategy()\r\n",
      "    \u001b[37m# smdebug currently (0.7.2) does not support MultiWorkerMirroredStrategy()\u001b[39;49;00m\r\n",
      "    \u001b[37m# distributed_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\u001b[39;49;00m\r\n",
      "    \u001b[34mwith\u001b[39;49;00m distributed_strategy.scope():\r\n",
      "        tf.config.optimizer.set_jit(use_xla)\r\n",
      "        tf.config.optimizer.set_experimental_options({\u001b[33m\"\u001b[39;49;00m\u001b[33mauto_mixed_precision\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: use_amp})\r\n",
      "\r\n",
      "        train_data_filenames = glob(os.path.join(train_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtrain_data_filenames {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(train_data_filenames))\r\n",
      "        train_dataset = file_based_input_dataset_builder(\r\n",
      "            channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "            input_filenames=train_data_filenames,\r\n",
      "            pipe_mode=pipe_mode,\r\n",
      "            is_training=\u001b[36mTrue\u001b[39;49;00m,\r\n",
      "            drop_remainder=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "            batch_size=train_batch_size,\r\n",
      "            epochs=epochs,\r\n",
      "            steps_per_epoch=train_steps_per_epoch,\r\n",
      "            max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "        tokenizer = \u001b[36mNone\u001b[39;49;00m\r\n",
      "        config = \u001b[36mNone\u001b[39;49;00m\r\n",
      "        model = \u001b[36mNone\u001b[39;49;00m\r\n",
      "\r\n",
      "        \u001b[37m# This is required when launching many instances at once...  the urllib request seems to get denied periodically\u001b[39;49;00m\r\n",
      "        successful_download = \u001b[36mFalse\u001b[39;49;00m\r\n",
      "        retries = \u001b[34m0\u001b[39;49;00m\r\n",
      "        \u001b[34mwhile\u001b[39;49;00m (retries < \u001b[34m5\u001b[39;49;00m \u001b[35mand\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m successful_download):\r\n",
      "            \u001b[34mtry\u001b[39;49;00m:\r\n",
      "                tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "                config = DistilBertConfig.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                                          num_labels=\u001b[36mlen\u001b[39;49;00m(CLASSES))\r\n",
      "                model = TFDistilBertForSequenceClassification.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                                                              config=config)\r\n",
      "                successful_download = \u001b[36mTrue\u001b[39;49;00m\r\n",
      "                \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mSucessfully downloaded after {} retries.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries))\r\n",
      "            \u001b[34mexcept\u001b[39;49;00m:\r\n",
      "                retries = retries + \u001b[34m1\u001b[39;49;00m\r\n",
      "                random_sleep = random.randint(\u001b[34m1\u001b[39;49;00m, \u001b[34m30\u001b[39;49;00m)\r\n",
      "                \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mRetry #{}.  Sleeping for {} seconds\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(retries, random_sleep))\r\n",
      "                time.sleep(random_sleep)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m tokenizer \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m model \u001b[35mor\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m config:\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mNot properly initialized...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=epsilon)\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m** use_amp {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(use_amp))        \r\n",
      "        \u001b[34mif\u001b[39;49;00m use_amp:\r\n",
      "            \u001b[37m# loss scaling is currently required when using mixed precision\u001b[39;49;00m\r\n",
      "            optimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(optimizer, \u001b[33m'\u001b[39;49;00m\u001b[33mdynamic\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        callbacks = []\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33menable_sagemaker_debugger {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(enable_sagemaker_debugger))\r\n",
      "        \u001b[34mif\u001b[39;49;00m enable_sagemaker_debugger:\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** DEBUGGING ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "            \u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msmdebug.tensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36msmd\u001b[39;49;00m\r\n",
      "            \u001b[37m# This assumes that we specified debugger_hook_config\u001b[39;49;00m\r\n",
      "            callback = smd.KerasHook.create_from_json_file()\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(callback)\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** CALLBACK {} ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(callback))\r\n",
      "            callbacks.append(callback)\r\n",
      "            optimizer = callback.wrap_optimizer(optimizer)\r\n",
      "\r\n",
      "        callback = tf.keras.callbacks.TensorBoard(log_dir=tensorboard_logs_path)\r\n",
      "        callbacks.append(callback)\r\n",
      "            \r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33m*** OPTIMIZER {} ***\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(optimizer))\r\n",
      "        \r\n",
      "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[36mTrue\u001b[39;49;00m)\r\n",
      "        metric = tf.keras.metrics.SparseCategoricalAccuracy(\u001b[33m'\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mTrained model {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(model))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(model.summary())\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m run_validation:\r\n",
      "            validation_data_filenames = glob(os.path.join(validation_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation_data_filenames {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(validation_data_filenames))\r\n",
      "            validation_dataset = file_based_input_dataset_builder(\r\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                input_filenames=validation_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "                drop_remainder=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "                batch_size=validation_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=validation_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "            \r\n",
      "            \u001b[37m# HACK:  trim the Validation dataset down to equal the number of validation steps to workaround PipeMode issue\u001b[39;49;00m\r\n",
      "            validation_dataset = validation_dataset.take(validation_steps)\r\n",
      "            \r\n",
      "            train_and_validation_history = model.fit(train_dataset,\r\n",
      "                                                     shuffle=\u001b[36mTrue\u001b[39;49;00m,\r\n",
      "                                                     epochs=epochs,\r\n",
      "                                                     steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                                     validation_data=validation_dataset,\r\n",
      "                                                     validation_steps=validation_steps,\r\n",
      "                                                     callbacks=callbacks)\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(train_and_validation_history)\r\n",
      "        \u001b[34melse\u001b[39;49;00m: \u001b[37m# Not running validation\u001b[39;49;00m\r\n",
      "            train_history = model.fit(train_dataset,\r\n",
      "                                      shuffle=\u001b[36mTrue\u001b[39;49;00m,\r\n",
      "                                      epochs=epochs,\r\n",
      "                                      steps_per_epoch=train_steps_per_epoch,\r\n",
      "                                      callbacks=callbacks)\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(train_history)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m run_test:\r\n",
      "            test_data_filenames = glob(os.path.join(test_data, \u001b[33m'\u001b[39;49;00m\u001b[33m*.tfrecord\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtest_data_filenames {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(test_data_filenames))\r\n",
      "            test_dataset = file_based_input_dataset_builder(\r\n",
      "                channel=\u001b[33m'\u001b[39;49;00m\u001b[33mtest\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                input_filenames=test_data_filenames,\r\n",
      "                pipe_mode=pipe_mode,\r\n",
      "                is_training=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "                drop_remainder=\u001b[36mFalse\u001b[39;49;00m,\r\n",
      "                batch_size=test_batch_size,\r\n",
      "                epochs=epochs,\r\n",
      "                steps_per_epoch=test_steps,\r\n",
      "                max_seq_length=max_seq_length).map(select_data_and_label_from_record)\r\n",
      "\r\n",
      "            test_history = model.evaluate(test_dataset,\r\n",
      "                                          steps=test_steps,\r\n",
      "                                          callbacks=callbacks)\r\n",
      "            \u001b[34mprint\u001b[39;49;00m(test_history)\r\n",
      "\r\n",
      "            \r\n",
      "        \u001b[37m# Save the fine-tuned Transformers Model\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtransformer_fine_tuned_model_path {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(transformer_fine_tuned_model_path))   \r\n",
      "\r\n",
      "        model.save_pretrained(transformer_fine_tuned_model_path)\r\n",
      "\r\n",
      "        \u001b[37m# Save the TensorFlow SavedModel\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mtensorflow_saved_model_path {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(tensorflow_saved_model_path))   \r\n",
      "        model.save(tensorflow_saved_model_path, save_format=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m run_sample_predictions:\r\n",
      "        loaded_model = TFDistilBertForSequenceClassification.from_pretrained(transformer_fine_tuned_model_path,\r\n",
      "                                                                       id2label={\r\n",
      "                                                                        \u001b[34m0\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\r\n",
      "                                                                        \u001b[34m1\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\r\n",
      "                                                                        \u001b[34m2\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\r\n",
      "                                                                        \u001b[34m3\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m,\r\n",
      "                                                                        \u001b[34m4\u001b[39;49;00m: \u001b[34m5\u001b[39;49;00m\r\n",
      "                                                                       },\r\n",
      "                                                                       label2id={\r\n",
      "                                                                        \u001b[34m1\u001b[39;49;00m: \u001b[34m0\u001b[39;49;00m,\r\n",
      "                                                                        \u001b[34m2\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\r\n",
      "                                                                        \u001b[34m3\u001b[39;49;00m: \u001b[34m2\u001b[39;49;00m,\r\n",
      "                                                                        \u001b[34m4\u001b[39;49;00m: \u001b[34m3\u001b[39;49;00m,\r\n",
      "                                                                        \u001b[34m5\u001b[39;49;00m: \u001b[34m4\u001b[39;49;00m\r\n",
      "                                                                       })\r\n",
      "\r\n",
      "        tokenizer = DistilBertTokenizer.from_pretrained(\u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\r\n",
      "\r\n",
      "        \u001b[34mif\u001b[39;49;00m num_gpus >= \u001b[34m1\u001b[39;49;00m:\r\n",
      "            inference_device = \u001b[34m0\u001b[39;49;00m \u001b[37m# GPU 0\u001b[39;49;00m\r\n",
      "        \u001b[34melse\u001b[39;49;00m:\r\n",
      "            inference_device = -\u001b[34m1\u001b[39;49;00m \u001b[37m# CPU\u001b[39;49;00m\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33minference_device {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(inference_device))\r\n",
      "\r\n",
      "        inference_pipeline = TextClassificationPipeline(model=loaded_model, \r\n",
      "                                                        tokenizer=tokenizer,\r\n",
      "                                                        framework=\u001b[33m'\u001b[39;49;00m\u001b[33mtf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\r\n",
      "                                                        device=inference_device)  \r\n",
      "\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, inference_pipeline(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mI loved it!  I will recommend this to everyone.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, inference_pipeline(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mIt\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33ms OK.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\r\n",
      "        \u001b[34mprint\u001b[39;49;00m(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m, inference_pipeline(\u001b[33m\"\"\"\u001b[39;49;00m\u001b[33mReally bad.  I hope they don\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mt make this anymore.\u001b[39;49;00m\u001b[33m\"\"\"\u001b[39;49;00m))\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/tf_bert_reviews.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=1\n",
    "learning_rate=0.00001\n",
    "epsilon=0.00000001\n",
    "train_batch_size=128\n",
    "validation_batch_size=128\n",
    "test_batch_size=128\n",
    "train_steps_per_epoch=100\n",
    "validation_steps=100\n",
    "test_steps=100\n",
    "train_instance_count=1\n",
    "train_instance_type='ml.p3.2xlarge'\n",
    "train_volume_size=1024\n",
    "use_xla=True\n",
    "use_amp=True\n",
    "max_seq_length=128\n",
    "freeze_bert_layer=True\n",
    "input_mode='Pipe'\n",
    "run_validation=True\n",
    "run_test=True\n",
    "run_sample_predictions=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_definitions = [\n",
    "     {'Name': 'train:loss', 'Regex': 'loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'train:accuracy', 'Regex': 'accuracy: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:loss', 'Regex': 'val_loss: ([0-9\\\\.]+)'},\n",
    "     {'Name': 'validation:accuracy', 'Regex': 'val_accuracy: ([0-9\\\\.]+)'},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='tf_bert_reviews.py',\n",
    "                       source_dir='src',\n",
    "                       role=role,\n",
    "                       train_instance_count=train_instance_count, # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_volume_size=train_volume_size,\n",
    "                       py_version='py3',\n",
    "                       framework_version='2.1.0',\n",
    "                       hyperparameters={'epochs': epochs,\n",
    "                                        'learning_rate': learning_rate,\n",
    "                                        'epsilon': epsilon,\n",
    "                                        'train_batch_size': train_batch_size,\n",
    "                                        'validation_batch_size': validation_batch_size,\n",
    "                                        'test_batch_size': test_batch_size,                                             \n",
    "                                        'train_steps_per_epoch': train_steps_per_epoch,\n",
    "                                        'validation_steps': validation_steps,\n",
    "                                        'test_steps': test_steps,\n",
    "                                        'use_xla': use_xla,\n",
    "                                        'use_amp': use_amp,                                             \n",
    "                                        'max_seq_length': max_seq_length,\n",
    "                                        'freeze_bert_layer': freeze_bert_layer,\n",
    "                                        'run_validation': run_validation,\n",
    "                                        'run_test': run_test,\n",
    "                                        'run_sample_predictions': run_sample_predictions},\n",
    "                       input_mode=input_mode,\n",
    "                       metric_definitions=metrics_definitions,\n",
    "                       train_max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 seconds per minute\n",
    "                      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Pipeline with the Step Functions SDK\n",
    "\n",
    "A typical task for a data scientist is to train a model and deploy that model to an endpoint. Without the Step Functions SDK, this is a four step process on SageMaker that includes the following.\n",
    "\n",
    "1. Training the model\n",
    "2. Creating the model on SageMaker\n",
    "3. Creating an endpoint configuration\n",
    "4. Deploying the trained model to the configured endpoint\n",
    "\n",
    "The Step Functions SDK provides the [TrainingPipeline](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/pipelines.html#stepfunctions.template.pipeline.train.TrainingPipeline) API to simplify this procedure. The following configures `pipeline` with the necessary parameters to define a training pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-835319576252/sagemaker-scikit-learn-2020-05-08-19-54-41-801/output/bert-train\n"
     ]
    }
   ],
   "source": [
    "print(train_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:  Paste the StepFunctionsWorkflowExecutionRole ARN from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_execution_role='arn:aws:iam::XXX:role/StepFunctionsWorkflowExecutionRole' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = TrainingPipeline(\n",
    "    estimator=estimator,\n",
    "    role=workflow_execution_role,\n",
    "    inputs={'train': s3_input_train_data, \n",
    "            'validation': s3_input_validation_data,\n",
    "            'test': s3_input_test_data\n",
    "    },\n",
    "    s3_bucket=bucket\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now view the workflow definition, and also visualize it as a graph. This workflow and graph represent your training pipeline.\n",
    "\n",
    "#### View the workflow definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"StartAt\": \"Training\",\n",
      "    \"States\": {\n",
      "        \"Training\": {\n",
      "            \"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\",\n",
      "            \"Parameters\": {\n",
      "                \"AlgorithmSpecification.$\": \"$$.Execution.Input['Training'].AlgorithmSpecification\",\n",
      "                \"OutputDataConfig.$\": \"$$.Execution.Input['Training'].OutputDataConfig\",\n",
      "                \"StoppingCondition.$\": \"$$.Execution.Input['Training'].StoppingCondition\",\n",
      "                \"ResourceConfig.$\": \"$$.Execution.Input['Training'].ResourceConfig\",\n",
      "                \"RoleArn.$\": \"$$.Execution.Input['Training'].RoleArn\",\n",
      "                \"InputDataConfig.$\": \"$$.Execution.Input['Training'].InputDataConfig\",\n",
      "                \"HyperParameters.$\": \"$$.Execution.Input['Training'].HyperParameters\",\n",
      "                \"TrainingJobName.$\": \"$$.Execution.Input['Training'].TrainingJobName\",\n",
      "                \"DebugHookConfig.$\": \"$$.Execution.Input['Training'].DebugHookConfig\"\n",
      "            },\n",
      "            \"Type\": \"Task\",\n",
      "            \"Next\": \"Create Model\"\n",
      "        },\n",
      "        \"Create Model\": {\n",
      "            \"Parameters\": {\n",
      "                \"ModelName.$\": \"$$.Execution.Input['Create Model'].ModelName\",\n",
      "                \"PrimaryContainer.$\": \"$$.Execution.Input['Create Model'].PrimaryContainer\",\n",
      "                \"ExecutionRoleArn.$\": \"$$.Execution.Input['Create Model'].ExecutionRoleArn\"\n",
      "            },\n",
      "            \"Resource\": \"arn:aws:states:::sagemaker:createModel\",\n",
      "            \"Type\": \"Task\",\n",
      "            \"Next\": \"Configure Endpoint\"\n",
      "        },\n",
      "        \"Configure Endpoint\": {\n",
      "            \"Resource\": \"arn:aws:states:::sagemaker:createEndpointConfig\",\n",
      "            \"Parameters\": {\n",
      "                \"EndpointConfigName.$\": \"$$.Execution.Input['Configure Endpoint'].EndpointConfigName\",\n",
      "                \"ProductionVariants.$\": \"$$.Execution.Input['Configure Endpoint'].ProductionVariants\"\n",
      "            },\n",
      "            \"Type\": \"Task\",\n",
      "            \"Next\": \"Deploy\"\n",
      "        },\n",
      "        \"Deploy\": {\n",
      "            \"Resource\": \"arn:aws:states:::sagemaker:createEndpoint\",\n",
      "            \"Parameters\": {\n",
      "                \"EndpointConfigName.$\": \"$$.Execution.Input['Deploy'].EndpointConfigName\",\n",
      "                \"EndpointName.$\": \"$$.Execution.Input['Deploy'].EndpointName\"\n",
      "            },\n",
      "            \"Type\": \"Task\",\n",
      "            \"End\": true\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(pipeline.workflow.definition.to_json(pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the workflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://do0of8uwbahzz.cloudfront.net/graph.css\">\n",
       "<div id=\"graph-273\" class=\"workflowgraph\">\n",
       "    \n",
       "    <svg></svg>\n",
       "    \n",
       "</div>\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "require.config({\n",
       "    paths: {\n",
       "        sfn: \"https://do0of8uwbahzz.cloudfront.net/sfn\",\n",
       "    }\n",
       "});\n",
       "\n",
       "require(['sfn'], function(sfn) {\n",
       "    var element = document.getElementById('graph-273')\n",
       "\n",
       "    var options = {\n",
       "        width: parseFloat(getComputedStyle(element, null).width.replace(\"px\", \"\")),\n",
       "        height: 600,\n",
       "        layout: 'LR',\n",
       "        resizeHeight: true\n",
       "    };\n",
       "\n",
       "    var definition = {\"StartAt\": \"Training\", \"States\": {\"Training\": {\"Resource\": \"arn:aws:states:::sagemaker:createTrainingJob.sync\", \"Parameters\": {\"AlgorithmSpecification.$\": \"$$.Execution.Input['Training'].AlgorithmSpecification\", \"OutputDataConfig.$\": \"$$.Execution.Input['Training'].OutputDataConfig\", \"StoppingCondition.$\": \"$$.Execution.Input['Training'].StoppingCondition\", \"ResourceConfig.$\": \"$$.Execution.Input['Training'].ResourceConfig\", \"RoleArn.$\": \"$$.Execution.Input['Training'].RoleArn\", \"InputDataConfig.$\": \"$$.Execution.Input['Training'].InputDataConfig\", \"HyperParameters.$\": \"$$.Execution.Input['Training'].HyperParameters\", \"TrainingJobName.$\": \"$$.Execution.Input['Training'].TrainingJobName\", \"DebugHookConfig.$\": \"$$.Execution.Input['Training'].DebugHookConfig\"}, \"Type\": \"Task\", \"Next\": \"Create Model\"}, \"Create Model\": {\"Parameters\": {\"ModelName.$\": \"$$.Execution.Input['Create Model'].ModelName\", \"PrimaryContainer.$\": \"$$.Execution.Input['Create Model'].PrimaryContainer\", \"ExecutionRoleArn.$\": \"$$.Execution.Input['Create Model'].ExecutionRoleArn\"}, \"Resource\": \"arn:aws:states:::sagemaker:createModel\", \"Type\": \"Task\", \"Next\": \"Configure Endpoint\"}, \"Configure Endpoint\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpointConfig\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['Configure Endpoint'].EndpointConfigName\", \"ProductionVariants.$\": \"$$.Execution.Input['Configure Endpoint'].ProductionVariants\"}, \"Type\": \"Task\", \"Next\": \"Deploy\"}, \"Deploy\": {\"Resource\": \"arn:aws:states:::sagemaker:createEndpoint\", \"Parameters\": {\"EndpointConfigName.$\": \"$$.Execution.Input['Deploy'].EndpointConfigName\", \"EndpointName.$\": \"$$.Execution.Input['Deploy'].EndpointName\"}, \"Type\": \"Task\", \"End\": true}}};\n",
       "    var elementId = '#graph-273';\n",
       "\n",
       "    var graph = new sfn.StateMachineGraph(definition, elementId, options);\n",
       "    graph.render();\n",
       "});\n",
       "\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.render_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and execute the pipeline on AWS Step Functions\n",
    "\n",
    "Create the pipeline in AWS Step Functions with [create](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArn",
     "evalue": "An error occurred (InvalidArn) when calling the CreateStateMachine operation: Invalid Role Arn: 'arn:aws:iam::XXX:role/StepFunctionsWorkflowExecutionRole'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArn\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-deb8d3eece37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/template/pipeline/common.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/stepfunctions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_machine_arn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStateMachineAlreadyExists\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_machine_arn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_state_machine_arn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/stepfunctions/workflow/stepfunctions.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mdefinition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mroleArn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         )\n\u001b[1;32m    219\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Workflow created successfully on AWS Step Functions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    315\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArn\u001b[0m: An error occurred (InvalidArn) when calling the CreateStateMachine operation: Invalid Role Arn: 'arn:aws:iam::XXX:role/StepFunctionsWorkflowExecutionRole'"
     ]
    }
   ],
   "source": [
    "pipeline.create()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the workflow with [execute](https://aws-step-functions-data-science-sdk.readthedocs.io/en/latest/workflow.html#stepfunctions.workflow.Workflow.execute). A link will be provided after the following cell is executed. Following this link, you can monitor your pipeline execution on Step Functions' console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.render_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "events = execution.list_events()\n",
    "\n",
    "while len(events) < 5:\n",
    "    print('Number of events:  {}'.format(len(events)))\n",
    "    time.sleep(10)\n",
    "    events = execution.list_events()\n",
    "\n",
    "print('Number of events:  {}'.format(len(events)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for Number of Events to Reach at Least 5 ^^ Above ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "training_job_name = json.loads(events[5]['taskSucceededEventDetails']['output'])['TrainingJobName']\n",
    "print('Training Job Name: {}'.format(training_job_name))\n",
    "\n",
    "print('')\n",
    "\n",
    "trained_model_s3_uri = json.loads(events[5]['taskSucceededEventDetails']['output'])['ModelArtifacts']['S3ModelArtifacts']\n",
    "print('Trained Model S3 URI: {}'.format(trained_model_s3_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:  Copy the Inference Model\n",
    "TODO:  Update the S3 location to the deploymed model (not trained model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp s3://$bucket/training-pipeline-2020-04-30-03-15-17/models/estimator-training-pipeline-2020-04-30-03-15-22/output/model.tar.gz ./model.tar.gz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf ./model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir ./tensorflow/saved_model/0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = execution.list_events()\n",
    "\n",
    "while len(events) < 18:\n",
    "    print('Number of events:  {}'.format(len(events)))\n",
    "    time.sleep(10)\n",
    "    events = execution.list_events()\n",
    "\n",
    "print('Number of events:  {}'.format(len(events)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for Number of Events to Reach at Least 18 ^^ Above ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "step_functions_pipeline_endpoint_name = json.loads(events[18]['taskScheduledEventDetails']['parameters'])['EndpointName']\n",
    "\n",
    "print('Endpoint Name: {}'.format(step_functions_pipeline_endpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = execution.list_events()\n",
    "\n",
    "while len(events) < 21:\n",
    "    print('Number of events:  {}'.format(len(events)))\n",
    "    time.sleep(10)\n",
    "    events = execution.list_events()    \n",
    "\n",
    "print('Number of events:  {}'.format(len(events)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait for Number of Events to Reach at Least 21 ^^ Above ^^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_details = json.loads(events[21]['stateExitedEventDetails']['output'])\n",
    "\n",
    "print(event_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(step_functions_pipeline_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store step_functions_pipeline_endpoint_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
