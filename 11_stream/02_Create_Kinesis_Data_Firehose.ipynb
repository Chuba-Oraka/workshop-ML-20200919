{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Kinesis Data Firehose\n",
    "\n",
    "Amazon Kinesis Data Firehose is a fully managed service for delivering real-time streaming data to destinations such as Amazon Simple Storage Service (Amazon S3), Amazon Redshift, Amazon Elasticsearch Service (Amazon ES), Splunk, and any custom HTTP endpoint. \n",
    "\n",
    "For Amazon S3 destinations, streaming data is delivered to your S3 bucket. If data transformation is enabled, you can optionally back up source data to another Amazon S3 bucket.\n",
    "\n",
    "![](img/kinesis_firehose_s3_docs.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "sess   = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "sm = boto3.Session().client(service_name='sagemaker', region_name=region)\n",
    "firehose = boto3.Session().client(service_name='firehose', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r firehose_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(firehose_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r iam_role_kinesis_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iam_role_kinesis_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Kinesis Data Firehose Delivery Stream\n",
    "_This may take 1-2 minutes.  Please be patient._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "try: \n",
    "    response = firehose.create_delivery_stream(\n",
    "        DeliveryStreamName=firehose_name,\n",
    "        DeliveryStreamType='DirectPut',\n",
    "        S3DestinationConfiguration={\n",
    "            'RoleARN': iam_role_kinesis_arn,\n",
    "            'BucketARN': 'arn:aws:s3:::{}'.format(bucket),\n",
    "            'Prefix': 'kinesis-data-firehose',        \n",
    "        }\n",
    "    )\n",
    "    print('Delivery stream {} successfully created.'.format(firehose_name))\n",
    "    print(json.dumps(response, indent=4, sort_keys=True, default=str))\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceInUseException':\n",
    "        print('Delivery stream {} already exists.'.format(firehose_name))\n",
    "    else:\n",
    "        print('Unexpected error: %s' % e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "status = ''\n",
    "while status != 'ACTIVE':    \n",
    "    r = firehose.describe_delivery_stream(DeliveryStreamName=firehose_name)\n",
    "    description = r.get('DeliveryStreamDescription')\n",
    "    status = description.get('DeliveryStreamStatus')\n",
    "    time.sleep(5)\n",
    "    \n",
    "print('Delivery Stream {} is active'.format(firehose_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = firehose.describe_delivery_stream(DeliveryStreamName=firehose_name)\n",
    "\n",
    "status = description.get('DeliveryStreamStatus')\n",
    "print(status)\n",
    "\n",
    "print()\n",
    "\n",
    "description = r.get('DeliveryStreamDescription')\n",
    "print(json.dumps(description, indent=4, sort_keys=True, default=str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "firehose_arn = r['DeliveryStreamDescription']['DeliveryStreamARN']\n",
    "print(firehose_arn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Variables for the Next Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store firehose_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.save_checkpoint();\n",
    "Jupyter.notebook.session.delete();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "\n",
    "* https://github.com/aws-samples/aws-ml-data-lake-workshop\n",
    "* https://aws.amazon.com/blogs/big-data/snakes-in-the-stream-feeding-and-eating-amazon-kinesis-streams-with-python/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
